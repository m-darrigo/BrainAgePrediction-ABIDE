{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 22:27:30.192860: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.alpha\": 0.5,\n",
    "        \"grid.linestyle\": \"--\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/FS_features_ABIDE_males.csv\", sep=\";\")\n",
    "df = df.set_index(\"FILE_ID\")\n",
    "\n",
    "# drop target\n",
    "y = df[\"AGE_AT_SCAN\"]\n",
    "df = df.drop([\"AGE_AT_SCAN\", \"SEX\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "linear_regressor = Pipeline(\n",
    "    [\n",
    "        ('scaler', PowerTransformer()),\n",
    "        (\"principal_components\", PCA(n_components=20, svd_solver = 'full')),\n",
    "        (\"model\",  Lasso())\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_distr = {\"model__alpha\": np.arange(1, 10)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "search = GridSearchCV(\n",
    "    linear_regressor, \n",
    "    param_distr, \n",
    "    return_train_score=True, \n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    cv=KFold(shuffle=True)\n",
    ")\n",
    "\n",
    "search.fit(df.values, y.values)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_regressor = pd.DataFrame(search.cv_results_)\n",
    "no_overfit = results_regressor[\"mean_train_score\"] - results_regressor[\"mean_test_score\"] < 6\n",
    "\n",
    "(\n",
    "    results_regressor[no_overfit]\n",
    "    .sort_values(\"mean_test_score\", ascending=False)\n",
    "    .to_csv(\"best_model_regressor.csv\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.525392</td>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.011376</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>1</td>\n",
       "      <td>{'model__alpha': 1}</td>\n",
       "      <td>-3.130726</td>\n",
       "      <td>-3.587047</td>\n",
       "      <td>-3.343415</td>\n",
       "      <td>-3.574417</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.504940</td>\n",
       "      <td>0.255057</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.513624</td>\n",
       "      <td>-3.360552</td>\n",
       "      <td>-3.375411</td>\n",
       "      <td>-3.404058</td>\n",
       "      <td>-3.365302</td>\n",
       "      <td>-3.403790</td>\n",
       "      <td>0.056956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.525057</td>\n",
       "      <td>0.008904</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>2</td>\n",
       "      <td>{'model__alpha': 2}</td>\n",
       "      <td>-3.206421</td>\n",
       "      <td>-3.733756</td>\n",
       "      <td>-3.511315</td>\n",
       "      <td>-3.606067</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.615991</td>\n",
       "      <td>0.267521</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.665446</td>\n",
       "      <td>-3.473042</td>\n",
       "      <td>-3.597431</td>\n",
       "      <td>-3.517184</td>\n",
       "      <td>-3.478062</td>\n",
       "      <td>-3.546233</td>\n",
       "      <td>0.074405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.528082</td>\n",
       "      <td>0.005280</td>\n",
       "      <td>0.011495</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>3</td>\n",
       "      <td>{'model__alpha': 3}</td>\n",
       "      <td>-3.316920</td>\n",
       "      <td>-3.835746</td>\n",
       "      <td>-3.673683</td>\n",
       "      <td>-3.614957</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.712537</td>\n",
       "      <td>0.264529</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.782780</td>\n",
       "      <td>-3.577047</td>\n",
       "      <td>-3.679416</td>\n",
       "      <td>-3.637104</td>\n",
       "      <td>-3.566375</td>\n",
       "      <td>-3.648544</td>\n",
       "      <td>0.078698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.527357</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.012581</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>4</td>\n",
       "      <td>{'model__alpha': 4}</td>\n",
       "      <td>-3.371966</td>\n",
       "      <td>-3.921743</td>\n",
       "      <td>-3.753456</td>\n",
       "      <td>-3.617083</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.757274</td>\n",
       "      <td>0.256241</td>\n",
       "      <td>4</td>\n",
       "      <td>-3.837830</td>\n",
       "      <td>-3.636879</td>\n",
       "      <td>-3.734047</td>\n",
       "      <td>-3.704395</td>\n",
       "      <td>-3.616281</td>\n",
       "      <td>-3.705886</td>\n",
       "      <td>0.078734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.542543</td>\n",
       "      <td>0.025223</td>\n",
       "      <td>0.011669</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>5</td>\n",
       "      <td>{'model__alpha': 5}</td>\n",
       "      <td>-3.418648</td>\n",
       "      <td>-3.978349</td>\n",
       "      <td>-3.786265</td>\n",
       "      <td>-3.631704</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.785537</td>\n",
       "      <td>0.245966</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.863648</td>\n",
       "      <td>-3.658446</td>\n",
       "      <td>-3.760791</td>\n",
       "      <td>-3.735464</td>\n",
       "      <td>-3.640877</td>\n",
       "      <td>-3.731845</td>\n",
       "      <td>0.079857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.547673</td>\n",
       "      <td>0.032449</td>\n",
       "      <td>0.015497</td>\n",
       "      <td>0.007710</td>\n",
       "      <td>6</td>\n",
       "      <td>{'model__alpha': 6}</td>\n",
       "      <td>-3.483819</td>\n",
       "      <td>-4.047048</td>\n",
       "      <td>-3.837421</td>\n",
       "      <td>-3.657301</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.826043</td>\n",
       "      <td>0.233312</td>\n",
       "      <td>6</td>\n",
       "      <td>-3.896894</td>\n",
       "      <td>-3.693432</td>\n",
       "      <td>-3.791917</td>\n",
       "      <td>-3.770688</td>\n",
       "      <td>-3.677133</td>\n",
       "      <td>-3.766013</td>\n",
       "      <td>0.078728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.560056</td>\n",
       "      <td>0.044647</td>\n",
       "      <td>0.011518</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>7</td>\n",
       "      <td>{'model__alpha': 7}</td>\n",
       "      <td>-3.560112</td>\n",
       "      <td>-4.126330</td>\n",
       "      <td>-3.877569</td>\n",
       "      <td>-3.694166</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.871395</td>\n",
       "      <td>0.221381</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.940902</td>\n",
       "      <td>-3.735851</td>\n",
       "      <td>-3.821946</td>\n",
       "      <td>-3.821176</td>\n",
       "      <td>-3.726483</td>\n",
       "      <td>-3.809271</td>\n",
       "      <td>0.077296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.606102</td>\n",
       "      <td>0.120090</td>\n",
       "      <td>0.012028</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>8</td>\n",
       "      <td>{'model__alpha': 8}</td>\n",
       "      <td>-3.640036</td>\n",
       "      <td>-4.211265</td>\n",
       "      <td>-3.920920</td>\n",
       "      <td>-3.751284</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.924093</td>\n",
       "      <td>0.211146</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.992336</td>\n",
       "      <td>-3.792506</td>\n",
       "      <td>-3.860022</td>\n",
       "      <td>-3.877966</td>\n",
       "      <td>-3.787238</td>\n",
       "      <td>-3.862014</td>\n",
       "      <td>0.074384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.539231</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.011601</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>9</td>\n",
       "      <td>{'model__alpha': 9}</td>\n",
       "      <td>-3.687827</td>\n",
       "      <td>-4.304946</td>\n",
       "      <td>-3.967656</td>\n",
       "      <td>-3.790742</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.970647</td>\n",
       "      <td>0.219722</td>\n",
       "      <td>9</td>\n",
       "      <td>-4.031380</td>\n",
       "      <td>-3.856364</td>\n",
       "      <td>-3.905056</td>\n",
       "      <td>-3.921027</td>\n",
       "      <td>-3.856806</td>\n",
       "      <td>-3.914126</td>\n",
       "      <td>0.064032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.525392      0.008408         0.011376        0.000152   \n",
       "1       0.525057      0.008904         0.011378        0.000142   \n",
       "2       0.528082      0.005280         0.011495        0.000167   \n",
       "3       0.527357      0.002299         0.012581        0.001830   \n",
       "4       0.542543      0.025223         0.011669        0.000214   \n",
       "5       0.547673      0.032449         0.015497        0.007710   \n",
       "6       0.560056      0.044647         0.011518        0.000214   \n",
       "7       0.606102      0.120090         0.012028        0.000344   \n",
       "8       0.539231      0.004812         0.011601        0.000204   \n",
       "\n",
       "  param_model__alpha               params  split0_test_score  \\\n",
       "0                  1  {'model__alpha': 1}          -3.130726   \n",
       "1                  2  {'model__alpha': 2}          -3.206421   \n",
       "2                  3  {'model__alpha': 3}          -3.316920   \n",
       "3                  4  {'model__alpha': 4}          -3.371966   \n",
       "4                  5  {'model__alpha': 5}          -3.418648   \n",
       "5                  6  {'model__alpha': 6}          -3.483819   \n",
       "6                  7  {'model__alpha': 7}          -3.560112   \n",
       "7                  8  {'model__alpha': 8}          -3.640036   \n",
       "8                  9  {'model__alpha': 9}          -3.687827   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0          -3.587047          -3.343415          -3.574417  ...   \n",
       "1          -3.733756          -3.511315          -3.606067  ...   \n",
       "2          -3.835746          -3.673683          -3.614957  ...   \n",
       "3          -3.921743          -3.753456          -3.617083  ...   \n",
       "4          -3.978349          -3.786265          -3.631704  ...   \n",
       "5          -4.047048          -3.837421          -3.657301  ...   \n",
       "6          -4.126330          -3.877569          -3.694166  ...   \n",
       "7          -4.211265          -3.920920          -3.751284  ...   \n",
       "8          -4.304946          -3.967656          -3.790742  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0        -3.504940        0.255057                1           -3.513624   \n",
       "1        -3.615991        0.267521                2           -3.665446   \n",
       "2        -3.712537        0.264529                3           -3.782780   \n",
       "3        -3.757274        0.256241                4           -3.837830   \n",
       "4        -3.785537        0.245966                5           -3.863648   \n",
       "5        -3.826043        0.233312                6           -3.896894   \n",
       "6        -3.871395        0.221381                7           -3.940902   \n",
       "7        -3.924093        0.211146                8           -3.992336   \n",
       "8        -3.970647        0.219722                9           -4.031380   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0           -3.360552           -3.375411           -3.404058   \n",
       "1           -3.473042           -3.597431           -3.517184   \n",
       "2           -3.577047           -3.679416           -3.637104   \n",
       "3           -3.636879           -3.734047           -3.704395   \n",
       "4           -3.658446           -3.760791           -3.735464   \n",
       "5           -3.693432           -3.791917           -3.770688   \n",
       "6           -3.735851           -3.821946           -3.821176   \n",
       "7           -3.792506           -3.860022           -3.877966   \n",
       "8           -3.856364           -3.905056           -3.921027   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0           -3.365302         -3.403790         0.056956  \n",
       "1           -3.478062         -3.546233         0.074405  \n",
       "2           -3.566375         -3.648544         0.078698  \n",
       "3           -3.616281         -3.705886         0.078734  \n",
       "4           -3.640877         -3.731845         0.079857  \n",
       "5           -3.677133         -3.766013         0.078728  \n",
       "6           -3.726483         -3.809271         0.077296  \n",
       "7           -3.787238         -3.862014         0.074384  \n",
       "8           -3.856806         -3.914126         0.064032  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.525392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.008408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.011376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_model__alpha</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'model__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>-3.130726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>-3.587047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>-3.343415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>-3.574417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>-3.889095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>-3.50494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.255057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>-3.513624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>-3.360552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>-3.375411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>-3.404058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>-3.365302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>-3.40379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.056956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0\n",
       "mean_fit_time                  0.525392\n",
       "std_fit_time                   0.008408\n",
       "mean_score_time                0.011376\n",
       "std_score_time                 0.000152\n",
       "param_model__alpha                    1\n",
       "params              {'model__alpha': 1}\n",
       "split0_test_score             -3.130726\n",
       "split1_test_score             -3.587047\n",
       "split2_test_score             -3.343415\n",
       "split3_test_score             -3.574417\n",
       "split4_test_score             -3.889095\n",
       "mean_test_score                -3.50494\n",
       "std_test_score                 0.255057\n",
       "rank_test_score                       1\n",
       "split0_train_score            -3.513624\n",
       "split1_train_score            -3.360552\n",
       "split2_train_score            -3.375411\n",
       "split3_train_score            -3.404058\n",
       "split4_train_score            -3.365302\n",
       "mean_train_score               -3.40379\n",
       "std_train_score                0.056956"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"best_model_regressor.csv\").head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from itertools import product\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoLayerFeedForward(nlayers, hiddens, dropouts, meta):\n",
    "    clf = Sequential()\n",
    "    X_shape_ = (meta[\"X_shape_\"][1],)\n",
    "\n",
    "    clf.add(Dense(hiddens[0], activation='relu', input_shape=X_shape_))\n",
    "    if dropouts[0] > 0:\n",
    "        clf.add(Dropout(dropouts[0]))\n",
    "    for i in range(1, nlayers):\n",
    "        clf.add(Dense(hiddens[i], activation='relu'))\n",
    "        if dropouts[i] > 0:\n",
    "            clf.add(Dropout(dropouts[i]))\n",
    "    clf.add(Dense(1))\n",
    "    return clf\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "mlp = KerasRegressor(\n",
    "    twoLayerFeedForward, \n",
    "    epochs=200, \n",
    "    loss=\"mse\",\n",
    "    callbacks=[callback],\n",
    "    validation_split=0.2,\n",
    "    nlayers=3,\n",
    "    hiddens=[2,2,2],\n",
    "    dropouts=[0.2, 0, 0],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_entries = [4, 8, 16, 32, 64]\n",
    "hidden_layers = list(product(valid_entries, repeat=6))\n",
    "dropouts = list(np.random.choice([0.0, 0.1, 0.2, 0.3], p=[0.4, 0.2, 0.2, 0.2], size=(2000, 6)))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(\n",
    "    [\n",
    "        ('scaler', PowerTransformer()),\n",
    "        (\"principal_components\", PCA(n_components=20, svd_solver = 'full')),\n",
    "        (\"mlp\", mlp)\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"mlp__nlayers\": np.random.randint(1, 7, size=2000),\n",
    "    \"mlp__hiddens\": hidden_layers,\n",
    "    \"mlp__dropouts\": dropouts,\n",
    "    \"mlp__optimizer__learning_rate\": [0.0001, 0.001, 0.005],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = RandomizedSearchCV(\n",
    "    model, \n",
    "    params, \n",
    "    refit=False, \n",
    "    cv=KFold(shuffle=True), \n",
    "    return_train_score=True,\n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    n_iter=100, \n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.289, test=-3.781) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.438, test=-4.039) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.314, test=-3.313) total time=   1.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.438, test=-3.739) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.972, test=-4.341) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.152, test=-5.407) total time=   4.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.794, test=-6.169) total time=   3.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.085, test=-5.236) total time=   3.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.510, test=-5.689) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.741, test=-5.824) total time=   2.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.994, test=-4.279) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.170, test=-3.824) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.809, test=-2.969) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-4.061, test=-3.935) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.608, test=-3.644) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.608, test=-4.738) total time=   5.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.709, test=-5.298) total time=   4.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.461, test=-4.550) total time=   4.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.828, test=-5.462) total time=   3.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.518, test=-4.821) total time=   5.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.051, test=-3.547) total time=   1.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.504, test=-3.927) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.199, test=-3.558) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.399, test=-3.746) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.964, test=-3.570) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.212, test=-9.094) total time=   5.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-10.256, test=-10.930) total time=   5.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.315, test=-13.520) total time=   5.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.209, test=-11.440) total time=   5.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.838, test=-16.196) total time=   5.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.142, test=-3.511) total time=   2.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.310, test=-3.866) total time=   2.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.102, test=-3.238) total time=   2.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.180, test=-2.949) total time=   2.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.135, test=-3.389) total time=   2.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.809, test=-5.464) total time=   3.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.934, test=-5.384) total time=   4.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.593, test=-5.603) total time=   3.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.546, test=-5.422) total time=   3.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.232, test=-5.366) total time=   4.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.401, test=-3.680) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.871, test=-3.348) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.252, test=-3.538) total time=   1.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.121, test=-3.155) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.304, test=-3.567) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.999, test=-3.634) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.325, test=-3.700) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.415, test=-3.420) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.283, test=-3.609) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.845, test=-3.528) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.639, test=-3.984) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.225, test=-3.652) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.512, test=-3.311) total time=   1.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.147, test=-3.126) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.962, test=-3.483) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.328, test=-3.853) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.254, test=-3.565) total time=   3.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.625, test=-3.684) total time=   2.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.641, test=-3.997) total time=   2.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.392, test=-3.909) total time=   2.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.209, test=-4.514) total time=   2.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.626, test=-4.193) total time=   4.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.022, test=-3.986) total time=   3.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.710, test=-4.086) total time=   4.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.766, test=-4.165) total time=   3.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.754, test=-4.075) total time=   5.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.635, test=-4.117) total time=   5.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.654, test=-3.734) total time=   5.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.675, test=-4.045) total time=   5.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.635, test=-3.919) total time=   5.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.807, test=-4.234) total time=   2.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.513, test=-3.837) total time=   2.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.832, test=-3.825) total time=   1.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.933, test=-4.334) total time=   1.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.434, test=-3.666) total time=   2.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.708, test=-4.286) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.132, test=-3.633) total time=   1.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.469, test=-3.461) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.516, test=-3.356) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.352, test=-3.903) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.418, test=-3.810) total time=   5.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.431, test=-4.163) total time=   5.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.473, test=-3.450) total time=   4.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.431, test=-3.871) total time=   5.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.432, test=-3.828) total time=   5.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.239, test=-3.812) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.246, test=-3.687) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.299, test=-3.349) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.162, test=-4.264) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.994, test=-3.456) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.278, test=-4.802) total time=   3.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.015, test=-4.356) total time=   3.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.455, test=-3.566) total time=   4.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.205, test=-4.645) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.540, test=-3.804) total time=   4.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.679, test=-5.544) total time=   5.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.053, test=-5.828) total time=   5.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.488, test=-7.781) total time=   5.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.581, test=-5.390) total time=   5.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.894, test=-5.644) total time=   5.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.538, test=-4.693) total time=   1.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.665, test=-4.089) total time=   2.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.579, test=-4.790) total time=   2.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.229, test=-5.322) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.610, test=-5.835) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.636, test=-3.940) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.348, test=-3.862) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.138, test=-3.313) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.018, test=-3.257) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.205, test=-3.715) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.089, test=-4.260) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.440, test=-4.006) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.576, test=-3.379) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.252, test=-3.645) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.629, test=-4.032) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.206, test=-3.767) total time=   1.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.469, test=-3.883) total time=   1.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.472, test=-3.375) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.025, test=-3.287) total time=   2.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.132, test=-3.763) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.405, test=-4.021) total time=   1.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.599, test=-3.999) total time=   1.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.312, test=-3.447) total time=   2.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.500, test=-3.946) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.362, test=-3.770) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.078, test=-3.622) total time=   1.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.851, test=-3.558) total time=   1.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.247, test=-3.569) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.036, test=-3.353) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.115, test=-3.775) total time=   1.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.344, test=-3.739) total time=   5.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.454, test=-4.059) total time=   5.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.408, test=-3.285) total time=   5.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.572, test=-4.009) total time=   5.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.492, test=-3.878) total time=   6.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.015, test=-3.694) total time=   1.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.911, test=-3.530) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.008, test=-3.297) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.933, test=-3.063) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.926, test=-3.710) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.384, test=-4.627) total time=   6.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.264, test=-5.165) total time=   5.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.405, test=-5.476) total time=   5.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.585, test=-4.866) total time=   5.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.508, test=-4.457) total time=   5.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.262, test=-4.518) total time=   2.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.325, test=-4.736) total time=   2.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.488, test=-4.550) total time=   3.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.734, test=-4.937) total time=   2.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.508, test=-4.808) total time=   2.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.835, test=-3.715) total time=   3.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.155, test=-3.840) total time=   2.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.039, test=-3.246) total time=   2.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.119, test=-3.750) total time=   3.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.146, test=-4.089) total time=   2.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.123, test=-5.367) total time=   2.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.947, test=-5.290) total time=   2.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.976, test=-4.945) total time=   2.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.205, test=-5.596) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.030, test=-5.121) total time=   2.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.627, test=-4.562) total time=   6.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.195, test=-4.551) total time=   5.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.067, test=-4.135) total time=   5.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.285, test=-4.876) total time=   5.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.144, test=-5.554) total time=   5.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.643, test=-4.252) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.071, test=-3.665) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.718, test=-3.714) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-4.618, test=-4.747) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.744, test=-4.118) total time=   1.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.819, test=-5.900) total time=   5.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.232, test=-4.469) total time=   6.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.493, test=-5.885) total time=   5.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.592, test=-5.621) total time=   6.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.701, test=-5.713) total time=   6.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.032, test=-5.148) total time=   6.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.784, test=-5.373) total time=   4.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.199, test=-4.267) total time=   4.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.925, test=-4.420) total time=   3.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.367, test=-4.518) total time=   4.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.781, test=-3.868) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.867, test=-3.477) total time=   1.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-4.889, test=-5.137) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-4.257, test=-4.061) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-5.249, test=-5.346) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.633, test=-3.884) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.862, test=-4.335) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.327, test=-3.394) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.332, test=-4.562) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.423, test=-4.768) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.954, test=-3.631) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.860, test=-3.524) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.981, test=-3.356) total time=   1.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.584, test=-3.790) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.977, test=-3.644) total time=   1.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.510, test=-3.987) total time=   1.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.720, test=-3.841) total time=   1.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.882, test=-3.929) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.831, test=-3.872) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.028, test=-4.469) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.394, test=-4.243) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.067, test=-3.908) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.594, test=-3.660) total time=   2.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.970, test=-3.695) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.806, test=-3.679) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-5.637, test=-5.432) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.289, test=-4.569) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.363, test=-3.179) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.305, test=-4.340) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.246, test=-3.282) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.792, test=-3.713) total time=   2.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.002, test=-4.355) total time=   2.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.997, test=-4.404) total time=   3.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.040, test=-4.848) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.944, test=-5.826) total time=   2.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.042, test=-3.683) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.743, test=-3.860) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.109, test=-3.309) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.579, test=-3.697) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-6.031, test=-5.793) total time=   6.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.128, test=-3.207) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.862, test=-4.408) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.073, test=-2.998) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.394, test=-3.082) total time=   1.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.496, test=-3.565) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.931, test=-3.393) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.044, test=-3.689) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.636, test=-4.020) total time=   1.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.014, test=-3.235) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.955, test=-4.067) total time=   1.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.934, test=-3.932) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-4.096, test=-4.524) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.454, test=-3.658) total time=   2.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.810, test=-3.822) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-4.125, test=-4.430) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.202, test=-3.503) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.265, test=-3.621) total time=   2.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.033, test=-3.097) total time=   3.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.440, test=-3.087) total time=   2.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.161, test=-3.225) total time=   2.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.302, test=-4.553) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.723, test=-4.207) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.483, test=-3.794) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.505, test=-3.648) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.312, test=-3.726) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.934, test=-5.176) total time=   4.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.879, test=-5.551) total time=   5.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.756, test=-5.988) total time=   5.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.993, test=-5.435) total time=   5.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.500, test=-5.964) total time=   5.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.606, test=-3.773) total time=   1.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.195, test=-3.975) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.060, test=-3.226) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.527, test=-3.530) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.105, test=-3.238) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.403, test=-6.522) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.756, test=-4.490) total time=   5.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.806, test=-4.086) total time=   6.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.307, test=-4.580) total time=   3.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.558, test=-4.915) total time=   4.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.749, test=-5.209) total time=   4.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.827, test=-5.201) total time=   3.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.885, test=-4.086) total time=   4.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.719, test=-5.033) total time=   4.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.011, test=-5.153) total time=   5.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.214, test=-4.538) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.659, test=-4.194) total time=   4.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.854, test=-3.916) total time=   4.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.895, test=-4.275) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.938, test=-4.183) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.578, test=-3.837) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.824, test=-4.593) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.555, test=-3.872) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.614, test=-3.400) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.734, test=-3.906) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.985, test=-5.032) total time=   4.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.052, test=-5.554) total time=   4.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.648, test=-4.745) total time=   4.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.604, test=-4.464) total time=   5.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.588, test=-4.583) total time=   5.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.027, test=-3.930) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.471, test=-3.809) total time=   1.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.409, test=-3.419) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.492, test=-3.432) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-5.077, test=-5.404) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.949, test=-5.246) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.413, test=-4.716) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-5.257, test=-5.418) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-7.783, test=-7.896) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.658, test=-4.836) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.453, test=-8.218) total time=   6.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-10.680, test=-11.205) total time=   5.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-12.443, test=-13.336) total time=   5.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-14.928, test=-15.182) total time=   5.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-14.464, test=-13.860) total time=   5.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.827, test=-4.974) total time=   5.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.120, test=-5.696) total time=   4.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.564, test=-5.466) total time=   3.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.146, test=-16.114) total time=   6.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.837, test=-5.250) total time=   5.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.581, test=-3.901) total time=   1.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.333, test=-3.966) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.994, test=-3.196) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.992, test=-3.155) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.957, test=-3.625) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-4.076, test=-4.033) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.205, test=-3.669) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.280, test=-3.690) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.613, test=-3.422) total time=   2.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.804, test=-3.807) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.142, test=-13.103) total time=   5.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.312, test=-13.772) total time=   5.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.065, test=-17.060) total time=   5.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.374, test=-11.210) total time=   5.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.900, test=-9.252) total time=   5.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.206, test=-3.738) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.451, test=-3.824) total time=   2.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.505, test=-3.437) total time=   2.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.251, test=-2.926) total time=   2.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.434, test=-3.486) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.117, test=-3.748) total time=   1.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.035, test=-3.547) total time=   2.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.841, test=-3.275) total time=   2.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.121, test=-3.375) total time=   2.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.035, test=-3.707) total time=   2.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.493, test=-5.655) total time=   6.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.939, test=-17.492) total time=   6.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.907, test=-17.620) total time=   7.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.185, test=-6.013) total time=   6.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.661, test=-6.664) total time=   6.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.014, test=-4.725) total time=   2.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.085, test=-4.586) total time=   2.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.033, test=-4.465) total time=   2.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.246, test=-4.049) total time=   2.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-13.773, test=-13.127) total time=   6.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.294, test=-3.910) total time=   1.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.744, test=-3.927) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.674, test=-3.724) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.281, test=-3.525) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.463, test=-3.946) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.277, test=-3.478) total time=   1.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.002, test=-3.617) total time=   1.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.007, test=-3.280) total time=   2.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.745, test=-3.948) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-2.878, test=-3.222) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.600, test=-4.959) total time=   1.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.692, test=-4.091) total time=   2.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.299, test=-4.243) total time=   1.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.591, test=-4.746) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.152, test=-5.167) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.078, test=-3.722) total time=   2.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.056, test=-3.633) total time=   2.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.113, test=-3.419) total time=   2.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.031, test=-3.939) total time=   1.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.733, test=-3.955) total time=   2.1s\n",
      "[CV 1/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.179, test=-4.040) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.330, test=-4.103) total time=   2.8s\n",
      "[CV 3/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-2.954, test=-3.284) total time=   2.6s\n",
      "[CV 4/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.409, test=-3.440) total time=   1.8s\n",
      "[CV 5/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.388, test=-3.751) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.064, test=-5.028) total time=   3.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.293, test=-4.834) total time=   4.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.596, test=-4.683) total time=   4.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.934, test=-5.325) total time=   2.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.611, test=-4.789) total time=   4.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.243, test=-3.990) total time=   3.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.319, test=-3.718) total time=   2.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.416, test=-3.732) total time=   2.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.542, test=-3.876) total time=   2.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.582, test=-4.047) total time=   2.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.775, test=-7.938) total time=   2.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.167, test=-6.661) total time=   6.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.067, test=-4.089) total time=   5.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.444, test=-5.522) total time=   5.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-17.179, test=-16.533) total time=   5.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.604, test=-3.709) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.994, test=-3.510) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-5.109, test=-5.547) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.939, test=-3.968) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.920, test=-3.959) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.788, test=-5.616) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.216, test=-6.736) total time=   3.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.913, test=-7.227) total time=   2.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.089, test=-7.422) total time=   2.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.636, test=-5.819) total time=   3.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.616, test=-3.870) total time=   1.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.252, test=-3.730) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.922, test=-3.291) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.057, test=-3.057) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.654, test=-3.182) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.029, test=-4.545) total time=   4.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.000, test=-4.428) total time=   4.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.564, test=-3.999) total time=   4.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.511, test=-3.775) total time=   6.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.339, test=-3.720) total time=   6.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.947, test=-3.698) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.233, test=-3.936) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.793, test=-3.568) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.189, test=-3.434) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.242, test=-3.822) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.073, test=-4.060) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.199, test=-3.887) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.104, test=-3.583) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.496, test=-3.563) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.058, test=-3.877) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.520, test=-3.760) total time=   3.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-13.532, test=-14.085) total time=   5.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.066, test=-3.475) total time=   2.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.308, test=-3.195) total time=   2.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.837, test=-4.154) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.858, test=-3.624) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.878, test=-3.650) total time=   2.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.433, test=-3.337) total time=   1.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.142, test=-3.586) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.991, test=-3.666) total time=   2.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.032, test=-3.357) total time=   2.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.293, test=-3.952) total time=   1.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.188, test=-3.121) total time=   1.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.491, test=-3.321) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.274, test=-3.708) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-2.959, test=-3.655) total time=   2.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.184, test=-3.856) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.014, test=-3.252) total time=   2.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.417, test=-3.395) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.070, test=-3.689) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.837, test=-3.257) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.138, test=-3.735) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.500, test=-3.914) total time=   2.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.471, test=-3.432) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.694, test=-3.982) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.658, test=-3.841) total time=   1.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.306, test=-3.543) total time=   1.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.938, test=-4.301) total time=   1.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.235, test=-4.061) total time=   2.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.827, test=-3.986) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.969, test=-3.649) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.054, test=-3.936) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.697, test=-3.335) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.034, test=-3.436) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.925, test=-3.746) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.483, test=-3.801) total time=   1.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.011, test=-3.589) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.564, test=-3.681) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.022, test=-3.006) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.536, test=-3.923) total time=   1.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-17.147, test=-16.660) total time=   5.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.507, test=-9.657) total time=   5.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.190, test=-8.611) total time=   5.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.952, test=-14.079) total time=   5.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.187, test=-9.618) total time=   5.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.123, test=-3.445) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.881, test=-3.440) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.200, test=-3.266) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.204, test=-2.979) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.884, test=-3.099) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.969, test=-3.194) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.900, test=-3.603) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.490, test=-3.543) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.435, test=-4.590) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.310, test=-3.488) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.051, test=-3.710) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.392, test=-3.950) total time=   1.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.194, test=-3.302) total time=   1.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.922, test=-3.252) total time=   2.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.226, test=-3.730) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.900, test=-4.316) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.995, test=-5.210) total time=   1.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.869, test=-5.075) total time=   1.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.366, test=-4.573) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.432, test=-4.553) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.141, test=-3.704) total time=   1.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.010, test=-3.683) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.783, test=-3.071) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.041, test=-3.260) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.701, test=-3.544) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.939, test=-9.293) total time=   7.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.808, test=-8.976) total time=   6.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-14.355, test=-14.680) total time=   5.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.149, test=-7.658) total time=   5.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.180, test=-15.657) total time=   5.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.161, test=-4.205) total time=   4.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.743, test=-6.863) total time=   2.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.454, test=-4.785) total time=   5.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.723, test=-3.750) total time=   6.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.295, test=-4.493) total time=   4.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.304, test=-3.749) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.469, test=-3.845) total time=   2.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.098, test=-3.430) total time=   2.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.089, test=-2.948) total time=   2.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.087, test=-4.376) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.512, test=-5.294) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.287, test=-6.765) total time=   2.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.947, test=-6.004) total time=   3.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.070, test=-5.161) total time=   4.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.157, test=-5.176) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.847, test=-9.026) total time=   5.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.127, test=-8.612) total time=   5.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.089, test=-11.622) total time=   5.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.278, test=-16.186) total time=   5.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-10.790, test=-10.616) total time=   5.7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[(&#x27;scaler&#x27;, PowerTransformer()),\n",
       "                                             (&#x27;principal_components&#x27;,\n",
       "                                              PCA(n_components=20,\n",
       "                                                  svd_solver=&#x27;full&#x27;)),\n",
       "                                             (&#x27;mlp&#x27;,\n",
       "                                              KerasRegressor(callbacks=[&lt;keras.callbacks.EarlyStopping object at 0x7fd76c713640&gt;], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss=&#x27;mse&#x27;, model=&lt;functio...\n",
       "                                                         (4, 4, 4, 4, 32, 32),\n",
       "                                                         (4, 4, 4, 4, 32, 64),\n",
       "                                                         (4, 4, 4, 4, 64, 4),\n",
       "                                                         (4, 4, 4, 4, 64, 8),\n",
       "                                                         (4, 4, 4, 4, 64, 16),\n",
       "                                                         (4, 4, 4, 4, 64, 32),\n",
       "                                                         (4, 4, 4, 4, 64, 64),\n",
       "                                                         (4, 4, 4, 8, 4, 4),\n",
       "                                                         (4, 4, 4, 8, 4, 8),\n",
       "                                                         (4, 4, 4, 8, 4, 16),\n",
       "                                                         (4, 4, 4, 8, 4, 32),\n",
       "                                                         (4, 4, 4, 8, 4, 64), ...],\n",
       "                                        &#x27;mlp__nlayers&#x27;: array([1, 6, 3, ..., 5, 3, 3]),\n",
       "                                        &#x27;mlp__optimizer__learning_rate&#x27;: [0.0001,\n",
       "                                                                          0.001,\n",
       "                                                                          0.005]},\n",
       "                   refit=False, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[(&#x27;scaler&#x27;, PowerTransformer()),\n",
       "                                             (&#x27;principal_components&#x27;,\n",
       "                                              PCA(n_components=20,\n",
       "                                                  svd_solver=&#x27;full&#x27;)),\n",
       "                                             (&#x27;mlp&#x27;,\n",
       "                                              KerasRegressor(callbacks=[&lt;keras.callbacks.EarlyStopping object at 0x7fd76c713640&gt;], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss=&#x27;mse&#x27;, model=&lt;functio...\n",
       "                                                         (4, 4, 4, 4, 32, 32),\n",
       "                                                         (4, 4, 4, 4, 32, 64),\n",
       "                                                         (4, 4, 4, 4, 64, 4),\n",
       "                                                         (4, 4, 4, 4, 64, 8),\n",
       "                                                         (4, 4, 4, 4, 64, 16),\n",
       "                                                         (4, 4, 4, 4, 64, 32),\n",
       "                                                         (4, 4, 4, 4, 64, 64),\n",
       "                                                         (4, 4, 4, 8, 4, 4),\n",
       "                                                         (4, 4, 4, 8, 4, 8),\n",
       "                                                         (4, 4, 4, 8, 4, 16),\n",
       "                                                         (4, 4, 4, 8, 4, 32),\n",
       "                                                         (4, 4, 4, 8, 4, 64), ...],\n",
       "                                        &#x27;mlp__nlayers&#x27;: array([1, 6, 3, ..., 5, 3, 3]),\n",
       "                                        &#x27;mlp__optimizer__learning_rate&#x27;: [0.0001,\n",
       "                                                                          0.001,\n",
       "                                                                          0.005]},\n",
       "                   refit=False, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, PowerTransformer()),\n",
       "                (&#x27;principal_components&#x27;,\n",
       "                 PCA(n_components=20, svd_solver=&#x27;full&#x27;)),\n",
       "                (&#x27;mlp&#x27;,\n",
       "                 KerasRegressor(callbacks=[&lt;keras.callbacks.EarlyStopping object at 0x7fd76c713640&gt;], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss=&#x27;mse&#x27;, model=&lt;function twoLayerFeedForward at 0x7fd708e42790&gt;, nlayers=3, validation_split=0.2, verbose=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PowerTransformer</label><div class=\"sk-toggleable__content\"><pre>PowerTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=20, svd_solver=&#x27;full&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function twoLayerFeedForward at 0x7fd708e42790&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=mse\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=[&lt;keras.callbacks.EarlyStopping object at 0x7fd76c713640&gt;]\n",
       "\tvalidation_split=0.2\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=200\n",
       "\tnlayers=3\n",
       "\thiddens=[2, 2, 2]\n",
       "\tdropouts=[0.2, 0, 0]\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[('scaler', PowerTransformer()),\n",
       "                                             ('principal_components',\n",
       "                                              PCA(n_components=20,\n",
       "                                                  svd_solver='full')),\n",
       "                                             ('mlp',\n",
       "                                              KerasRegressor(callbacks=[<keras.callbacks.EarlyStopping object at 0x7fd76c713640>], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss='mse', model=<functio...\n",
       "                                                         (4, 4, 4, 4, 32, 32),\n",
       "                                                         (4, 4, 4, 4, 32, 64),\n",
       "                                                         (4, 4, 4, 4, 64, 4),\n",
       "                                                         (4, 4, 4, 4, 64, 8),\n",
       "                                                         (4, 4, 4, 4, 64, 16),\n",
       "                                                         (4, 4, 4, 4, 64, 32),\n",
       "                                                         (4, 4, 4, 4, 64, 64),\n",
       "                                                         (4, 4, 4, 8, 4, 4),\n",
       "                                                         (4, 4, 4, 8, 4, 8),\n",
       "                                                         (4, 4, 4, 8, 4, 16),\n",
       "                                                         (4, 4, 4, 8, 4, 32),\n",
       "                                                         (4, 4, 4, 8, 4, 64), ...],\n",
       "                                        'mlp__nlayers': array([1, 6, 3, ..., 5, 3, 3]),\n",
       "                                        'mlp__optimizer__learning_rate': [0.0001,\n",
       "                                                                          0.001,\n",
       "                                                                          0.005]},\n",
       "                   refit=False, return_train_score=True,\n",
       "                   scoring='neg_mean_absolute_error', verbose=3)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(df.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_overfit = df_results[\"mean_train_score\"] - df_results[\"mean_test_score\"] < 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_mlp__optimizer__learning_rate</th>\n",
       "      <th>param_mlp__nlayers</th>\n",
       "      <th>param_mlp__hiddens</th>\n",
       "      <th>param_mlp__dropouts</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.381248</td>\n",
       "      <td>0.038405</td>\n",
       "      <td>0.068078</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 64, 8, 4, 8, 16)</td>\n",
       "      <td>[0.1, 0.0, 0.2, 0.0, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.444679</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.245788</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.123399</td>\n",
       "      <td>-2.881489</td>\n",
       "      <td>-3.200226</td>\n",
       "      <td>-3.204078</td>\n",
       "      <td>-2.883873</td>\n",
       "      <td>-3.058613</td>\n",
       "      <td>0.146505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2.399896</td>\n",
       "      <td>0.283920</td>\n",
       "      <td>0.072665</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 4, 32, 8, 8, 16)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.1, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.502558</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.306568</td>\n",
       "      <td>0.217319</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.202178</td>\n",
       "      <td>-3.265401</td>\n",
       "      <td>-3.032918</td>\n",
       "      <td>-3.439771</td>\n",
       "      <td>-3.160560</td>\n",
       "      <td>-3.220165</td>\n",
       "      <td>0.133575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.196659</td>\n",
       "      <td>0.183173</td>\n",
       "      <td>0.065548</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 32, 16, 8, 32, 4)</td>\n",
       "      <td>[0.1, 0.2, 0.2, 0.0, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.510855</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.390546</td>\n",
       "      <td>0.302877</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.142341</td>\n",
       "      <td>-3.310471</td>\n",
       "      <td>-3.101906</td>\n",
       "      <td>-3.179767</td>\n",
       "      <td>-3.135069</td>\n",
       "      <td>-3.173911</td>\n",
       "      <td>0.072626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.448093</td>\n",
       "      <td>0.160615</td>\n",
       "      <td>0.075976</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 64, 8, 32, 16, 16)</td>\n",
       "      <td>[0.2, 0.0, 0.1, 0.0, 0.3, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.207207</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.451918</td>\n",
       "      <td>0.515716</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.128185</td>\n",
       "      <td>-3.862343</td>\n",
       "      <td>-3.072557</td>\n",
       "      <td>-3.394035</td>\n",
       "      <td>-3.495929</td>\n",
       "      <td>-3.390610</td>\n",
       "      <td>0.284134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.277616</td>\n",
       "      <td>0.060783</td>\n",
       "      <td>0.070754</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(16, 16, 32, 8, 32, 4)</td>\n",
       "      <td>[0.2, 0.1, 0.1, 0.0, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.679821</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.457536</td>\n",
       "      <td>0.185252</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.400886</td>\n",
       "      <td>-2.870856</td>\n",
       "      <td>-3.252017</td>\n",
       "      <td>-3.121352</td>\n",
       "      <td>-3.304009</td>\n",
       "      <td>-3.189824</td>\n",
       "      <td>0.183239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2.156716</td>\n",
       "      <td>0.165973</td>\n",
       "      <td>0.071492</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 16, 32, 8, 4, 8)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.3, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.738486</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.482312</td>\n",
       "      <td>0.314462</td>\n",
       "      <td>9</td>\n",
       "      <td>-3.206298</td>\n",
       "      <td>-3.450732</td>\n",
       "      <td>-3.505153</td>\n",
       "      <td>-3.251069</td>\n",
       "      <td>-3.434089</td>\n",
       "      <td>-3.369468</td>\n",
       "      <td>0.118180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.747765</td>\n",
       "      <td>0.127198</td>\n",
       "      <td>0.065332</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 64, 32, 8, 8, 8)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.3, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.356810</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.491877</td>\n",
       "      <td>0.297740</td>\n",
       "      <td>10</td>\n",
       "      <td>-3.032396</td>\n",
       "      <td>-3.292912</td>\n",
       "      <td>-3.188334</td>\n",
       "      <td>-3.490692</td>\n",
       "      <td>-3.274313</td>\n",
       "      <td>-3.255729</td>\n",
       "      <td>0.149235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.272680</td>\n",
       "      <td>0.062594</td>\n",
       "      <td>0.078090</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(64, 16, 64, 4, 32, 8)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.0, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.984387</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.511034</td>\n",
       "      <td>0.294358</td>\n",
       "      <td>12</td>\n",
       "      <td>-3.639168</td>\n",
       "      <td>-3.225021</td>\n",
       "      <td>-3.512329</td>\n",
       "      <td>-3.147330</td>\n",
       "      <td>-2.962338</td>\n",
       "      <td>-3.297237</td>\n",
       "      <td>0.246181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.446125</td>\n",
       "      <td>0.064014</td>\n",
       "      <td>0.076042</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 32, 16, 64, 4, 4)</td>\n",
       "      <td>[0.0, 0.2, 0.2, 0.2, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.772745</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.548365</td>\n",
       "      <td>0.294348</td>\n",
       "      <td>14</td>\n",
       "      <td>-3.605911</td>\n",
       "      <td>-3.195032</td>\n",
       "      <td>-3.059955</td>\n",
       "      <td>-3.527445</td>\n",
       "      <td>-3.104705</td>\n",
       "      <td>-3.298610</td>\n",
       "      <td>0.224536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.121251</td>\n",
       "      <td>0.054270</td>\n",
       "      <td>0.064064</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 4, 32, 4, 32, 32)</td>\n",
       "      <td>[0.1, 0.2, 0.0, 0.1, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.801082</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.600028</td>\n",
       "      <td>0.317610</td>\n",
       "      <td>22</td>\n",
       "      <td>-3.483200</td>\n",
       "      <td>-3.011451</td>\n",
       "      <td>-3.564499</td>\n",
       "      <td>-3.021721</td>\n",
       "      <td>-3.536013</td>\n",
       "      <td>-3.323377</td>\n",
       "      <td>0.251870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2.074067</td>\n",
       "      <td>0.299716</td>\n",
       "      <td>0.068131</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(8, 16, 4, 16, 4, 16)</td>\n",
       "      <td>[0.0, 0.1, 0.1, 0.0, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.748797</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.669659</td>\n",
       "      <td>0.471889</td>\n",
       "      <td>28</td>\n",
       "      <td>-3.303509</td>\n",
       "      <td>-3.469351</td>\n",
       "      <td>-3.097847</td>\n",
       "      <td>-3.089463</td>\n",
       "      <td>-4.086803</td>\n",
       "      <td>-3.409395</td>\n",
       "      <td>0.366908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.484825</td>\n",
       "      <td>0.137363</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 16, 8, 32, 16, 8)</td>\n",
       "      <td>[0.0, 0.2, 0.2, 0.3, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.194103</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.683814</td>\n",
       "      <td>0.474299</td>\n",
       "      <td>30</td>\n",
       "      <td>-2.969188</td>\n",
       "      <td>-2.899978</td>\n",
       "      <td>-3.489649</td>\n",
       "      <td>-4.435165</td>\n",
       "      <td>-3.310104</td>\n",
       "      <td>-3.420817</td>\n",
       "      <td>0.551568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.545654</td>\n",
       "      <td>0.200134</td>\n",
       "      <td>0.069848</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 4, 8, 16, 64, 16)</td>\n",
       "      <td>[0.0, 0.3, 0.2, 0.2, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.032763</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.724189</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>34</td>\n",
       "      <td>-4.076120</td>\n",
       "      <td>-3.204912</td>\n",
       "      <td>-3.280440</td>\n",
       "      <td>-3.612611</td>\n",
       "      <td>-3.803830</td>\n",
       "      <td>-3.595583</td>\n",
       "      <td>0.324503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.407048</td>\n",
       "      <td>0.058881</td>\n",
       "      <td>0.081968</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 64, 8, 32, 64, 16)</td>\n",
       "      <td>[0.2, 0.3, 0.0, 0.2, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.286118</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.727956</td>\n",
       "      <td>0.334774</td>\n",
       "      <td>35</td>\n",
       "      <td>-3.708094</td>\n",
       "      <td>-3.132101</td>\n",
       "      <td>-3.468539</td>\n",
       "      <td>-3.515535</td>\n",
       "      <td>-3.352134</td>\n",
       "      <td>-3.435281</td>\n",
       "      <td>0.190161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.290341</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>0.069295</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(16, 32, 64, 32, 16, 64)</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0.0, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.278897</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.730299</td>\n",
       "      <td>0.433099</td>\n",
       "      <td>37</td>\n",
       "      <td>-3.993858</td>\n",
       "      <td>-3.169882</td>\n",
       "      <td>-2.808735</td>\n",
       "      <td>-4.060848</td>\n",
       "      <td>-3.607992</td>\n",
       "      <td>-3.528263</td>\n",
       "      <td>0.480196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.530041</td>\n",
       "      <td>0.246018</td>\n",
       "      <td>0.079734</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 16, 8, 16, 64, 8)</td>\n",
       "      <td>[0.0, 0.3, 0.3, 0.2, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.242633</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.836662</td>\n",
       "      <td>0.221970</td>\n",
       "      <td>45</td>\n",
       "      <td>-4.394014</td>\n",
       "      <td>-3.067084</td>\n",
       "      <td>-3.593900</td>\n",
       "      <td>-3.970299</td>\n",
       "      <td>-3.805948</td>\n",
       "      <td>-3.766249</td>\n",
       "      <td>0.437227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.502557</td>\n",
       "      <td>0.107721</td>\n",
       "      <td>0.079624</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(32, 16, 64, 64, 32, 32)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.3, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.260488</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.864190</td>\n",
       "      <td>0.312776</td>\n",
       "      <td>47</td>\n",
       "      <td>-4.088623</td>\n",
       "      <td>-3.439558</td>\n",
       "      <td>-3.575928</td>\n",
       "      <td>-3.251769</td>\n",
       "      <td>-3.628871</td>\n",
       "      <td>-3.596950</td>\n",
       "      <td>0.278253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.379140</td>\n",
       "      <td>0.088124</td>\n",
       "      <td>0.079533</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 8, 32, 8, 8, 16)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.2, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.836822</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.921562</td>\n",
       "      <td>0.382886</td>\n",
       "      <td>49</td>\n",
       "      <td>-3.577503</td>\n",
       "      <td>-3.824137</td>\n",
       "      <td>-3.555268</td>\n",
       "      <td>-3.613887</td>\n",
       "      <td>-3.733543</td>\n",
       "      <td>-3.660868</td>\n",
       "      <td>0.102222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.781326</td>\n",
       "      <td>0.124139</td>\n",
       "      <td>0.078681</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(4, 32, 16, 32, 64, 32)</td>\n",
       "      <td>[0.0, 0.2, 0.0, 0.0, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.841363</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.946563</td>\n",
       "      <td>0.250703</td>\n",
       "      <td>50</td>\n",
       "      <td>-3.658225</td>\n",
       "      <td>-3.306484</td>\n",
       "      <td>-3.938358</td>\n",
       "      <td>-4.235332</td>\n",
       "      <td>-3.827102</td>\n",
       "      <td>-3.793100</td>\n",
       "      <td>0.307510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.078301</td>\n",
       "      <td>0.350160</td>\n",
       "      <td>0.069616</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(16, 8, 16, 16, 16, 4)</td>\n",
       "      <td>[0.1, 0.2, 0.3, 0.0, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.233727</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.979165</td>\n",
       "      <td>0.257731</td>\n",
       "      <td>52</td>\n",
       "      <td>-3.806590</td>\n",
       "      <td>-3.512726</td>\n",
       "      <td>-3.832004</td>\n",
       "      <td>-3.933008</td>\n",
       "      <td>-3.433693</td>\n",
       "      <td>-3.703604</td>\n",
       "      <td>0.194425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.577962</td>\n",
       "      <td>0.328636</td>\n",
       "      <td>0.085771</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(32, 16, 32, 16, 64, 8)</td>\n",
       "      <td>[0.1, 0.3, 0.3, 0.3, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.929504</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.998526</td>\n",
       "      <td>0.731424</td>\n",
       "      <td>54</td>\n",
       "      <td>-4.026567</td>\n",
       "      <td>-3.470867</td>\n",
       "      <td>-3.408566</td>\n",
       "      <td>-3.492128</td>\n",
       "      <td>-5.077090</td>\n",
       "      <td>-3.895044</td>\n",
       "      <td>0.631421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.647439</td>\n",
       "      <td>0.057403</td>\n",
       "      <td>0.074555</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 64, 8, 16, 16, 4)</td>\n",
       "      <td>[0.0, 0.0, 0.1, 0.0, 0.1, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.986691</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.019573</td>\n",
       "      <td>0.230108</td>\n",
       "      <td>55</td>\n",
       "      <td>-3.509549</td>\n",
       "      <td>-3.720099</td>\n",
       "      <td>-3.881995</td>\n",
       "      <td>-3.830726</td>\n",
       "      <td>-4.027582</td>\n",
       "      <td>-3.793990</td>\n",
       "      <td>0.173220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.262394</td>\n",
       "      <td>1.964590</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(64, 4, 64, 4, 4, 32)</td>\n",
       "      <td>[0.0, 0.1, 0.1, 0.3, 0.2, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.683450</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.068708</td>\n",
       "      <td>0.881039</td>\n",
       "      <td>56</td>\n",
       "      <td>-3.042402</td>\n",
       "      <td>-3.743024</td>\n",
       "      <td>-3.109341</td>\n",
       "      <td>-3.579479</td>\n",
       "      <td>-6.030831</td>\n",
       "      <td>-3.901015</td>\n",
       "      <td>1.098037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.819248</td>\n",
       "      <td>0.434627</td>\n",
       "      <td>0.066853</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(8, 64, 32, 16, 32, 32)</td>\n",
       "      <td>[0.3, 0.0, 0.3, 0.3, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.931859</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.073323</td>\n",
       "      <td>0.342279</td>\n",
       "      <td>57</td>\n",
       "      <td>-3.933999</td>\n",
       "      <td>-4.095889</td>\n",
       "      <td>-3.454255</td>\n",
       "      <td>-3.810474</td>\n",
       "      <td>-4.124846</td>\n",
       "      <td>-3.883893</td>\n",
       "      <td>0.243101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.340920</td>\n",
       "      <td>0.196845</td>\n",
       "      <td>0.068869</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "      <td>(8, 4, 8, 16, 8, 4)</td>\n",
       "      <td>[0.1, 0.3, 0.2, 0.2, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.709024</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.138777</td>\n",
       "      <td>0.724627</td>\n",
       "      <td>60</td>\n",
       "      <td>-3.604376</td>\n",
       "      <td>-2.993532</td>\n",
       "      <td>-5.108695</td>\n",
       "      <td>-3.939392</td>\n",
       "      <td>-3.920404</td>\n",
       "      <td>-3.913280</td>\n",
       "      <td>0.688637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.262931</td>\n",
       "      <td>0.065133</td>\n",
       "      <td>0.073932</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 8, 64, 8, 32, 32)</td>\n",
       "      <td>[0.3, 0.0, 0.2, 0.3, 0.0, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-5.431517</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.160269</td>\n",
       "      <td>0.842450</td>\n",
       "      <td>61</td>\n",
       "      <td>-5.637015</td>\n",
       "      <td>-4.289149</td>\n",
       "      <td>-3.363489</td>\n",
       "      <td>-4.305180</td>\n",
       "      <td>-3.246187</td>\n",
       "      <td>-4.168204</td>\n",
       "      <td>0.858892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.517129</td>\n",
       "      <td>0.100984</td>\n",
       "      <td>0.075037</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(64, 4, 8, 8, 64, 8)</td>\n",
       "      <td>[0.0, 0.2, 0.0, 0.1, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.883612</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.188626</td>\n",
       "      <td>0.494258</td>\n",
       "      <td>62</td>\n",
       "      <td>-3.632736</td>\n",
       "      <td>-3.861969</td>\n",
       "      <td>-3.327079</td>\n",
       "      <td>-4.332283</td>\n",
       "      <td>-4.422741</td>\n",
       "      <td>-3.915362</td>\n",
       "      <td>0.414744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.310591</td>\n",
       "      <td>0.159938</td>\n",
       "      <td>0.068115</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "      <td>(8, 16, 4, 16, 16, 64)</td>\n",
       "      <td>[0.1, 0.0, 0.3, 0.0, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.868113</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.378028</td>\n",
       "      <td>0.732928</td>\n",
       "      <td>66</td>\n",
       "      <td>-3.780779</td>\n",
       "      <td>-2.866882</td>\n",
       "      <td>-4.889332</td>\n",
       "      <td>-4.257324</td>\n",
       "      <td>-5.248864</td>\n",
       "      <td>-4.208636</td>\n",
       "      <td>0.840376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.399842</td>\n",
       "      <td>0.570349</td>\n",
       "      <td>0.071272</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 4, 32, 8, 8, 4)</td>\n",
       "      <td>[0.3, 0.1, 0.0, 0.3, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.713081</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.629266</td>\n",
       "      <td>0.699482</td>\n",
       "      <td>67</td>\n",
       "      <td>-3.792214</td>\n",
       "      <td>-4.001750</td>\n",
       "      <td>-3.996552</td>\n",
       "      <td>-5.039602</td>\n",
       "      <td>-5.944006</td>\n",
       "      <td>-4.554825</td>\n",
       "      <td>0.820240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.752546</td>\n",
       "      <td>0.231012</td>\n",
       "      <td>0.076944</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 64, 16, 64, 4, 16)</td>\n",
       "      <td>[0.1, 0.3, 0.3, 0.1, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.959479</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.641373</td>\n",
       "      <td>0.412405</td>\n",
       "      <td>68</td>\n",
       "      <td>-4.600227</td>\n",
       "      <td>-3.691846</td>\n",
       "      <td>-4.298586</td>\n",
       "      <td>-4.591487</td>\n",
       "      <td>-5.152407</td>\n",
       "      <td>-4.466911</td>\n",
       "      <td>0.475869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.769110</td>\n",
       "      <td>0.134735</td>\n",
       "      <td>0.079284</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(64, 8, 64, 32, 4, 64)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.2, 0.1, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.518302</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.710017</td>\n",
       "      <td>0.157673</td>\n",
       "      <td>69</td>\n",
       "      <td>-4.262499</td>\n",
       "      <td>-4.325011</td>\n",
       "      <td>-4.487673</td>\n",
       "      <td>-4.733686</td>\n",
       "      <td>-4.507604</td>\n",
       "      <td>-4.463295</td>\n",
       "      <td>0.164384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.372091</td>\n",
       "      <td>0.345872</td>\n",
       "      <td>0.057939</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(32, 8, 8, 4, 16, 4)</td>\n",
       "      <td>[0.3, 0.0, 0.1, 0.1, 0.0, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.562274</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.735709</td>\n",
       "      <td>0.472276</td>\n",
       "      <td>70</td>\n",
       "      <td>-4.626635</td>\n",
       "      <td>-4.195252</td>\n",
       "      <td>-4.067453</td>\n",
       "      <td>-4.284728</td>\n",
       "      <td>-5.144372</td>\n",
       "      <td>-4.463688</td>\n",
       "      <td>0.387548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.520440</td>\n",
       "      <td>0.151489</td>\n",
       "      <td>0.080155</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 32, 8, 8, 32, 16)</td>\n",
       "      <td>[0.0, 0.0, 0.1, 0.3, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.315631</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.745196</td>\n",
       "      <td>0.339333</td>\n",
       "      <td>71</td>\n",
       "      <td>-3.899865</td>\n",
       "      <td>-4.994592</td>\n",
       "      <td>-4.869039</td>\n",
       "      <td>-4.365901</td>\n",
       "      <td>-4.432202</td>\n",
       "      <td>-4.512320</td>\n",
       "      <td>0.390587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.577236</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.070861</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(64, 16, 4, 4, 4, 64)</td>\n",
       "      <td>[0.0, 0.3, 0.0, 0.1, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.147595</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.745275</td>\n",
       "      <td>0.434045</td>\n",
       "      <td>72</td>\n",
       "      <td>-5.032434</td>\n",
       "      <td>-4.783619</td>\n",
       "      <td>-4.198656</td>\n",
       "      <td>-3.924726</td>\n",
       "      <td>-4.367122</td>\n",
       "      <td>-4.461311</td>\n",
       "      <td>0.398960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4.654827</td>\n",
       "      <td>1.034928</td>\n",
       "      <td>0.075963</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 32, 4, 32, 4, 32)</td>\n",
       "      <td>[0.1, 0.1, 0.0, 0.0, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.205419</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.819183</td>\n",
       "      <td>1.077468</td>\n",
       "      <td>73</td>\n",
       "      <td>-4.161263</td>\n",
       "      <td>-6.743170</td>\n",
       "      <td>-4.453614</td>\n",
       "      <td>-3.723066</td>\n",
       "      <td>-4.294684</td>\n",
       "      <td>-4.675159</td>\n",
       "      <td>1.062175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4.841628</td>\n",
       "      <td>0.642366</td>\n",
       "      <td>0.071950</td>\n",
       "      <td>0.009444</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 8, 64, 64, 16, 4)</td>\n",
       "      <td>[0.2, 0.1, 0.0, 0.0, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.032173</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.875657</td>\n",
       "      <td>0.388905</td>\n",
       "      <td>74</td>\n",
       "      <td>-4.985263</td>\n",
       "      <td>-5.051864</td>\n",
       "      <td>-4.647574</td>\n",
       "      <td>-4.604249</td>\n",
       "      <td>-4.588098</td>\n",
       "      <td>-4.775410</td>\n",
       "      <td>0.200594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.870517</td>\n",
       "      <td>0.275743</td>\n",
       "      <td>0.065118</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>(16, 4, 16, 8, 32, 8)</td>\n",
       "      <td>[0.1, 0.0, 0.3, 0.2, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.627315</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.918170</td>\n",
       "      <td>0.366687</td>\n",
       "      <td>75</td>\n",
       "      <td>-4.383821</td>\n",
       "      <td>-4.264166</td>\n",
       "      <td>-5.404635</td>\n",
       "      <td>-4.585191</td>\n",
       "      <td>-4.508484</td>\n",
       "      <td>-4.629259</td>\n",
       "      <td>0.402807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4.626636</td>\n",
       "      <td>1.202737</td>\n",
       "      <td>0.072876</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 16, 64, 64, 64, 16)</td>\n",
       "      <td>[0.1, 0.2, 0.0, 0.3, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-6.522488</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.918541</td>\n",
       "      <td>0.844340</td>\n",
       "      <td>76</td>\n",
       "      <td>-6.403205</td>\n",
       "      <td>-3.755758</td>\n",
       "      <td>-3.805934</td>\n",
       "      <td>-4.307264</td>\n",
       "      <td>-5.557701</td>\n",
       "      <td>-4.765973</td>\n",
       "      <td>1.044987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3.711105</td>\n",
       "      <td>0.864177</td>\n",
       "      <td>0.078757</td>\n",
       "      <td>0.005837</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 64, 32, 4, 4, 8)</td>\n",
       "      <td>[0.3, 0.3, 0.0, 0.1, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.027965</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.931777</td>\n",
       "      <td>0.226004</td>\n",
       "      <td>77</td>\n",
       "      <td>-5.064423</td>\n",
       "      <td>-4.292847</td>\n",
       "      <td>-4.596335</td>\n",
       "      <td>-4.934298</td>\n",
       "      <td>-4.610921</td>\n",
       "      <td>-4.699765</td>\n",
       "      <td>0.272820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.205330</td>\n",
       "      <td>0.662003</td>\n",
       "      <td>0.075952</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 4, 32, 16, 8, 64)</td>\n",
       "      <td>[0.0, 0.0, 0.3, 0.0, 0.3, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.209240</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.936507</td>\n",
       "      <td>0.429749</td>\n",
       "      <td>78</td>\n",
       "      <td>-4.749343</td>\n",
       "      <td>-4.827422</td>\n",
       "      <td>-3.885274</td>\n",
       "      <td>-4.718909</td>\n",
       "      <td>-5.010941</td>\n",
       "      <td>-4.638378</td>\n",
       "      <td>0.390001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.870194</td>\n",
       "      <td>0.267567</td>\n",
       "      <td>0.070096</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(32, 4, 4, 32, 8, 32)</td>\n",
       "      <td>[0.2, 0.3, 0.1, 0.0, 0.2, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.693189</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.945885</td>\n",
       "      <td>0.592438</td>\n",
       "      <td>79</td>\n",
       "      <td>-4.537680</td>\n",
       "      <td>-3.665157</td>\n",
       "      <td>-4.578929</td>\n",
       "      <td>-5.228545</td>\n",
       "      <td>-5.610092</td>\n",
       "      <td>-4.724081</td>\n",
       "      <td>0.665828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.615868</td>\n",
       "      <td>0.196203</td>\n",
       "      <td>0.078078</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(64, 8, 32, 64, 8, 64)</td>\n",
       "      <td>[0.2, 0.3, 0.0, 0.3, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.366928</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.263866</td>\n",
       "      <td>0.220901</td>\n",
       "      <td>81</td>\n",
       "      <td>-5.123247</td>\n",
       "      <td>-4.947372</td>\n",
       "      <td>-4.975546</td>\n",
       "      <td>-5.205015</td>\n",
       "      <td>-5.030421</td>\n",
       "      <td>-5.056320</td>\n",
       "      <td>0.095556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.737277</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.083777</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 16, 4, 32, 64, 8)</td>\n",
       "      <td>[0.3, 0.3, 0.0, 0.0, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.464281</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.447851</td>\n",
       "      <td>0.084832</td>\n",
       "      <td>82</td>\n",
       "      <td>-5.809070</td>\n",
       "      <td>-4.933755</td>\n",
       "      <td>-5.593369</td>\n",
       "      <td>-5.546139</td>\n",
       "      <td>-5.232346</td>\n",
       "      <td>-5.422936</td>\n",
       "      <td>0.306251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6.096340</td>\n",
       "      <td>0.386804</td>\n",
       "      <td>0.069647</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(8, 4, 8, 32, 16, 32)</td>\n",
       "      <td>[0.1, 0.0, 0.3, 0.3, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.900061</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.517619</td>\n",
       "      <td>0.534699</td>\n",
       "      <td>83</td>\n",
       "      <td>-5.819024</td>\n",
       "      <td>-4.232330</td>\n",
       "      <td>-5.493301</td>\n",
       "      <td>-5.591993</td>\n",
       "      <td>-5.701148</td>\n",
       "      <td>-5.367559</td>\n",
       "      <td>0.577932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.274352</td>\n",
       "      <td>0.052584</td>\n",
       "      <td>0.077322</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 16, 8, 4, 32, 4)</td>\n",
       "      <td>[0.0, 0.3, 0.2, 0.2, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-5.245757</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.622228</td>\n",
       "      <td>1.165520</td>\n",
       "      <td>84</td>\n",
       "      <td>-4.948552</td>\n",
       "      <td>-4.412811</td>\n",
       "      <td>-5.256732</td>\n",
       "      <td>-7.782584</td>\n",
       "      <td>-4.657990</td>\n",
       "      <td>-5.411734</td>\n",
       "      <td>1.218644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.349942</td>\n",
       "      <td>0.441901</td>\n",
       "      <td>0.082150</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 64, 16, 4, 8, 64)</td>\n",
       "      <td>[0.3, 0.1, 0.3, 0.0, 0.1, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.407163</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.664937</td>\n",
       "      <td>0.325811</td>\n",
       "      <td>86</td>\n",
       "      <td>-5.151940</td>\n",
       "      <td>-5.793719</td>\n",
       "      <td>-5.084618</td>\n",
       "      <td>-5.509892</td>\n",
       "      <td>-5.740507</td>\n",
       "      <td>-5.456135</td>\n",
       "      <td>0.292670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3.276229</td>\n",
       "      <td>0.806072</td>\n",
       "      <td>0.220423</td>\n",
       "      <td>0.285118</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 64, 8, 8, 64, 64)</td>\n",
       "      <td>[0.2, 0.2, 0.0, 0.2, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.294130</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.680199</td>\n",
       "      <td>0.625255</td>\n",
       "      <td>87</td>\n",
       "      <td>-5.511517</td>\n",
       "      <td>-6.286701</td>\n",
       "      <td>-5.946515</td>\n",
       "      <td>-5.070109</td>\n",
       "      <td>-5.157332</td>\n",
       "      <td>-5.594435</td>\n",
       "      <td>0.463906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.875356</td>\n",
       "      <td>1.411328</td>\n",
       "      <td>0.064165</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(8, 32, 4, 8, 16, 16)</td>\n",
       "      <td>[0.1, 0.0, 0.1, 0.0, 0.2, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.760444</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.733932</td>\n",
       "      <td>4.187808</td>\n",
       "      <td>88</td>\n",
       "      <td>-3.519809</td>\n",
       "      <td>-13.532279</td>\n",
       "      <td>-3.066280</td>\n",
       "      <td>-3.308044</td>\n",
       "      <td>-3.837000</td>\n",
       "      <td>-5.452682</td>\n",
       "      <td>4.047733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.669213</td>\n",
       "      <td>0.150287</td>\n",
       "      <td>0.066952</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 16, 32, 4, 8, 8)</td>\n",
       "      <td>[0.2, 0.3, 0.0, 0.3, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.544407</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.037519</td>\n",
       "      <td>0.883102</td>\n",
       "      <td>89</td>\n",
       "      <td>-5.679337</td>\n",
       "      <td>-5.053432</td>\n",
       "      <td>-7.487936</td>\n",
       "      <td>-5.580757</td>\n",
       "      <td>-5.893959</td>\n",
       "      <td>-5.939084</td>\n",
       "      <td>0.822341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.952600</td>\n",
       "      <td>1.658953</td>\n",
       "      <td>0.077643</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 4, 4, 32, 64, 4)</td>\n",
       "      <td>[0.3, 0.3, 0.1, 0.2, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.724928</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.190416</td>\n",
       "      <td>3.475616</td>\n",
       "      <td>90</td>\n",
       "      <td>-5.013691</td>\n",
       "      <td>-4.084607</td>\n",
       "      <td>-4.033420</td>\n",
       "      <td>-4.245999</td>\n",
       "      <td>-13.772973</td>\n",
       "      <td>-6.230138</td>\n",
       "      <td>3.787870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2.814177</td>\n",
       "      <td>0.214926</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 32, 64, 4, 32, 4)</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0.3, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.616088</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.564017</td>\n",
       "      <td>0.729446</td>\n",
       "      <td>91</td>\n",
       "      <td>-5.788209</td>\n",
       "      <td>-6.216347</td>\n",
       "      <td>-6.913198</td>\n",
       "      <td>-7.088507</td>\n",
       "      <td>-5.635606</td>\n",
       "      <td>-6.328374</td>\n",
       "      <td>0.583794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4.946305</td>\n",
       "      <td>0.890021</td>\n",
       "      <td>0.074963</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(16, 16, 16, 16, 4, 16)</td>\n",
       "      <td>[0.3, 0.0, 0.0, 0.3, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.973960</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.500001</td>\n",
       "      <td>4.313533</td>\n",
       "      <td>92</td>\n",
       "      <td>-4.826792</td>\n",
       "      <td>-5.119690</td>\n",
       "      <td>-5.563558</td>\n",
       "      <td>-16.146329</td>\n",
       "      <td>-4.837408</td>\n",
       "      <td>-7.298756</td>\n",
       "      <td>4.431876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5.057990</td>\n",
       "      <td>1.269600</td>\n",
       "      <td>0.065355</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 64, 4, 16, 4, 4)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-7.938048</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.148529</td>\n",
       "      <td>4.380331</td>\n",
       "      <td>93</td>\n",
       "      <td>-7.774635</td>\n",
       "      <td>-6.167044</td>\n",
       "      <td>-4.067231</td>\n",
       "      <td>-5.443891</td>\n",
       "      <td>-17.179002</td>\n",
       "      <td>-8.126360</td>\n",
       "      <td>4.681562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.307224</td>\n",
       "      <td>0.539358</td>\n",
       "      <td>0.077992</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 4, 8, 32, 4, 4)</td>\n",
       "      <td>[0.1, 0.0, 0.0, 0.0, 0.3, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.654573</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.688800</td>\n",
       "      <td>5.616778</td>\n",
       "      <td>94</td>\n",
       "      <td>-5.493066</td>\n",
       "      <td>-16.939133</td>\n",
       "      <td>-16.907280</td>\n",
       "      <td>-6.184639</td>\n",
       "      <td>-6.661172</td>\n",
       "      <td>-10.437058</td>\n",
       "      <td>5.308939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.765096</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.067367</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 4, 4, 16, 16, 8)</td>\n",
       "      <td>[0.3, 0.2, 0.1, 0.3, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-9.025707</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.212495</td>\n",
       "      <td>2.713143</td>\n",
       "      <td>95</td>\n",
       "      <td>-9.846847</td>\n",
       "      <td>-8.126998</td>\n",
       "      <td>-11.089035</td>\n",
       "      <td>-16.278426</td>\n",
       "      <td>-10.789633</td>\n",
       "      <td>-11.226188</td>\n",
       "      <td>2.729178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.990242</td>\n",
       "      <td>0.602887</td>\n",
       "      <td>0.066875</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 8, 4, 16, 4, 32)</td>\n",
       "      <td>[0.3, 0.3, 0.2, 0.3, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-9.293062</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.252686</td>\n",
       "      <td>3.258617</td>\n",
       "      <td>96</td>\n",
       "      <td>-9.939363</td>\n",
       "      <td>-8.807537</td>\n",
       "      <td>-14.355163</td>\n",
       "      <td>-7.149443</td>\n",
       "      <td>-16.180493</td>\n",
       "      <td>-11.286400</td>\n",
       "      <td>3.418864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.490373</td>\n",
       "      <td>0.065778</td>\n",
       "      <td>0.064155</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 4, 4, 4, 64, 64)</td>\n",
       "      <td>[0.2, 0.1, 0.2, 0.3, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-16.659912</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.724782</td>\n",
       "      <td>3.108373</td>\n",
       "      <td>97</td>\n",
       "      <td>-17.147026</td>\n",
       "      <td>-9.507247</td>\n",
       "      <td>-8.190365</td>\n",
       "      <td>-13.952026</td>\n",
       "      <td>-9.187030</td>\n",
       "      <td>-11.596739</td>\n",
       "      <td>3.409666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.117471</td>\n",
       "      <td>0.049489</td>\n",
       "      <td>0.057340</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(4, 4, 4, 16, 8, 16)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.3, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-9.094137</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.236092</td>\n",
       "      <td>2.430642</td>\n",
       "      <td>98</td>\n",
       "      <td>-9.211746</td>\n",
       "      <td>-10.256436</td>\n",
       "      <td>-13.314954</td>\n",
       "      <td>-11.208554</td>\n",
       "      <td>-16.838069</td>\n",
       "      <td>-12.165952</td>\n",
       "      <td>2.699595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.744885</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.064530</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>(4, 4, 64, 8, 16, 64)</td>\n",
       "      <td>[0.2, 0.2, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-8.217524</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.360122</td>\n",
       "      <td>2.435588</td>\n",
       "      <td>99</td>\n",
       "      <td>-8.452629</td>\n",
       "      <td>-10.679656</td>\n",
       "      <td>-12.443422</td>\n",
       "      <td>-14.927909</td>\n",
       "      <td>-14.464075</td>\n",
       "      <td>-12.193538</td>\n",
       "      <td>2.407522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "90       1.381248      0.038405         0.068078        0.005896   \n",
       "47       2.399896      0.283920         0.072665        0.002338   \n",
       "6        2.196659      0.183173         0.065548        0.001259   \n",
       "44       1.448093      0.160615         0.075976        0.004562   \n",
       "8        1.277616      0.060783         0.070754        0.003336   \n",
       "63       2.156716      0.165973         0.071492        0.001492   \n",
       "83       1.747765      0.127198         0.065332        0.003197   \n",
       "10       1.272680      0.062594         0.078090        0.004779   \n",
       "50       1.446125      0.064014         0.076042        0.004591   \n",
       "88       1.121251      0.054270         0.064064        0.000459   \n",
       "97       2.074067      0.299716         0.068131        0.000985   \n",
       "91       1.484825      0.137363         0.075200        0.001080   \n",
       "61       1.545654      0.200134         0.069848        0.000691   \n",
       "15       1.407048      0.058881         0.081968        0.005464   \n",
       "2        1.290341      0.103828         0.069295        0.002260   \n",
       "40       1.530041      0.246018         0.079734        0.005308   \n",
       "22       1.502557      0.107721         0.079624        0.003246   \n",
       "54       1.379140      0.088124         0.079533        0.003351   \n",
       "86       1.781326      0.124139         0.078681        0.001120   \n",
       "14       2.078301      0.350160         0.069616        0.002228   \n",
       "56       1.577962      0.328636         0.085771        0.011449   \n",
       "39       1.647439      0.057403         0.074555        0.001031   \n",
       "43       2.262394      1.964590         0.069721        0.002387   \n",
       "46       1.819248      0.434627         0.066853        0.002264   \n",
       "75       1.340920      0.196845         0.068869        0.009164   \n",
       "41       1.262931      0.065133         0.073932        0.002217   \n",
       "37       1.517129      0.100984         0.075037        0.000743   \n",
       "36       1.310591      0.159938         0.068115        0.005472   \n",
       "42       2.399842      0.570349         0.071272        0.003106   \n",
       "69       1.752546      0.231012         0.076944        0.001937   \n",
       "29       2.769110      0.134735         0.079284        0.003135   \n",
       "32       5.372091      0.345872         0.057939        0.002211   \n",
       "93       1.520440      0.151489         0.080155        0.007016   \n",
       "35       4.577236      0.873900         0.070861        0.001916   \n",
       "96       4.654827      1.034928         0.075963        0.000651   \n",
       "55       4.841628      0.642366         0.071950        0.009444   \n",
       "28       5.870517      0.275743         0.065118        0.001494   \n",
       "51       4.626636      1.202737         0.072876        0.000783   \n",
       "72       3.711105      0.864177         0.078757        0.005837   \n",
       "52       4.205330      0.662003         0.075952        0.004268   \n",
       "20       1.870194      0.267567         0.070096        0.002482   \n",
       "31       2.615868      0.196203         0.078078        0.003747   \n",
       "7        3.737277      0.634409         0.083777        0.004083   \n",
       "34       6.096340      0.386804         0.069647        0.002039   \n",
       "57       1.274352      0.052584         0.077322        0.003825   \n",
       "1        3.349942      0.441901         0.082150        0.003621   \n",
       "98       3.276229      0.806072         0.220423        0.285118   \n",
       "81       2.875356      1.411328         0.064165        0.001089   \n",
       "19       5.669213      0.150287         0.066952        0.003482   \n",
       "66       2.952600      1.658953         0.077643        0.002434   \n",
       "76       2.814177      0.214926         0.072581        0.001316   \n",
       "59       4.946305      0.890021         0.074963        0.002848   \n",
       "74       5.057990      1.269600         0.065355        0.003223   \n",
       "65       6.307224      0.539358         0.077992        0.003392   \n",
       "99       5.765096      0.082333         0.067367        0.000889   \n",
       "95       5.990242      0.602887         0.066875        0.001736   \n",
       "89       5.490373      0.065778         0.064155        0.000709   \n",
       "5        5.117471      0.049489         0.057340        0.001454   \n",
       "58       5.744885      0.223526         0.064530        0.001768   \n",
       "\n",
       "   param_mlp__optimizer__learning_rate param_mlp__nlayers  \\\n",
       "90                               0.005                  3   \n",
       "47                               0.001                  4   \n",
       "6                                0.001                  3   \n",
       "44                               0.005                  5   \n",
       "8                                0.005                  4   \n",
       "63                               0.001                  5   \n",
       "83                               0.001                  3   \n",
       "10                               0.005                  5   \n",
       "50                               0.005                  5   \n",
       "88                               0.005                  3   \n",
       "97                               0.001                  4   \n",
       "91                               0.005                  5   \n",
       "61                               0.005                  4   \n",
       "15                               0.005                  6   \n",
       "2                                0.005                  4   \n",
       "40                               0.005                  6   \n",
       "22                               0.001                  6   \n",
       "54                               0.005                  6   \n",
       "86                               0.001                  6   \n",
       "14                               0.001                  3   \n",
       "56                               0.005                  6   \n",
       "39                               0.001                  6   \n",
       "43                               0.005                  4   \n",
       "46                               0.001                  3   \n",
       "75                               0.005                  3   \n",
       "41                               0.005                  5   \n",
       "37                               0.001                  6   \n",
       "36                               0.005                  3   \n",
       "42                               0.001                  4   \n",
       "69                               0.001                  6   \n",
       "29                              0.0001                  6   \n",
       "32                              0.0001                  1   \n",
       "93                               0.001                  5   \n",
       "35                              0.0001                  4   \n",
       "96                              0.0001                  6   \n",
       "55                              0.0001                  4   \n",
       "28                              0.0001                  2   \n",
       "51                              0.0001                  5   \n",
       "72                              0.0001                  5   \n",
       "52                              0.0001                  5   \n",
       "20                               0.001                  4   \n",
       "31                              0.0001                  6   \n",
       "7                               0.0001                  6   \n",
       "34                              0.0001                  4   \n",
       "57                               0.005                  6   \n",
       "1                               0.0001                  6   \n",
       "98                              0.0001                  6   \n",
       "81                               0.001                  3   \n",
       "19                              0.0001                  3   \n",
       "66                               0.001                  6   \n",
       "76                              0.0001                  5   \n",
       "59                              0.0001                  5   \n",
       "74                              0.0001                  3   \n",
       "65                              0.0001                  6   \n",
       "99                              0.0001                  3   \n",
       "95                              0.0001                  3   \n",
       "89                              0.0001                  3   \n",
       "5                               0.0001                  1   \n",
       "58                              0.0001                  2   \n",
       "\n",
       "          param_mlp__hiddens             param_mlp__dropouts  \\\n",
       "90      (4, 64, 8, 4, 8, 16)  [0.1, 0.0, 0.2, 0.0, 0.2, 0.0]   \n",
       "47      (4, 4, 32, 8, 8, 16)  [0.0, 0.1, 0.0, 0.1, 0.2, 0.2]   \n",
       "6      (4, 32, 16, 8, 32, 4)  [0.1, 0.2, 0.2, 0.0, 0.1, 0.2]   \n",
       "44    (4, 64, 8, 32, 16, 16)  [0.2, 0.0, 0.1, 0.0, 0.3, 0.1]   \n",
       "8     (16, 16, 32, 8, 32, 4)  [0.2, 0.1, 0.1, 0.0, 0.3, 0.3]   \n",
       "63      (4, 16, 32, 8, 4, 8)  [0.0, 0.0, 0.0, 0.3, 0.0, 0.3]   \n",
       "83      (4, 64, 32, 8, 8, 8)  [0.0, 0.0, 0.0, 0.3, 0.3, 0.0]   \n",
       "10    (64, 16, 64, 4, 32, 8)  [0.0, 0.1, 0.0, 0.0, 0.3, 0.3]   \n",
       "50     (8, 32, 16, 64, 4, 4)  [0.0, 0.2, 0.2, 0.2, 0.2, 0.0]   \n",
       "88    (64, 4, 32, 4, 32, 32)  [0.1, 0.2, 0.0, 0.1, 0.0, 0.1]   \n",
       "97     (8, 16, 4, 16, 4, 16)  [0.0, 0.1, 0.1, 0.0, 0.0, 0.3]   \n",
       "91     (8, 16, 8, 32, 16, 8)  [0.0, 0.2, 0.2, 0.3, 0.0, 0.1]   \n",
       "61     (4, 4, 8, 16, 64, 16)  [0.0, 0.3, 0.2, 0.2, 0.0, 0.1]   \n",
       "15   (16, 64, 8, 32, 64, 16)  [0.2, 0.3, 0.0, 0.2, 0.1, 0.2]   \n",
       "2   (16, 32, 64, 32, 16, 64)  [0.2, 0.2, 0.2, 0.0, 0.3, 0.0]   \n",
       "40    (16, 16, 8, 16, 64, 8)  [0.0, 0.3, 0.3, 0.2, 0.0, 0.0]   \n",
       "22  (32, 16, 64, 64, 32, 32)  [0.0, 0.1, 0.0, 0.3, 0.1, 0.0]   \n",
       "54      (8, 8, 32, 8, 8, 16)  [0.0, 0.1, 0.0, 0.2, 0.2, 0.0]   \n",
       "86   (4, 32, 16, 32, 64, 32)  [0.0, 0.2, 0.0, 0.0, 0.0, 0.3]   \n",
       "14    (16, 8, 16, 16, 16, 4)  [0.1, 0.2, 0.3, 0.0, 0.3, 0.3]   \n",
       "56   (32, 16, 32, 16, 64, 8)  [0.1, 0.3, 0.3, 0.3, 0.1, 0.0]   \n",
       "39     (8, 64, 8, 16, 16, 4)  [0.0, 0.0, 0.1, 0.0, 0.1, 0.1]   \n",
       "43     (64, 4, 64, 4, 4, 32)  [0.0, 0.1, 0.1, 0.3, 0.2, 0.1]   \n",
       "46   (8, 64, 32, 16, 32, 32)  [0.3, 0.0, 0.3, 0.3, 0.2, 0.2]   \n",
       "75       (8, 4, 8, 16, 8, 4)  [0.1, 0.3, 0.2, 0.2, 0.3, 0.3]   \n",
       "41     (8, 8, 64, 8, 32, 32)  [0.3, 0.0, 0.2, 0.3, 0.0, 0.2]   \n",
       "37      (64, 4, 8, 8, 64, 8)  [0.0, 0.2, 0.0, 0.1, 0.1, 0.2]   \n",
       "36    (8, 16, 4, 16, 16, 64)  [0.1, 0.0, 0.3, 0.0, 0.0, 0.3]   \n",
       "42       (4, 4, 32, 8, 8, 4)  [0.3, 0.1, 0.0, 0.3, 0.1, 0.2]   \n",
       "69    (8, 64, 16, 64, 4, 16)  [0.1, 0.3, 0.3, 0.1, 0.1, 0.0]   \n",
       "29    (64, 8, 64, 32, 4, 64)  [0.0, 0.1, 0.0, 0.2, 0.1, 0.3]   \n",
       "32      (32, 8, 8, 4, 16, 4)  [0.3, 0.0, 0.1, 0.1, 0.0, 0.2]   \n",
       "93    (32, 32, 8, 8, 32, 16)  [0.0, 0.0, 0.1, 0.3, 0.1, 0.0]   \n",
       "35     (64, 16, 4, 4, 4, 64)  [0.0, 0.3, 0.0, 0.1, 0.0, 0.0]   \n",
       "96     (8, 32, 4, 32, 4, 32)  [0.1, 0.1, 0.0, 0.0, 0.0, 0.1]   \n",
       "55     (4, 8, 64, 64, 16, 4)  [0.2, 0.1, 0.0, 0.0, 0.2, 0.2]   \n",
       "28     (16, 4, 16, 8, 32, 8)  [0.1, 0.0, 0.3, 0.2, 0.0, 0.0]   \n",
       "51   (4, 16, 64, 64, 64, 16)  [0.1, 0.2, 0.0, 0.3, 0.1, 0.2]   \n",
       "72     (32, 64, 32, 4, 4, 8)  [0.3, 0.3, 0.0, 0.1, 0.0, 0.1]   \n",
       "52    (32, 4, 32, 16, 8, 64)  [0.0, 0.0, 0.3, 0.0, 0.3, 0.2]   \n",
       "20     (32, 4, 4, 32, 8, 32)  [0.2, 0.3, 0.1, 0.0, 0.2, 0.3]   \n",
       "31    (64, 8, 32, 64, 8, 64)  [0.2, 0.3, 0.0, 0.3, 0.0, 0.0]   \n",
       "7     (16, 16, 4, 32, 64, 8)  [0.3, 0.3, 0.0, 0.0, 0.3, 0.0]   \n",
       "34     (8, 4, 8, 32, 16, 32)  [0.1, 0.0, 0.3, 0.3, 0.3, 0.0]   \n",
       "57     (16, 16, 8, 4, 32, 4)  [0.0, 0.3, 0.2, 0.2, 0.0, 0.3]   \n",
       "1     (16, 64, 16, 4, 8, 64)  [0.3, 0.1, 0.3, 0.0, 0.1, 0.1]   \n",
       "98     (8, 64, 8, 8, 64, 64)  [0.2, 0.2, 0.0, 0.2, 0.1, 0.0]   \n",
       "81     (8, 32, 4, 8, 16, 16)  [0.1, 0.0, 0.1, 0.0, 0.2, 0.3]   \n",
       "19      (4, 16, 32, 4, 8, 8)  [0.2, 0.3, 0.0, 0.3, 0.0, 0.1]   \n",
       "66      (8, 4, 4, 32, 64, 4)  [0.3, 0.3, 0.1, 0.2, 0.3, 0.0]   \n",
       "76    (32, 32, 64, 4, 32, 4)  [0.2, 0.2, 0.2, 0.3, 0.3, 0.3]   \n",
       "59   (16, 16, 16, 16, 4, 16)  [0.3, 0.0, 0.0, 0.3, 0.0, 0.0]   \n",
       "74      (4, 64, 4, 16, 4, 4)  [0.0, 0.0, 0.0, 0.0, 0.1, 0.0]   \n",
       "65       (8, 4, 8, 32, 4, 4)  [0.1, 0.0, 0.0, 0.0, 0.3, 0.1]   \n",
       "99      (4, 4, 4, 16, 16, 8)  [0.3, 0.2, 0.1, 0.3, 0.0, 0.1]   \n",
       "95      (4, 8, 4, 16, 4, 32)  [0.3, 0.3, 0.2, 0.3, 0.0, 0.0]   \n",
       "89      (4, 4, 4, 4, 64, 64)  [0.2, 0.1, 0.2, 0.3, 0.2, 0.0]   \n",
       "5       (4, 4, 4, 16, 8, 16)  [0.0, 0.0, 0.0, 0.3, 0.2, 0.2]   \n",
       "58     (4, 4, 64, 8, 16, 64)  [0.2, 0.2, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "                                               params  split0_test_score  ...  \\\n",
       "90  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.444679  ...   \n",
       "47  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.502558  ...   \n",
       "6   {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.510855  ...   \n",
       "44  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.207207  ...   \n",
       "8   {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.679821  ...   \n",
       "63  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.738486  ...   \n",
       "83  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.356810  ...   \n",
       "10  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.984387  ...   \n",
       "50  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.772745  ...   \n",
       "88  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.801082  ...   \n",
       "97  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.748797  ...   \n",
       "91  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.194103  ...   \n",
       "61  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.032763  ...   \n",
       "15  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.286118  ...   \n",
       "2   {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.278897  ...   \n",
       "40  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.242633  ...   \n",
       "22  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.260488  ...   \n",
       "54  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.836822  ...   \n",
       "86  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.841363  ...   \n",
       "14  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.233727  ...   \n",
       "56  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.929504  ...   \n",
       "39  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.986691  ...   \n",
       "43  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.683450  ...   \n",
       "46  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.931859  ...   \n",
       "75  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.709024  ...   \n",
       "41  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -5.431517  ...   \n",
       "37  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.883612  ...   \n",
       "36  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.868113  ...   \n",
       "42  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.713081  ...   \n",
       "69  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.959479  ...   \n",
       "29  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.518302  ...   \n",
       "32  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.562274  ...   \n",
       "93  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.315631  ...   \n",
       "35  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.147595  ...   \n",
       "96  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.205419  ...   \n",
       "55  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.032173  ...   \n",
       "28  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.627315  ...   \n",
       "51  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -6.522488  ...   \n",
       "72  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.027965  ...   \n",
       "52  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.209240  ...   \n",
       "20  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.693189  ...   \n",
       "31  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.366928  ...   \n",
       "7   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.464281  ...   \n",
       "34  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.900061  ...   \n",
       "57  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -5.245757  ...   \n",
       "1   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.407163  ...   \n",
       "98  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.294130  ...   \n",
       "81  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.760444  ...   \n",
       "19  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.544407  ...   \n",
       "66  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.724928  ...   \n",
       "76  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.616088  ...   \n",
       "59  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.973960  ...   \n",
       "74  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -7.938048  ...   \n",
       "65  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.654573  ...   \n",
       "99  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -9.025707  ...   \n",
       "95  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -9.293062  ...   \n",
       "89  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...         -16.659912  ...   \n",
       "5   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -9.094137  ...   \n",
       "58  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -8.217524  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "90        -3.245788        0.184700                1           -3.123399   \n",
       "47        -3.306568        0.217319                2           -3.202178   \n",
       "6         -3.390546        0.302877                3           -3.142341   \n",
       "44        -3.451918        0.515716                5           -3.128185   \n",
       "8         -3.457536        0.185252                7           -3.400886   \n",
       "63        -3.482312        0.314462                9           -3.206298   \n",
       "83        -3.491877        0.297740               10           -3.032396   \n",
       "10        -3.511034        0.294358               12           -3.639168   \n",
       "50        -3.548365        0.294348               14           -3.605911   \n",
       "88        -3.600028        0.317610               22           -3.483200   \n",
       "97        -3.669659        0.471889               28           -3.303509   \n",
       "91        -3.683814        0.474299               30           -2.969188   \n",
       "61        -3.724189        0.198925               34           -4.076120   \n",
       "15        -3.727956        0.334774               35           -3.708094   \n",
       "2         -3.730299        0.433099               37           -3.993858   \n",
       "40        -3.836662        0.221970               45           -4.394014   \n",
       "22        -3.864190        0.312776               47           -4.088623   \n",
       "54        -3.921562        0.382886               49           -3.577503   \n",
       "86        -3.946563        0.250703               50           -3.658225   \n",
       "14        -3.979165        0.257731               52           -3.806590   \n",
       "56        -3.998526        0.731424               54           -4.026567   \n",
       "39        -4.019573        0.230108               55           -3.509549   \n",
       "43        -4.068708        0.881039               56           -3.042402   \n",
       "46        -4.073323        0.342279               57           -3.933999   \n",
       "75        -4.138777        0.724627               60           -3.604376   \n",
       "41        -4.160269        0.842450               61           -5.637015   \n",
       "37        -4.188626        0.494258               62           -3.632736   \n",
       "36        -4.378028        0.732928               66           -3.780779   \n",
       "42        -4.629266        0.699482               67           -3.792214   \n",
       "69        -4.641373        0.412405               68           -4.600227   \n",
       "29        -4.710017        0.157673               69           -4.262499   \n",
       "32        -4.735709        0.472276               70           -4.626635   \n",
       "93        -4.745196        0.339333               71           -3.899865   \n",
       "35        -4.745275        0.434045               72           -5.032434   \n",
       "96        -4.819183        1.077468               73           -4.161263   \n",
       "55        -4.875657        0.388905               74           -4.985263   \n",
       "28        -4.918170        0.366687               75           -4.383821   \n",
       "51        -4.918541        0.844340               76           -6.403205   \n",
       "72        -4.931777        0.226004               77           -5.064423   \n",
       "52        -4.936507        0.429749               78           -4.749343   \n",
       "20        -4.945885        0.592438               79           -4.537680   \n",
       "31        -5.263866        0.220901               81           -5.123247   \n",
       "7         -5.447851        0.084832               82           -5.809070   \n",
       "34        -5.517619        0.534699               83           -5.819024   \n",
       "57        -5.622228        1.165520               84           -4.948552   \n",
       "1         -5.664937        0.325811               86           -5.151940   \n",
       "98        -5.680199        0.625255               87           -5.511517   \n",
       "81        -5.733932        4.187808               88           -3.519809   \n",
       "19        -6.037519        0.883102               89           -5.679337   \n",
       "66        -6.190416        3.475616               90           -5.013691   \n",
       "76        -6.564017        0.729446               91           -5.788209   \n",
       "59        -7.500001        4.313533               92           -4.826792   \n",
       "74        -8.148529        4.380331               93           -7.774635   \n",
       "65       -10.688800        5.616778               94           -5.493066   \n",
       "99       -11.212495        2.713143               95           -9.846847   \n",
       "95       -11.252686        3.258617               96           -9.939363   \n",
       "89       -11.724782        3.108373               97          -17.147026   \n",
       "5        -12.236092        2.430642               98           -9.211746   \n",
       "58       -12.360122        2.435588               99           -8.452629   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "90           -2.881489           -3.200226           -3.204078   \n",
       "47           -3.265401           -3.032918           -3.439771   \n",
       "6            -3.310471           -3.101906           -3.179767   \n",
       "44           -3.862343           -3.072557           -3.394035   \n",
       "8            -2.870856           -3.252017           -3.121352   \n",
       "63           -3.450732           -3.505153           -3.251069   \n",
       "83           -3.292912           -3.188334           -3.490692   \n",
       "10           -3.225021           -3.512329           -3.147330   \n",
       "50           -3.195032           -3.059955           -3.527445   \n",
       "88           -3.011451           -3.564499           -3.021721   \n",
       "97           -3.469351           -3.097847           -3.089463   \n",
       "91           -2.899978           -3.489649           -4.435165   \n",
       "61           -3.204912           -3.280440           -3.612611   \n",
       "15           -3.132101           -3.468539           -3.515535   \n",
       "2            -3.169882           -2.808735           -4.060848   \n",
       "40           -3.067084           -3.593900           -3.970299   \n",
       "22           -3.439558           -3.575928           -3.251769   \n",
       "54           -3.824137           -3.555268           -3.613887   \n",
       "86           -3.306484           -3.938358           -4.235332   \n",
       "14           -3.512726           -3.832004           -3.933008   \n",
       "56           -3.470867           -3.408566           -3.492128   \n",
       "39           -3.720099           -3.881995           -3.830726   \n",
       "43           -3.743024           -3.109341           -3.579479   \n",
       "46           -4.095889           -3.454255           -3.810474   \n",
       "75           -2.993532           -5.108695           -3.939392   \n",
       "41           -4.289149           -3.363489           -4.305180   \n",
       "37           -3.861969           -3.327079           -4.332283   \n",
       "36           -2.866882           -4.889332           -4.257324   \n",
       "42           -4.001750           -3.996552           -5.039602   \n",
       "69           -3.691846           -4.298586           -4.591487   \n",
       "29           -4.325011           -4.487673           -4.733686   \n",
       "32           -4.195252           -4.067453           -4.284728   \n",
       "93           -4.994592           -4.869039           -4.365901   \n",
       "35           -4.783619           -4.198656           -3.924726   \n",
       "96           -6.743170           -4.453614           -3.723066   \n",
       "55           -5.051864           -4.647574           -4.604249   \n",
       "28           -4.264166           -5.404635           -4.585191   \n",
       "51           -3.755758           -3.805934           -4.307264   \n",
       "72           -4.292847           -4.596335           -4.934298   \n",
       "52           -4.827422           -3.885274           -4.718909   \n",
       "20           -3.665157           -4.578929           -5.228545   \n",
       "31           -4.947372           -4.975546           -5.205015   \n",
       "7            -4.933755           -5.593369           -5.546139   \n",
       "34           -4.232330           -5.493301           -5.591993   \n",
       "57           -4.412811           -5.256732           -7.782584   \n",
       "1            -5.793719           -5.084618           -5.509892   \n",
       "98           -6.286701           -5.946515           -5.070109   \n",
       "81          -13.532279           -3.066280           -3.308044   \n",
       "19           -5.053432           -7.487936           -5.580757   \n",
       "66           -4.084607           -4.033420           -4.245999   \n",
       "76           -6.216347           -6.913198           -7.088507   \n",
       "59           -5.119690           -5.563558          -16.146329   \n",
       "74           -6.167044           -4.067231           -5.443891   \n",
       "65          -16.939133          -16.907280           -6.184639   \n",
       "99           -8.126998          -11.089035          -16.278426   \n",
       "95           -8.807537          -14.355163           -7.149443   \n",
       "89           -9.507247           -8.190365          -13.952026   \n",
       "5           -10.256436          -13.314954          -11.208554   \n",
       "58          -10.679656          -12.443422          -14.927909   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "90           -2.883873         -3.058613         0.146505  \n",
       "47           -3.160560         -3.220165         0.133575  \n",
       "6            -3.135069         -3.173911         0.072626  \n",
       "44           -3.495929         -3.390610         0.284134  \n",
       "8            -3.304009         -3.189824         0.183239  \n",
       "63           -3.434089         -3.369468         0.118180  \n",
       "83           -3.274313         -3.255729         0.149235  \n",
       "10           -2.962338         -3.297237         0.246181  \n",
       "50           -3.104705         -3.298610         0.224536  \n",
       "88           -3.536013         -3.323377         0.251870  \n",
       "97           -4.086803         -3.409395         0.366908  \n",
       "91           -3.310104         -3.420817         0.551568  \n",
       "61           -3.803830         -3.595583         0.324503  \n",
       "15           -3.352134         -3.435281         0.190161  \n",
       "2            -3.607992         -3.528263         0.480196  \n",
       "40           -3.805948         -3.766249         0.437227  \n",
       "22           -3.628871         -3.596950         0.278253  \n",
       "54           -3.733543         -3.660868         0.102222  \n",
       "86           -3.827102         -3.793100         0.307510  \n",
       "14           -3.433693         -3.703604         0.194425  \n",
       "56           -5.077090         -3.895044         0.631421  \n",
       "39           -4.027582         -3.793990         0.173220  \n",
       "43           -6.030831         -3.901015         1.098037  \n",
       "46           -4.124846         -3.883893         0.243101  \n",
       "75           -3.920404         -3.913280         0.688637  \n",
       "41           -3.246187         -4.168204         0.858892  \n",
       "37           -4.422741         -3.915362         0.414744  \n",
       "36           -5.248864         -4.208636         0.840376  \n",
       "42           -5.944006         -4.554825         0.820240  \n",
       "69           -5.152407         -4.466911         0.475869  \n",
       "29           -4.507604         -4.463295         0.164384  \n",
       "32           -5.144372         -4.463688         0.387548  \n",
       "93           -4.432202         -4.512320         0.390587  \n",
       "35           -4.367122         -4.461311         0.398960  \n",
       "96           -4.294684         -4.675159         1.062175  \n",
       "55           -4.588098         -4.775410         0.200594  \n",
       "28           -4.508484         -4.629259         0.402807  \n",
       "51           -5.557701         -4.765973         1.044987  \n",
       "72           -4.610921         -4.699765         0.272820  \n",
       "52           -5.010941         -4.638378         0.390001  \n",
       "20           -5.610092         -4.724081         0.665828  \n",
       "31           -5.030421         -5.056320         0.095556  \n",
       "7            -5.232346         -5.422936         0.306251  \n",
       "34           -5.701148         -5.367559         0.577932  \n",
       "57           -4.657990         -5.411734         1.218644  \n",
       "1            -5.740507         -5.456135         0.292670  \n",
       "98           -5.157332         -5.594435         0.463906  \n",
       "81           -3.837000         -5.452682         4.047733  \n",
       "19           -5.893959         -5.939084         0.822341  \n",
       "66          -13.772973         -6.230138         3.787870  \n",
       "76           -5.635606         -6.328374         0.583794  \n",
       "59           -4.837408         -7.298756         4.431876  \n",
       "74          -17.179002         -8.126360         4.681562  \n",
       "65           -6.661172        -10.437058         5.308939  \n",
       "99          -10.789633        -11.226188         2.729178  \n",
       "95          -16.180493        -11.286400         3.418864  \n",
       "89           -9.187030        -11.596739         3.409666  \n",
       "5           -16.838069        -12.165952         2.699595  \n",
       "58          -14.464075        -12.193538         2.407522  \n",
       "\n",
       "[59 rows x 24 columns]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[no_overfit].sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_results[no_overfit]\n",
    "    .sort_values(\"mean_test_score\", ascending=False)\n",
    "    .to_csv(\"best_model_2.csv\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>1.381248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.038405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.068078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.005896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__optimizer__learning_rate</th>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__nlayers</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__hiddens</th>\n",
       "      <td>(4, 64, 8, 4, 8, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__dropouts</th>\n",
       "      <td>[0.1 0.  0.2 0.  0.2 0. ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>-3.444679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>-3.440077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>-3.266266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>-2.978596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>-3.099319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>-3.245788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.1847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>-3.123399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>-2.881489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>-3.200226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>-3.204078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>-2.883873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>-3.058613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.146505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     0\n",
       "mean_fit_time                                                                 1.381248\n",
       "std_fit_time                                                                  0.038405\n",
       "mean_score_time                                                               0.068078\n",
       "std_score_time                                                                0.005896\n",
       "param_mlp__optimizer__learning_rate                                              0.005\n",
       "param_mlp__nlayers                                                                   3\n",
       "param_mlp__hiddens                                                (4, 64, 8, 4, 8, 16)\n",
       "param_mlp__dropouts                                          [0.1 0.  0.2 0.  0.2 0. ]\n",
       "params                               {'mlp__optimizer__learning_rate': 0.005, 'mlp_...\n",
       "split0_test_score                                                            -3.444679\n",
       "split1_test_score                                                            -3.440077\n",
       "split2_test_score                                                            -3.266266\n",
       "split3_test_score                                                            -2.978596\n",
       "split4_test_score                                                            -3.099319\n",
       "mean_test_score                                                              -3.245788\n",
       "std_test_score                                                                  0.1847\n",
       "rank_test_score                                                                      1\n",
       "split0_train_score                                                           -3.123399\n",
       "split1_train_score                                                           -2.881489\n",
       "split2_train_score                                                           -3.200226\n",
       "split3_train_score                                                           -3.204078\n",
       "split4_train_score                                                           -2.883873\n",
       "mean_train_score                                                             -3.058613\n",
       "std_train_score                                                               0.146505"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = pd.read_csv(\"best_model_2.csv\")\n",
    "best_model.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = KerasRegressor(\n",
    "    twoLayerFeedForward, \n",
    "    epochs=200, \n",
    "    loss=\"mse\",\n",
    "    callbacks=[callback],\n",
    "    validation_split=0.2,\n",
    "    nlayers=3,\n",
    "    dropouts=[0.2, 0., 0.],\n",
    "    hiddens=(4, 4, 32),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipe = Pipeline(\n",
    "    [\n",
    "        ('scaler', PowerTransformer()),\n",
    "        (\"principal_components\", PCA(n_components=20, svd_solver = 'full')),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1690\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7591 (Dense)          (None, 4)                 84        \n",
      "                                                                 \n",
      " dropout_3654 (Dropout)      (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_7592 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " dense_7593 (Dense)          (None, 8)                 520       \n",
      "                                                                 \n",
      " dropout_3655 (Dropout)      (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_7594 (Dense)          (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 933\n",
      "Trainable params: 933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp = twoLayerFeedForward(\n",
    "    hiddens=(4, 64, 8,),\n",
    "    dropouts=[0.1, 0., 0.2],\n",
    "    nlayers=3,\n",
    "    meta={\"X_shape_\":(0, 20)}\n",
    ")\n",
    "mlp.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001), loss=\"mae\")\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = preprocessing_pipe.fit_transform(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd76c129d30>"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=6,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "mlp.fit(new_df, y.values, epochs=200, validation_split=0.2, callbacks=[callback], verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd76d50f580>"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = mlp.history.history[\"loss\"]\n",
    "loss_history_val = mlp.history.history[\"val_loss\"]\n",
    "plt.plot(loss_history, label=\"Train\")\n",
    "plt.plot(loss_history_val, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroimage-q89akeZ7-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
