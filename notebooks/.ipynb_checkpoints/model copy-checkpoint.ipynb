{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.alpha\": 0.5,\n",
    "        \"grid.linestyle\": \"--\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/FS_features_ABIDE_males.csv\", sep=\";\")\n",
    "df = df.set_index(\"FILE_ID\")\n",
    "\n",
    "# drop target\n",
    "y = df[\"AGE_AT_SCAN\"]\n",
    "df = df.drop([\"AGE_AT_SCAN\", \"SEX\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# L1 regularization\n",
    "\n",
    "linear_regressor = Pipeline(\n",
    "    [\n",
    "        ('scaler', PowerTransformer()),\n",
    "        (\"principal_components\", PCA(n_components=20, svd_solver = 'full')),\n",
    "        (\"model\",  Lasso())\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_distr = {\"model__alpha\": np.arange(1, 10)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "search = GridSearchCV(\n",
    "    linear_regressor, \n",
    "    param_distr, \n",
    "    return_train_score=True, \n",
    "    scoring=\"r2\", \n",
    "    cv=KFold(shuffle=True) # 5 split\n",
    ")\n",
    "\n",
    "search.fit(df.values, y.values)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_regressor = pd.DataFrame(search.cv_results_)\n",
    "no_overfit = results_regressor[\"mean_train_score\"] - results_regressor[\"mean_test_score\"] < 6\n",
    "\n",
    "(\n",
    "    results_regressor[no_overfit]\n",
    "    .sort_values(\"mean_test_score\", ascending=False)\n",
    "    .to_csv(\"best_model_regressor.csv\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.585921</td>\n",
       "      <td>0.020995</td>\n",
       "      <td>0.012327</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>1</td>\n",
       "      <td>{'model__alpha': 1}</td>\n",
       "      <td>0.615878</td>\n",
       "      <td>0.640934</td>\n",
       "      <td>0.654502</td>\n",
       "      <td>0.679240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.651129</td>\n",
       "      <td>0.021648</td>\n",
       "      <td>1</td>\n",
       "      <td>0.677312</td>\n",
       "      <td>0.685793</td>\n",
       "      <td>0.675309</td>\n",
       "      <td>0.664219</td>\n",
       "      <td>0.665360</td>\n",
       "      <td>0.673598</td>\n",
       "      <td>0.008016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.544735</td>\n",
       "      <td>0.011679</td>\n",
       "      <td>0.012382</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>2</td>\n",
       "      <td>{'model__alpha': 2}</td>\n",
       "      <td>0.597090</td>\n",
       "      <td>0.595131</td>\n",
       "      <td>0.624535</td>\n",
       "      <td>0.656675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.616755</td>\n",
       "      <td>0.022581</td>\n",
       "      <td>2</td>\n",
       "      <td>0.644072</td>\n",
       "      <td>0.637259</td>\n",
       "      <td>0.643665</td>\n",
       "      <td>0.636455</td>\n",
       "      <td>0.624839</td>\n",
       "      <td>0.637258</td>\n",
       "      <td>0.006962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.533757</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.011953</td>\n",
       "      <td>0.000318</td>\n",
       "      <td>3</td>\n",
       "      <td>{'model__alpha': 3}</td>\n",
       "      <td>0.587817</td>\n",
       "      <td>0.571976</td>\n",
       "      <td>0.603528</td>\n",
       "      <td>0.643639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.598595</td>\n",
       "      <td>0.024643</td>\n",
       "      <td>3</td>\n",
       "      <td>0.624547</td>\n",
       "      <td>0.618133</td>\n",
       "      <td>0.612354</td>\n",
       "      <td>0.609131</td>\n",
       "      <td>0.607105</td>\n",
       "      <td>0.614254</td>\n",
       "      <td>0.006356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.540314</td>\n",
       "      <td>0.009135</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>4</td>\n",
       "      <td>{'model__alpha': 4}</td>\n",
       "      <td>0.572153</td>\n",
       "      <td>0.561400</td>\n",
       "      <td>0.599368</td>\n",
       "      <td>0.632396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.588640</td>\n",
       "      <td>0.025136</td>\n",
       "      <td>4</td>\n",
       "      <td>0.603847</td>\n",
       "      <td>0.608048</td>\n",
       "      <td>0.597743</td>\n",
       "      <td>0.587420</td>\n",
       "      <td>0.600709</td>\n",
       "      <td>0.599553</td>\n",
       "      <td>0.006963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.542492</td>\n",
       "      <td>0.008440</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>5</td>\n",
       "      <td>{'model__alpha': 5}</td>\n",
       "      <td>0.565988</td>\n",
       "      <td>0.554890</td>\n",
       "      <td>0.592948</td>\n",
       "      <td>0.622458</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580987</td>\n",
       "      <td>0.024171</td>\n",
       "      <td>5</td>\n",
       "      <td>0.596712</td>\n",
       "      <td>0.600553</td>\n",
       "      <td>0.589168</td>\n",
       "      <td>0.576113</td>\n",
       "      <td>0.593210</td>\n",
       "      <td>0.591151</td>\n",
       "      <td>0.008410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.541930</td>\n",
       "      <td>0.008670</td>\n",
       "      <td>0.011891</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>6</td>\n",
       "      <td>{'model__alpha': 6}</td>\n",
       "      <td>0.557494</td>\n",
       "      <td>0.546476</td>\n",
       "      <td>0.585101</td>\n",
       "      <td>0.612222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571823</td>\n",
       "      <td>0.023884</td>\n",
       "      <td>6</td>\n",
       "      <td>0.587992</td>\n",
       "      <td>0.591393</td>\n",
       "      <td>0.580189</td>\n",
       "      <td>0.566491</td>\n",
       "      <td>0.584044</td>\n",
       "      <td>0.582022</td>\n",
       "      <td>0.008627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.541703</td>\n",
       "      <td>0.010078</td>\n",
       "      <td>0.012370</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>7</td>\n",
       "      <td>{'model__alpha': 7}</td>\n",
       "      <td>0.546669</td>\n",
       "      <td>0.536158</td>\n",
       "      <td>0.575814</td>\n",
       "      <td>0.600607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561222</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>7</td>\n",
       "      <td>0.577687</td>\n",
       "      <td>0.580567</td>\n",
       "      <td>0.569578</td>\n",
       "      <td>0.555119</td>\n",
       "      <td>0.574244</td>\n",
       "      <td>0.571439</td>\n",
       "      <td>0.008945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.545919</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.011987</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>8</td>\n",
       "      <td>{'model__alpha': 8}</td>\n",
       "      <td>0.533515</td>\n",
       "      <td>0.527248</td>\n",
       "      <td>0.565085</td>\n",
       "      <td>0.587614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.550358</td>\n",
       "      <td>0.022660</td>\n",
       "      <td>8</td>\n",
       "      <td>0.565796</td>\n",
       "      <td>0.571513</td>\n",
       "      <td>0.557334</td>\n",
       "      <td>0.541998</td>\n",
       "      <td>0.565760</td>\n",
       "      <td>0.560480</td>\n",
       "      <td>0.010289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.538047</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.012472</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>9</td>\n",
       "      <td>{'model__alpha': 9}</td>\n",
       "      <td>0.522491</td>\n",
       "      <td>0.520450</td>\n",
       "      <td>0.552916</td>\n",
       "      <td>0.573242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.539542</td>\n",
       "      <td>0.020442</td>\n",
       "      <td>9</td>\n",
       "      <td>0.555242</td>\n",
       "      <td>0.565572</td>\n",
       "      <td>0.543458</td>\n",
       "      <td>0.527127</td>\n",
       "      <td>0.556144</td>\n",
       "      <td>0.549508</td>\n",
       "      <td>0.013210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.585921      0.020995         0.012327        0.000670   \n",
       "1       0.544735      0.011679         0.012382        0.000905   \n",
       "2       0.533757      0.003248         0.011953        0.000318   \n",
       "3       0.540314      0.009135         0.011987        0.000457   \n",
       "4       0.542492      0.008440         0.012157        0.000355   \n",
       "5       0.541930      0.008670         0.011891        0.000163   \n",
       "6       0.541703      0.010078         0.012370        0.000566   \n",
       "7       0.545919      0.002891         0.011987        0.000281   \n",
       "8       0.538047      0.007091         0.012472        0.000758   \n",
       "\n",
       "  param_model__alpha               params  split0_test_score  \\\n",
       "0                  1  {'model__alpha': 1}           0.615878   \n",
       "1                  2  {'model__alpha': 2}           0.597090   \n",
       "2                  3  {'model__alpha': 3}           0.587817   \n",
       "3                  4  {'model__alpha': 4}           0.572153   \n",
       "4                  5  {'model__alpha': 5}           0.565988   \n",
       "5                  6  {'model__alpha': 6}           0.557494   \n",
       "6                  7  {'model__alpha': 7}           0.546669   \n",
       "7                  8  {'model__alpha': 8}           0.533515   \n",
       "8                  9  {'model__alpha': 9}           0.522491   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.640934           0.654502           0.679240  ...   \n",
       "1           0.595131           0.624535           0.656675  ...   \n",
       "2           0.571976           0.603528           0.643639  ...   \n",
       "3           0.561400           0.599368           0.632396  ...   \n",
       "4           0.554890           0.592948           0.622458  ...   \n",
       "5           0.546476           0.585101           0.612222  ...   \n",
       "6           0.536158           0.575814           0.600607  ...   \n",
       "7           0.527248           0.565085           0.587614  ...   \n",
       "8           0.520450           0.552916           0.573242  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.651129        0.021648                1            0.677312   \n",
       "1         0.616755        0.022581                2            0.644072   \n",
       "2         0.598595        0.024643                3            0.624547   \n",
       "3         0.588640        0.025136                4            0.603847   \n",
       "4         0.580987        0.024171                5            0.596712   \n",
       "5         0.571823        0.023884                6            0.587992   \n",
       "6         0.561222        0.023708                7            0.577687   \n",
       "7         0.550358        0.022660                8            0.565796   \n",
       "8         0.539542        0.020442                9            0.555242   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.685793            0.675309            0.664219   \n",
       "1            0.637259            0.643665            0.636455   \n",
       "2            0.618133            0.612354            0.609131   \n",
       "3            0.608048            0.597743            0.587420   \n",
       "4            0.600553            0.589168            0.576113   \n",
       "5            0.591393            0.580189            0.566491   \n",
       "6            0.580567            0.569578            0.555119   \n",
       "7            0.571513            0.557334            0.541998   \n",
       "8            0.565572            0.543458            0.527127   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.665360          0.673598         0.008016  \n",
       "1            0.624839          0.637258         0.006962  \n",
       "2            0.607105          0.614254         0.006356  \n",
       "3            0.600709          0.599553         0.006963  \n",
       "4            0.593210          0.591151         0.008410  \n",
       "5            0.584044          0.582022         0.008627  \n",
       "6            0.574244          0.571439         0.008945  \n",
       "7            0.565760          0.560480         0.010289  \n",
       "8            0.556144          0.549508         0.013210  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.525392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.008408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.011376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_model__alpha</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'model__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>-3.130726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>-3.587047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>-3.343415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>-3.574417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>-3.889095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>-3.50494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.255057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>-3.513624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>-3.360552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>-3.375411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>-3.404058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>-3.365302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>-3.40379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.056956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0\n",
       "mean_fit_time                  0.525392\n",
       "std_fit_time                   0.008408\n",
       "mean_score_time                0.011376\n",
       "std_score_time                 0.000152\n",
       "param_model__alpha                    1\n",
       "params              {'model__alpha': 1}\n",
       "split0_test_score             -3.130726\n",
       "split1_test_score             -3.587047\n",
       "split2_test_score             -3.343415\n",
       "split3_test_score             -3.574417\n",
       "split4_test_score             -3.889095\n",
       "mean_test_score                -3.50494\n",
       "std_test_score                 0.255057\n",
       "rank_test_score                       1\n",
       "split0_train_score            -3.513624\n",
       "split1_train_score            -3.360552\n",
       "split2_train_score            -3.375411\n",
       "split3_train_score            -3.404058\n",
       "split4_train_score            -3.365302\n",
       "mean_train_score               -3.40379\n",
       "std_train_score                0.056956"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"best_model_regressor.csv\").head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from itertools import product\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoLayerFeedForward(nlayers, hiddens, dropouts, meta):\n",
    "    clf = Sequential()\n",
    "    X_shape_ = (meta[\"X_shape_\"][1],)\n",
    "\n",
    "    clf.add(Dense(hiddens[0], activation='relu', input_shape=X_shape_))\n",
    "    if dropouts[0] > 0:\n",
    "        clf.add(Dropout(dropouts[0]))\n",
    "    for i in range(1, nlayers):\n",
    "        clf.add(Dense(hiddens[i], activation='relu'))\n",
    "        if dropouts[i] > 0:\n",
    "            clf.add(Dropout(dropouts[i]))\n",
    "    clf.add(Dense(1))\n",
    "    return clf\n",
    "\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "\n",
    "mlp = KerasRegressor(\n",
    "    twoLayerFeedForward, \n",
    "    epochs=200, \n",
    "    loss=\"mse\",\n",
    "    callbacks=[callback],\n",
    "    validation_split=0.2,\n",
    "    nlayers=3,\n",
    "    hiddens=[2,2,2],\n",
    "    dropouts=[0.2, 0, 0],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_entries = [4, 8, 16, 32, 64]\n",
    "hidden_layers = list(product(valid_entries, repeat=6))\n",
    "dropouts = list(np.random.choice([0.0, 0.1, 0.2, 0.3], p=[0.4, 0.2, 0.2, 0.2], size=(2000, 6)))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(\n",
    "    [\n",
    "        ('scaler', PowerTransformer()),\n",
    "        (\"principal_components\", PCA(n_components=20, svd_solver = 'full')),\n",
    "        (\"mlp\", mlp)\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"mlp__nlayers\": np.random.randint(1, 7, size=2000),\n",
    "    \"mlp__hiddens\": hidden_layers,\n",
    "    \"mlp__dropouts\": dropouts,\n",
    "    \"mlp__optimizer__learning_rate\": [0.0001, 0.001, 0.005],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = RandomizedSearchCV(\n",
    "    model, \n",
    "    params, \n",
    "    refit=False, \n",
    "    cv=KFold(shuffle=True), \n",
    "    return_train_score=True,\n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    n_iter=100, \n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.289, test=-3.781) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.438, test=-4.039) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.314, test=-3.313) total time=   1.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.438, test=-3.739) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 4, 16, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.972, test=-4.341) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.152, test=-5.407) total time=   4.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.794, test=-6.169) total time=   3.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.085, test=-5.236) total time=   3.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.510, test=-5.689) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.1 0.1], mlp__hiddens=(16, 64, 16, 4, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.741, test=-5.824) total time=   2.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.994, test=-4.279) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.170, test=-3.824) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.809, test=-2.969) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-4.061, test=-3.935) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.2 0.  0.3 0. ], mlp__hiddens=(16, 32, 64, 32, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.608, test=-3.644) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.608, test=-4.738) total time=   5.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.709, test=-5.298) total time=   4.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.461, test=-4.550) total time=   4.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.828, test=-5.462) total time=   3.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.3 0. ], mlp__hiddens=(32, 16, 8, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.518, test=-4.821) total time=   5.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.051, test=-3.547) total time=   1.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.504, test=-3.927) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.199, test=-3.558) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.399, test=-3.746) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.2 0.1 0.  0.2], mlp__hiddens=(8, 8, 32, 64, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.964, test=-3.570) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.212, test=-9.094) total time=   5.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-10.256, test=-10.930) total time=   5.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.315, test=-13.520) total time=   5.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.209, test=-11.440) total time=   5.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0.2], mlp__hiddens=(4, 4, 4, 16, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.838, test=-16.196) total time=   5.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.142, test=-3.511) total time=   2.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.310, test=-3.866) total time=   2.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.102, test=-3.238) total time=   2.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.180, test=-2.949) total time=   2.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.2 0.  0.1 0.2], mlp__hiddens=(4, 32, 16, 8, 32, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.135, test=-3.389) total time=   2.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.809, test=-5.464) total time=   3.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.934, test=-5.384) total time=   4.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.593, test=-5.603) total time=   3.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.546, test=-5.422) total time=   3.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.3 0.  0.  0.3 0. ], mlp__hiddens=(16, 16, 4, 32, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.232, test=-5.366) total time=   4.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.401, test=-3.680) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.871, test=-3.348) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.252, test=-3.538) total time=   1.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.121, test=-3.155) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.3 0.3], mlp__hiddens=(16, 16, 32, 8, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.304, test=-3.567) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.999, test=-3.634) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.325, test=-3.700) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.415, test=-3.420) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.283, test=-3.609) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.3 0.  0.1 0.3], mlp__hiddens=(64, 8, 4, 32, 64, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.845, test=-3.528) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.639, test=-3.984) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.225, test=-3.652) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.512, test=-3.311) total time=   1.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.147, test=-3.126) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.  0.3 0.3], mlp__hiddens=(64, 16, 64, 4, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.962, test=-3.483) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.328, test=-3.853) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.254, test=-3.565) total time=   3.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.625, test=-3.684) total time=   2.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.641, test=-3.997) total time=   2.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 4, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.392, test=-3.909) total time=   2.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.209, test=-4.514) total time=   2.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.626, test=-4.193) total time=   4.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.022, test=-3.986) total time=   3.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.710, test=-4.086) total time=   4.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.  0. ], mlp__hiddens=(32, 32, 32, 8, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.766, test=-4.165) total time=   3.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.754, test=-4.075) total time=   5.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.635, test=-4.117) total time=   5.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.654, test=-3.734) total time=   5.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.675, test=-4.045) total time=   5.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.2 0.2], mlp__hiddens=(64, 32, 4, 4, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.635, test=-3.919) total time=   5.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.807, test=-4.234) total time=   2.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.513, test=-3.837) total time=   2.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.832, test=-3.825) total time=   1.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.933, test=-4.334) total time=   1.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.3 0.3], mlp__hiddens=(16, 8, 16, 16, 16, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.434, test=-3.666) total time=   2.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.708, test=-4.286) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.132, test=-3.633) total time=   1.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.469, test=-3.461) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.516, test=-3.356) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.  0.2 0.1 0.2], mlp__hiddens=(16, 64, 8, 32, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.352, test=-3.903) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.418, test=-3.810) total time=   5.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.431, test=-4.163) total time=   5.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.473, test=-3.450) total time=   4.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.431, test=-3.871) total time=   5.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.  0.2 0.1 0. ], mlp__hiddens=(64, 8, 64, 64, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.432, test=-3.828) total time=   5.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.239, test=-3.812) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.246, test=-3.687) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.299, test=-3.349) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.162, test=-4.264) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.  0.2 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.994, test=-3.456) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.278, test=-4.802) total time=   3.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.015, test=-4.356) total time=   3.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.455, test=-3.566) total time=   4.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.205, test=-4.645) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.1 0.1 0. ], mlp__hiddens=(32, 8, 64, 8, 16, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.540, test=-3.804) total time=   4.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.679, test=-5.544) total time=   5.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.053, test=-5.828) total time=   5.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.488, test=-7.781) total time=   5.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.581, test=-5.390) total time=   5.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0.1], mlp__hiddens=(4, 16, 32, 4, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.894, test=-5.644) total time=   5.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.538, test=-4.693) total time=   1.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.665, test=-4.089) total time=   2.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.579, test=-4.790) total time=   2.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.229, test=-5.322) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.1 0.  0.2 0.3], mlp__hiddens=(32, 4, 4, 32, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.610, test=-5.835) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.636, test=-3.940) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.348, test=-3.862) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.138, test=-3.313) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.018, test=-3.257) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.3 0. ], mlp__hiddens=(64, 64, 8, 8, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.205, test=-3.715) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.089, test=-4.260) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.440, test=-4.006) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.576, test=-3.379) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.252, test=-3.645) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0. ], mlp__hiddens=(32, 16, 64, 64, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.629, test=-4.032) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.206, test=-3.767) total time=   1.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.469, test=-3.883) total time=   1.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.472, test=-3.375) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.025, test=-3.287) total time=   2.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0. ], mlp__hiddens=(64, 8, 32, 4, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.132, test=-3.763) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.405, test=-4.021) total time=   1.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.599, test=-3.999) total time=   1.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.312, test=-3.447) total time=   2.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.500, test=-3.946) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.1 0.3 0.  0.  0. ], mlp__hiddens=(32, 4, 16, 4, 16, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.362, test=-3.770) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.078, test=-3.622) total time=   1.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.851, test=-3.558) total time=   1.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.247, test=-3.569) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.036, test=-3.353) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 4, 16, 8, 32, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.115, test=-3.775) total time=   1.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.344, test=-3.739) total time=   5.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.454, test=-4.059) total time=   5.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.408, test=-3.285) total time=   5.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.572, test=-4.009) total time=   5.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.1 0. ], mlp__hiddens=(64, 8, 64, 8, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.492, test=-3.878) total time=   6.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.015, test=-3.694) total time=   1.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.911, test=-3.530) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.008, test=-3.297) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.933, test=-3.063) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0.1], mlp__hiddens=(64, 4, 16, 8, 4, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.926, test=-3.710) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.384, test=-4.627) total time=   6.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.264, test=-5.165) total time=   5.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.405, test=-5.476) total time=   5.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.585, test=-4.866) total time=   5.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.3 0.2 0.  0. ], mlp__hiddens=(16, 4, 16, 8, 32, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.508, test=-4.457) total time=   5.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.262, test=-4.518) total time=   2.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.325, test=-4.736) total time=   2.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.488, test=-4.550) total time=   3.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.734, test=-4.937) total time=   2.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.1 0.3], mlp__hiddens=(64, 8, 64, 32, 4, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.508, test=-4.808) total time=   2.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.835, test=-3.715) total time=   3.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.155, test=-3.840) total time=   2.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.039, test=-3.246) total time=   2.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.119, test=-3.750) total time=   3.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.1 0.1 0.2], mlp__hiddens=(16, 16, 32, 16, 64, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.146, test=-4.089) total time=   2.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.123, test=-5.367) total time=   2.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.947, test=-5.290) total time=   2.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.976, test=-4.945) total time=   2.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.205, test=-5.596) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.  0.3 0.  0. ], mlp__hiddens=(64, 8, 32, 64, 8, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.030, test=-5.121) total time=   2.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.627, test=-4.562) total time=   6.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.195, test=-4.551) total time=   5.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.067, test=-4.135) total time=   5.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.285, test=-4.876) total time=   5.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.1 0.1 0.  0.2], mlp__hiddens=(32, 8, 8, 4, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.144, test=-5.554) total time=   5.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.643, test=-4.252) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.071, test=-3.665) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.718, test=-3.714) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-4.618, test=-4.747) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 32, 8, 8, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.744, test=-4.118) total time=   1.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.819, test=-5.900) total time=   5.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.232, test=-4.469) total time=   6.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.493, test=-5.885) total time=   5.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.592, test=-5.621) total time=   6.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.3 0.3 0.3 0. ], mlp__hiddens=(8, 4, 8, 32, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.701, test=-5.713) total time=   6.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.032, test=-5.148) total time=   6.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.784, test=-5.373) total time=   4.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.199, test=-4.267) total time=   4.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.925, test=-4.420) total time=   3.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.  0.1 0.  0. ], mlp__hiddens=(64, 16, 4, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.367, test=-4.518) total time=   4.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.781, test=-3.868) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.867, test=-3.477) total time=   1.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-4.889, test=-5.137) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-4.257, test=-4.061) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 16, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-5.249, test=-5.346) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.633, test=-3.884) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.862, test=-4.335) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.327, test=-3.394) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.332, test=-4.562) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.  0.1 0.1 0.2], mlp__hiddens=(64, 4, 8, 8, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.423, test=-4.768) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.954, test=-3.631) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.860, test=-3.524) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.981, test=-3.356) total time=   1.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.584, test=-3.790) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.3 0. ], mlp__hiddens=(64, 16, 32, 64, 32, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.977, test=-3.644) total time=   1.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.510, test=-3.987) total time=   1.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.720, test=-3.841) total time=   1.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.882, test=-3.929) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.831, test=-3.872) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.1], mlp__hiddens=(8, 64, 8, 16, 16, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.028, test=-4.469) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.394, test=-4.243) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.067, test=-3.908) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.594, test=-3.660) total time=   2.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.970, test=-3.695) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.  0. ], mlp__hiddens=(16, 16, 8, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.806, test=-3.679) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-5.637, test=-5.432) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.289, test=-4.569) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.363, test=-3.179) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.305, test=-4.340) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.2 0.3 0.  0.2], mlp__hiddens=(8, 8, 64, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.246, test=-3.282) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.792, test=-3.713) total time=   2.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.002, test=-4.355) total time=   2.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.997, test=-4.404) total time=   3.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.040, test=-4.848) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.  0.3 0.1 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.944, test=-5.826) total time=   2.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.042, test=-3.683) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.743, test=-3.860) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.109, test=-3.309) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.579, test=-3.697) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.1 0.3 0.2 0.1], mlp__hiddens=(64, 4, 64, 4, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-6.031, test=-5.793) total time=   6.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.128, test=-3.207) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.862, test=-4.408) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.073, test=-2.998) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.394, test=-3.082) total time=   1.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.1 0.  0.3 0.1], mlp__hiddens=(4, 64, 8, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.496, test=-3.565) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.931, test=-3.393) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.044, test=-3.689) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.636, test=-4.020) total time=   1.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.014, test=-3.235) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.2 0.  0.  0.3], mlp__hiddens=(16, 64, 64, 4, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.955, test=-4.067) total time=   1.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.934, test=-3.932) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-4.096, test=-4.524) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.454, test=-3.658) total time=   2.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.810, test=-3.822) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.3 0.3 0.2 0.2], mlp__hiddens=(8, 64, 32, 16, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-4.125, test=-4.430) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.202, test=-3.503) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.265, test=-3.621) total time=   2.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.033, test=-3.097) total time=   3.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.440, test=-3.087) total time=   2.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(4, 4, 32, 8, 8, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.161, test=-3.225) total time=   2.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.302, test=-4.553) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.723, test=-4.207) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.483, test=-3.794) total time=   1.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.505, test=-3.648) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.1 0.  0.  0.3], mlp__hiddens=(32, 8, 8, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.312, test=-3.726) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.934, test=-5.176) total time=   4.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.879, test=-5.551) total time=   5.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.756, test=-5.988) total time=   5.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.993, test=-5.435) total time=   5.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.2 0.1], mlp__hiddens=(8, 32, 4, 8, 4, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.500, test=-5.964) total time=   5.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.606, test=-3.773) total time=   1.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.195, test=-3.975) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.060, test=-3.226) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.527, test=-3.530) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.2 0.2 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.105, test=-3.238) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.403, test=-6.522) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.756, test=-4.490) total time=   5.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.806, test=-4.086) total time=   6.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.307, test=-4.580) total time=   3.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.1 0.2], mlp__hiddens=(4, 16, 64, 64, 64, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.558, test=-4.915) total time=   4.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.749, test=-5.209) total time=   4.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.827, test=-5.201) total time=   3.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.885, test=-4.086) total time=   4.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.719, test=-5.033) total time=   4.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.  0.3 0.2], mlp__hiddens=(32, 4, 32, 16, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.011, test=-5.153) total time=   5.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.214, test=-4.538) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.659, test=-4.194) total time=   4.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.854, test=-3.916) total time=   4.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.895, test=-4.275) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 32, 8, 16, 16, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.938, test=-4.183) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.578, test=-3.837) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.824, test=-4.593) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.555, test=-3.872) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.614, test=-3.400) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.2 0. ], mlp__hiddens=(8, 8, 32, 8, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.734, test=-3.906) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.985, test=-5.032) total time=   4.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.052, test=-5.554) total time=   4.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.648, test=-4.745) total time=   4.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.604, test=-4.464) total time=   5.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.2 0.2], mlp__hiddens=(4, 8, 64, 64, 16, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.588, test=-4.583) total time=   5.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.027, test=-3.930) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.471, test=-3.809) total time=   1.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.409, test=-3.419) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.492, test=-3.432) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.3 0.3 0.1 0. ], mlp__hiddens=(32, 16, 32, 16, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-5.077, test=-5.404) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.949, test=-5.246) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.413, test=-4.716) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-5.257, test=-5.418) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-7.783, test=-7.896) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.3], mlp__hiddens=(16, 16, 8, 4, 32, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.658, test=-4.836) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.453, test=-8.218) total time=   6.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-10.680, test=-11.205) total time=   5.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-12.443, test=-13.336) total time=   5.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-14.928, test=-15.182) total time=   5.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.  0. ], mlp__hiddens=(4, 4, 64, 8, 16, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-14.464, test=-13.860) total time=   5.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.827, test=-4.974) total time=   5.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.120, test=-5.696) total time=   4.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.564, test=-5.466) total time=   3.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.146, test=-16.114) total time=   6.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 16, 16, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.837, test=-5.250) total time=   5.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.581, test=-3.901) total time=   1.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.333, test=-3.966) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.994, test=-3.196) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.992, test=-3.155) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 4, 32, 32, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.957, test=-3.625) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-4.076, test=-4.033) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.205, test=-3.669) total time=   1.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.280, test=-3.690) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.613, test=-3.422) total time=   2.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0.1], mlp__hiddens=(4, 4, 8, 16, 64, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.804, test=-3.807) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.142, test=-13.103) total time=   5.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.312, test=-13.772) total time=   5.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.065, test=-17.060) total time=   5.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.374, test=-11.210) total time=   5.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.1 0.1 0.2 0.  0.1], mlp__hiddens=(4, 8, 16, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.900, test=-9.252) total time=   5.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.206, test=-3.738) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.451, test=-3.824) total time=   2.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.505, test=-3.437) total time=   2.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.251, test=-2.926) total time=   2.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 16, 32, 8, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.434, test=-3.486) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.117, test=-3.748) total time=   1.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.035, test=-3.547) total time=   2.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.841, test=-3.275) total time=   2.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.121, test=-3.375) total time=   2.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.2 0.  0. ], mlp__hiddens=(32, 4, 4, 64, 32, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.035, test=-3.707) total time=   2.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.493, test=-5.655) total time=   6.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.939, test=-17.492) total time=   6.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.907, test=-17.620) total time=   7.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.185, test=-6.013) total time=   6.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.  0.  0.3 0.1], mlp__hiddens=(8, 4, 8, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.661, test=-6.664) total time=   6.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.014, test=-4.725) total time=   2.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.085, test=-4.586) total time=   2.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.033, test=-4.465) total time=   2.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.246, test=-4.049) total time=   2.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.3 0.1 0.2 0.3 0. ], mlp__hiddens=(8, 4, 4, 32, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-13.773, test=-13.127) total time=   6.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.294, test=-3.910) total time=   1.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.744, test=-3.927) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.674, test=-3.724) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.281, test=-3.525) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0.3], mlp__hiddens=(32, 64, 8, 4, 4, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.463, test=-3.946) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.277, test=-3.478) total time=   1.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.002, test=-3.617) total time=   1.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.007, test=-3.280) total time=   2.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.745, test=-3.948) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.3 0.2 0. ], mlp__hiddens=(8, 32, 16, 64, 4, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-2.878, test=-3.222) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.600, test=-4.959) total time=   1.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.692, test=-4.091) total time=   2.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.299, test=-4.243) total time=   1.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.591, test=-4.746) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.3 0.1 0.1 0. ], mlp__hiddens=(8, 64, 16, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.152, test=-5.167) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.078, test=-3.722) total time=   2.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.056, test=-3.633) total time=   2.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.113, test=-3.419) total time=   2.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.031, test=-3.939) total time=   1.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0.2], mlp__hiddens=(4, 4, 64, 16, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.733, test=-3.955) total time=   2.1s\n",
      "[CV 1/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.179, test=-4.040) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.330, test=-4.103) total time=   2.8s\n",
      "[CV 3/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-2.954, test=-3.284) total time=   2.6s\n",
      "[CV 4/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.409, test=-3.440) total time=   1.8s\n",
      "[CV 5/5] END mlp__dropouts=[0. 0. 0. 0. 0. 0.], mlp__hiddens=(16, 32, 16, 8, 8, 8), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.388, test=-3.751) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.064, test=-5.028) total time=   3.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.293, test=-4.834) total time=   4.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.596, test=-4.683) total time=   4.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.934, test=-5.325) total time=   2.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.3 0.  0.1 0.  0.1], mlp__hiddens=(32, 64, 32, 4, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.611, test=-4.789) total time=   4.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.243, test=-3.990) total time=   3.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.319, test=-3.718) total time=   2.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.416, test=-3.732) total time=   2.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.542, test=-3.876) total time=   2.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.  0.1 0.  0. ], mlp__hiddens=(16, 32, 64, 16, 32, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.582, test=-4.047) total time=   2.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.775, test=-7.938) total time=   2.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.167, test=-6.661) total time=   6.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.067, test=-4.089) total time=   5.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.444, test=-5.522) total time=   5.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0. ], mlp__hiddens=(4, 64, 4, 16, 4, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-17.179, test=-16.533) total time=   5.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.604, test=-3.709) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.994, test=-3.510) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-5.109, test=-5.547) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.939, test=-3.968) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.2 0.2 0.3 0.3], mlp__hiddens=(8, 4, 8, 16, 8, 4), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.920, test=-3.959) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.788, test=-5.616) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.216, test=-6.736) total time=   3.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.913, test=-7.227) total time=   2.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.089, test=-7.422) total time=   2.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.2 0.3 0.3 0.3], mlp__hiddens=(32, 32, 64, 4, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.636, test=-5.819) total time=   3.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.616, test=-3.870) total time=   1.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.252, test=-3.730) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.922, test=-3.291) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.057, test=-3.057) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.3 0.3], mlp__hiddens=(8, 16, 8, 32, 8, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.654, test=-3.182) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.029, test=-4.545) total time=   4.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.000, test=-4.428) total time=   4.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.564, test=-3.999) total time=   4.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.511, test=-3.775) total time=   6.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.3], mlp__hiddens=(8, 32, 32, 32, 16, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.339, test=-3.720) total time=   6.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.947, test=-3.698) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.233, test=-3.936) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.793, test=-3.568) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.189, test=-3.434) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.1 0.  0.2], mlp__hiddens=(64, 16, 32, 16, 32, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.242, test=-3.822) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.073, test=-4.060) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.199, test=-3.887) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.104, test=-3.583) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.496, test=-3.563) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.2 0.2], mlp__hiddens=(32, 64, 64, 32, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.058, test=-3.877) total time=   1.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.520, test=-3.760) total time=   3.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-13.532, test=-14.085) total time=   5.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.066, test=-3.475) total time=   2.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.308, test=-3.195) total time=   2.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.2 0.3], mlp__hiddens=(8, 32, 4, 8, 16, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.837, test=-4.154) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.858, test=-3.624) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.878, test=-3.650) total time=   2.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.433, test=-3.337) total time=   1.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.142, test=-3.586) total time=   1.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.3 0.1 0.  0. ], mlp__hiddens=(32, 8, 32, 32, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.991, test=-3.666) total time=   2.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.032, test=-3.357) total time=   2.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.293, test=-3.952) total time=   1.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.188, test=-3.121) total time=   1.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.491, test=-3.321) total time=   1.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.3 0.3 0. ], mlp__hiddens=(4, 64, 32, 8, 8, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.274, test=-3.708) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-2.959, test=-3.655) total time=   2.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.184, test=-3.856) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.014, test=-3.252) total time=   2.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.417, test=-3.395) total time=   1.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.  0.2 0.2], mlp__hiddens=(8, 32, 32, 16, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.070, test=-3.689) total time=   2.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.837, test=-3.257) total time=   1.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.138, test=-3.735) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.500, test=-3.914) total time=   2.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.471, test=-3.432) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.1 0.1], mlp__hiddens=(32, 4, 32, 8, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.694, test=-3.982) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.658, test=-3.841) total time=   1.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.306, test=-3.543) total time=   1.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.938, test=-4.301) total time=   1.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.235, test=-4.061) total time=   2.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0.3], mlp__hiddens=(4, 32, 16, 32, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.827, test=-3.986) total time=   1.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.969, test=-3.649) total time=   1.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.054, test=-3.936) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.697, test=-3.335) total time=   1.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.034, test=-3.436) total time=   1.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.1 0.  0.3 0.2], mlp__hiddens=(64, 64, 64, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.925, test=-3.746) total time=   1.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.483, test=-3.801) total time=   1.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.011, test=-3.589) total time=   1.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.564, test=-3.681) total time=   1.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.022, test=-3.006) total time=   1.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.  0.1], mlp__hiddens=(64, 4, 32, 4, 32, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.536, test=-3.923) total time=   1.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-17.147, test=-16.660) total time=   5.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.507, test=-9.657) total time=   5.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.190, test=-8.611) total time=   5.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.952, test=-14.079) total time=   5.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.2 0.3 0.2 0. ], mlp__hiddens=(4, 4, 4, 4, 64, 64), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.187, test=-9.618) total time=   5.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.123, test=-3.445) total time=   1.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.881, test=-3.440) total time=   1.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.200, test=-3.266) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-3.204, test=-2.979) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.2 0. ], mlp__hiddens=(4, 64, 8, 4, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.005;, score=(train=-2.884, test=-3.099) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.969, test=-3.194) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.900, test=-3.603) total time=   1.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.490, test=-3.543) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.435, test=-4.590) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.  0.1], mlp__hiddens=(8, 16, 8, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.310, test=-3.488) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.051, test=-3.710) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.392, test=-3.950) total time=   1.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.194, test=-3.302) total time=   1.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.922, test=-3.252) total time=   2.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 8, 8, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.226, test=-3.730) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.900, test=-4.316) total time=   1.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.995, test=-5.210) total time=   1.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.869, test=-5.075) total time=   1.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.366, test=-4.573) total time=   1.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.1 0. ], mlp__hiddens=(32, 32, 8, 8, 32, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.432, test=-4.553) total time=   1.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.141, test=-3.704) total time=   1.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.010, test=-3.683) total time=   1.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.783, test=-3.071) total time=   1.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.041, test=-3.260) total time=   1.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.1 0.2 0.2], mlp__hiddens=(32, 16, 64, 16, 32, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.701, test=-3.544) total time=   1.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.939, test=-9.293) total time=   7.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.808, test=-8.976) total time=   6.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-14.355, test=-14.680) total time=   5.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.149, test=-7.658) total time=   5.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.3 0.2 0.3 0.  0. ], mlp__hiddens=(4, 8, 4, 16, 4, 32), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.180, test=-15.657) total time=   5.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.161, test=-4.205) total time=   4.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.743, test=-6.863) total time=   2.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.454, test=-4.785) total time=   5.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.723, test=-3.750) total time=   6.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.1 0.  0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.295, test=-4.493) total time=   4.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.304, test=-3.749) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.469, test=-3.845) total time=   2.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.098, test=-3.430) total time=   2.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.089, test=-2.948) total time=   2.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.1 0.  0.  0.3], mlp__hiddens=(8, 16, 4, 16, 4, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.087, test=-4.376) total time=   1.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.512, test=-5.294) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.287, test=-6.765) total time=   2.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.947, test=-6.004) total time=   3.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.070, test=-5.161) total time=   4.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.  0.2 0.1 0. ], mlp__hiddens=(8, 64, 8, 8, 64, 64), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.157, test=-5.176) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.847, test=-9.026) total time=   5.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.127, test=-8.612) total time=   5.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.089, test=-11.622) total time=   5.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.278, test=-16.186) total time=   5.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.2 0.1 0.3 0.  0.1], mlp__hiddens=(4, 4, 4, 16, 16, 8), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-10.790, test=-10.616) total time=   5.7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[(&#x27;scaler&#x27;, PowerTransformer()),\n",
       "                                             (&#x27;principal_components&#x27;,\n",
       "                                              PCA(n_components=20,\n",
       "                                                  svd_solver=&#x27;full&#x27;)),\n",
       "                                             (&#x27;mlp&#x27;,\n",
       "                                              KerasRegressor(callbacks=[&lt;keras.callbacks.EarlyStopping object at 0x7fd76c713640&gt;], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss=&#x27;mse&#x27;, model=&lt;functio...\n",
       "                                                         (4, 4, 4, 4, 32, 32),\n",
       "                                                         (4, 4, 4, 4, 32, 64),\n",
       "                                                         (4, 4, 4, 4, 64, 4),\n",
       "                                                         (4, 4, 4, 4, 64, 8),\n",
       "                                                         (4, 4, 4, 4, 64, 16),\n",
       "                                                         (4, 4, 4, 4, 64, 32),\n",
       "                                                         (4, 4, 4, 4, 64, 64),\n",
       "                                                         (4, 4, 4, 8, 4, 4),\n",
       "                                                         (4, 4, 4, 8, 4, 8),\n",
       "                                                         (4, 4, 4, 8, 4, 16),\n",
       "                                                         (4, 4, 4, 8, 4, 32),\n",
       "                                                         (4, 4, 4, 8, 4, 64), ...],\n",
       "                                        &#x27;mlp__nlayers&#x27;: array([1, 6, 3, ..., 5, 3, 3]),\n",
       "                                        &#x27;mlp__optimizer__learning_rate&#x27;: [0.0001,\n",
       "                                                                          0.001,\n",
       "                                                                          0.005]},\n",
       "                   refit=False, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[(&#x27;scaler&#x27;, PowerTransformer()),\n",
       "                                             (&#x27;principal_components&#x27;,\n",
       "                                              PCA(n_components=20,\n",
       "                                                  svd_solver=&#x27;full&#x27;)),\n",
       "                                             (&#x27;mlp&#x27;,\n",
       "                                              KerasRegressor(callbacks=[&lt;keras.callbacks.EarlyStopping object at 0x7fd76c713640&gt;], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss=&#x27;mse&#x27;, model=&lt;functio...\n",
       "                                                         (4, 4, 4, 4, 32, 32),\n",
       "                                                         (4, 4, 4, 4, 32, 64),\n",
       "                                                         (4, 4, 4, 4, 64, 4),\n",
       "                                                         (4, 4, 4, 4, 64, 8),\n",
       "                                                         (4, 4, 4, 4, 64, 16),\n",
       "                                                         (4, 4, 4, 4, 64, 32),\n",
       "                                                         (4, 4, 4, 4, 64, 64),\n",
       "                                                         (4, 4, 4, 8, 4, 4),\n",
       "                                                         (4, 4, 4, 8, 4, 8),\n",
       "                                                         (4, 4, 4, 8, 4, 16),\n",
       "                                                         (4, 4, 4, 8, 4, 32),\n",
       "                                                         (4, 4, 4, 8, 4, 64), ...],\n",
       "                                        &#x27;mlp__nlayers&#x27;: array([1, 6, 3, ..., 5, 3, 3]),\n",
       "                                        &#x27;mlp__optimizer__learning_rate&#x27;: [0.0001,\n",
       "                                                                          0.001,\n",
       "                                                                          0.005]},\n",
       "                   refit=False, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, PowerTransformer()),\n",
       "                (&#x27;principal_components&#x27;,\n",
       "                 PCA(n_components=20, svd_solver=&#x27;full&#x27;)),\n",
       "                (&#x27;mlp&#x27;,\n",
       "                 KerasRegressor(callbacks=[&lt;keras.callbacks.EarlyStopping object at 0x7fd76c713640&gt;], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss=&#x27;mse&#x27;, model=&lt;function twoLayerFeedForward at 0x7fd708e42790&gt;, nlayers=3, validation_split=0.2, verbose=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PowerTransformer</label><div class=\"sk-toggleable__content\"><pre>PowerTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=20, svd_solver=&#x27;full&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function twoLayerFeedForward at 0x7fd708e42790&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=mse\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=[&lt;keras.callbacks.EarlyStopping object at 0x7fd76c713640&gt;]\n",
       "\tvalidation_split=0.2\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=200\n",
       "\tnlayers=3\n",
       "\thiddens=[2, 2, 2]\n",
       "\tdropouts=[0.2, 0, 0]\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[('scaler', PowerTransformer()),\n",
       "                                             ('principal_components',\n",
       "                                              PCA(n_components=20,\n",
       "                                                  svd_solver='full')),\n",
       "                                             ('mlp',\n",
       "                                              KerasRegressor(callbacks=[<keras.callbacks.EarlyStopping object at 0x7fd76c713640>], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss='mse', model=<functio...\n",
       "                                                         (4, 4, 4, 4, 32, 32),\n",
       "                                                         (4, 4, 4, 4, 32, 64),\n",
       "                                                         (4, 4, 4, 4, 64, 4),\n",
       "                                                         (4, 4, 4, 4, 64, 8),\n",
       "                                                         (4, 4, 4, 4, 64, 16),\n",
       "                                                         (4, 4, 4, 4, 64, 32),\n",
       "                                                         (4, 4, 4, 4, 64, 64),\n",
       "                                                         (4, 4, 4, 8, 4, 4),\n",
       "                                                         (4, 4, 4, 8, 4, 8),\n",
       "                                                         (4, 4, 4, 8, 4, 16),\n",
       "                                                         (4, 4, 4, 8, 4, 32),\n",
       "                                                         (4, 4, 4, 8, 4, 64), ...],\n",
       "                                        'mlp__nlayers': array([1, 6, 3, ..., 5, 3, 3]),\n",
       "                                        'mlp__optimizer__learning_rate': [0.0001,\n",
       "                                                                          0.001,\n",
       "                                                                          0.005]},\n",
       "                   refit=False, return_train_score=True,\n",
       "                   scoring='neg_mean_absolute_error', verbose=3)"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(df.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_overfit = df_results[\"mean_train_score\"] - df_results[\"mean_test_score\"] < 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_mlp__optimizer__learning_rate</th>\n",
       "      <th>param_mlp__nlayers</th>\n",
       "      <th>param_mlp__hiddens</th>\n",
       "      <th>param_mlp__dropouts</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.381248</td>\n",
       "      <td>0.038405</td>\n",
       "      <td>0.068078</td>\n",
       "      <td>0.005896</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 64, 8, 4, 8, 16)</td>\n",
       "      <td>[0.1, 0.0, 0.2, 0.0, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.444679</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.245788</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.123399</td>\n",
       "      <td>-2.881489</td>\n",
       "      <td>-3.200226</td>\n",
       "      <td>-3.204078</td>\n",
       "      <td>-2.883873</td>\n",
       "      <td>-3.058613</td>\n",
       "      <td>0.146505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2.399896</td>\n",
       "      <td>0.283920</td>\n",
       "      <td>0.072665</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 4, 32, 8, 8, 16)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.1, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.502558</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.306568</td>\n",
       "      <td>0.217319</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.202178</td>\n",
       "      <td>-3.265401</td>\n",
       "      <td>-3.032918</td>\n",
       "      <td>-3.439771</td>\n",
       "      <td>-3.160560</td>\n",
       "      <td>-3.220165</td>\n",
       "      <td>0.133575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.196659</td>\n",
       "      <td>0.183173</td>\n",
       "      <td>0.065548</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 32, 16, 8, 32, 4)</td>\n",
       "      <td>[0.1, 0.2, 0.2, 0.0, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.510855</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.390546</td>\n",
       "      <td>0.302877</td>\n",
       "      <td>3</td>\n",
       "      <td>-3.142341</td>\n",
       "      <td>-3.310471</td>\n",
       "      <td>-3.101906</td>\n",
       "      <td>-3.179767</td>\n",
       "      <td>-3.135069</td>\n",
       "      <td>-3.173911</td>\n",
       "      <td>0.072626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.448093</td>\n",
       "      <td>0.160615</td>\n",
       "      <td>0.075976</td>\n",
       "      <td>0.004562</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 64, 8, 32, 16, 16)</td>\n",
       "      <td>[0.2, 0.0, 0.1, 0.0, 0.3, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.207207</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.451918</td>\n",
       "      <td>0.515716</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.128185</td>\n",
       "      <td>-3.862343</td>\n",
       "      <td>-3.072557</td>\n",
       "      <td>-3.394035</td>\n",
       "      <td>-3.495929</td>\n",
       "      <td>-3.390610</td>\n",
       "      <td>0.284134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.277616</td>\n",
       "      <td>0.060783</td>\n",
       "      <td>0.070754</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(16, 16, 32, 8, 32, 4)</td>\n",
       "      <td>[0.2, 0.1, 0.1, 0.0, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.679821</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.457536</td>\n",
       "      <td>0.185252</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.400886</td>\n",
       "      <td>-2.870856</td>\n",
       "      <td>-3.252017</td>\n",
       "      <td>-3.121352</td>\n",
       "      <td>-3.304009</td>\n",
       "      <td>-3.189824</td>\n",
       "      <td>0.183239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2.156716</td>\n",
       "      <td>0.165973</td>\n",
       "      <td>0.071492</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 16, 32, 8, 4, 8)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.3, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.738486</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.482312</td>\n",
       "      <td>0.314462</td>\n",
       "      <td>9</td>\n",
       "      <td>-3.206298</td>\n",
       "      <td>-3.450732</td>\n",
       "      <td>-3.505153</td>\n",
       "      <td>-3.251069</td>\n",
       "      <td>-3.434089</td>\n",
       "      <td>-3.369468</td>\n",
       "      <td>0.118180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.747765</td>\n",
       "      <td>0.127198</td>\n",
       "      <td>0.065332</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 64, 32, 8, 8, 8)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.3, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.356810</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.491877</td>\n",
       "      <td>0.297740</td>\n",
       "      <td>10</td>\n",
       "      <td>-3.032396</td>\n",
       "      <td>-3.292912</td>\n",
       "      <td>-3.188334</td>\n",
       "      <td>-3.490692</td>\n",
       "      <td>-3.274313</td>\n",
       "      <td>-3.255729</td>\n",
       "      <td>0.149235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.272680</td>\n",
       "      <td>0.062594</td>\n",
       "      <td>0.078090</td>\n",
       "      <td>0.004779</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(64, 16, 64, 4, 32, 8)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.0, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.984387</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.511034</td>\n",
       "      <td>0.294358</td>\n",
       "      <td>12</td>\n",
       "      <td>-3.639168</td>\n",
       "      <td>-3.225021</td>\n",
       "      <td>-3.512329</td>\n",
       "      <td>-3.147330</td>\n",
       "      <td>-2.962338</td>\n",
       "      <td>-3.297237</td>\n",
       "      <td>0.246181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.446125</td>\n",
       "      <td>0.064014</td>\n",
       "      <td>0.076042</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 32, 16, 64, 4, 4)</td>\n",
       "      <td>[0.0, 0.2, 0.2, 0.2, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.772745</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.548365</td>\n",
       "      <td>0.294348</td>\n",
       "      <td>14</td>\n",
       "      <td>-3.605911</td>\n",
       "      <td>-3.195032</td>\n",
       "      <td>-3.059955</td>\n",
       "      <td>-3.527445</td>\n",
       "      <td>-3.104705</td>\n",
       "      <td>-3.298610</td>\n",
       "      <td>0.224536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.121251</td>\n",
       "      <td>0.054270</td>\n",
       "      <td>0.064064</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "      <td>(64, 4, 32, 4, 32, 32)</td>\n",
       "      <td>[0.1, 0.2, 0.0, 0.1, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.801082</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.600028</td>\n",
       "      <td>0.317610</td>\n",
       "      <td>22</td>\n",
       "      <td>-3.483200</td>\n",
       "      <td>-3.011451</td>\n",
       "      <td>-3.564499</td>\n",
       "      <td>-3.021721</td>\n",
       "      <td>-3.536013</td>\n",
       "      <td>-3.323377</td>\n",
       "      <td>0.251870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2.074067</td>\n",
       "      <td>0.299716</td>\n",
       "      <td>0.068131</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(8, 16, 4, 16, 4, 16)</td>\n",
       "      <td>[0.0, 0.1, 0.1, 0.0, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.748797</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.669659</td>\n",
       "      <td>0.471889</td>\n",
       "      <td>28</td>\n",
       "      <td>-3.303509</td>\n",
       "      <td>-3.469351</td>\n",
       "      <td>-3.097847</td>\n",
       "      <td>-3.089463</td>\n",
       "      <td>-4.086803</td>\n",
       "      <td>-3.409395</td>\n",
       "      <td>0.366908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.484825</td>\n",
       "      <td>0.137363</td>\n",
       "      <td>0.075200</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 16, 8, 32, 16, 8)</td>\n",
       "      <td>[0.0, 0.2, 0.2, 0.3, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.194103</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.683814</td>\n",
       "      <td>0.474299</td>\n",
       "      <td>30</td>\n",
       "      <td>-2.969188</td>\n",
       "      <td>-2.899978</td>\n",
       "      <td>-3.489649</td>\n",
       "      <td>-4.435165</td>\n",
       "      <td>-3.310104</td>\n",
       "      <td>-3.420817</td>\n",
       "      <td>0.551568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1.545654</td>\n",
       "      <td>0.200134</td>\n",
       "      <td>0.069848</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 4, 8, 16, 64, 16)</td>\n",
       "      <td>[0.0, 0.3, 0.2, 0.2, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.032763</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.724189</td>\n",
       "      <td>0.198925</td>\n",
       "      <td>34</td>\n",
       "      <td>-4.076120</td>\n",
       "      <td>-3.204912</td>\n",
       "      <td>-3.280440</td>\n",
       "      <td>-3.612611</td>\n",
       "      <td>-3.803830</td>\n",
       "      <td>-3.595583</td>\n",
       "      <td>0.324503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.407048</td>\n",
       "      <td>0.058881</td>\n",
       "      <td>0.081968</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 64, 8, 32, 64, 16)</td>\n",
       "      <td>[0.2, 0.3, 0.0, 0.2, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.286118</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.727956</td>\n",
       "      <td>0.334774</td>\n",
       "      <td>35</td>\n",
       "      <td>-3.708094</td>\n",
       "      <td>-3.132101</td>\n",
       "      <td>-3.468539</td>\n",
       "      <td>-3.515535</td>\n",
       "      <td>-3.352134</td>\n",
       "      <td>-3.435281</td>\n",
       "      <td>0.190161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.290341</td>\n",
       "      <td>0.103828</td>\n",
       "      <td>0.069295</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(16, 32, 64, 32, 16, 64)</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0.0, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.278897</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.730299</td>\n",
       "      <td>0.433099</td>\n",
       "      <td>37</td>\n",
       "      <td>-3.993858</td>\n",
       "      <td>-3.169882</td>\n",
       "      <td>-2.808735</td>\n",
       "      <td>-4.060848</td>\n",
       "      <td>-3.607992</td>\n",
       "      <td>-3.528263</td>\n",
       "      <td>0.480196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.530041</td>\n",
       "      <td>0.246018</td>\n",
       "      <td>0.079734</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 16, 8, 16, 64, 8)</td>\n",
       "      <td>[0.0, 0.3, 0.3, 0.2, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.242633</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.836662</td>\n",
       "      <td>0.221970</td>\n",
       "      <td>45</td>\n",
       "      <td>-4.394014</td>\n",
       "      <td>-3.067084</td>\n",
       "      <td>-3.593900</td>\n",
       "      <td>-3.970299</td>\n",
       "      <td>-3.805948</td>\n",
       "      <td>-3.766249</td>\n",
       "      <td>0.437227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.502557</td>\n",
       "      <td>0.107721</td>\n",
       "      <td>0.079624</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(32, 16, 64, 64, 32, 32)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.3, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.260488</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.864190</td>\n",
       "      <td>0.312776</td>\n",
       "      <td>47</td>\n",
       "      <td>-4.088623</td>\n",
       "      <td>-3.439558</td>\n",
       "      <td>-3.575928</td>\n",
       "      <td>-3.251769</td>\n",
       "      <td>-3.628871</td>\n",
       "      <td>-3.596950</td>\n",
       "      <td>0.278253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1.379140</td>\n",
       "      <td>0.088124</td>\n",
       "      <td>0.079533</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 8, 32, 8, 8, 16)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.2, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.836822</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.921562</td>\n",
       "      <td>0.382886</td>\n",
       "      <td>49</td>\n",
       "      <td>-3.577503</td>\n",
       "      <td>-3.824137</td>\n",
       "      <td>-3.555268</td>\n",
       "      <td>-3.613887</td>\n",
       "      <td>-3.733543</td>\n",
       "      <td>-3.660868</td>\n",
       "      <td>0.102222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.781326</td>\n",
       "      <td>0.124139</td>\n",
       "      <td>0.078681</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(4, 32, 16, 32, 64, 32)</td>\n",
       "      <td>[0.0, 0.2, 0.0, 0.0, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.841363</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.946563</td>\n",
       "      <td>0.250703</td>\n",
       "      <td>50</td>\n",
       "      <td>-3.658225</td>\n",
       "      <td>-3.306484</td>\n",
       "      <td>-3.938358</td>\n",
       "      <td>-4.235332</td>\n",
       "      <td>-3.827102</td>\n",
       "      <td>-3.793100</td>\n",
       "      <td>0.307510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.078301</td>\n",
       "      <td>0.350160</td>\n",
       "      <td>0.069616</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(16, 8, 16, 16, 16, 4)</td>\n",
       "      <td>[0.1, 0.2, 0.3, 0.0, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.233727</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.979165</td>\n",
       "      <td>0.257731</td>\n",
       "      <td>52</td>\n",
       "      <td>-3.806590</td>\n",
       "      <td>-3.512726</td>\n",
       "      <td>-3.832004</td>\n",
       "      <td>-3.933008</td>\n",
       "      <td>-3.433693</td>\n",
       "      <td>-3.703604</td>\n",
       "      <td>0.194425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1.577962</td>\n",
       "      <td>0.328636</td>\n",
       "      <td>0.085771</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(32, 16, 32, 16, 64, 8)</td>\n",
       "      <td>[0.1, 0.3, 0.3, 0.3, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.929504</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.998526</td>\n",
       "      <td>0.731424</td>\n",
       "      <td>54</td>\n",
       "      <td>-4.026567</td>\n",
       "      <td>-3.470867</td>\n",
       "      <td>-3.408566</td>\n",
       "      <td>-3.492128</td>\n",
       "      <td>-5.077090</td>\n",
       "      <td>-3.895044</td>\n",
       "      <td>0.631421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.647439</td>\n",
       "      <td>0.057403</td>\n",
       "      <td>0.074555</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 64, 8, 16, 16, 4)</td>\n",
       "      <td>[0.0, 0.0, 0.1, 0.0, 0.1, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.986691</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.019573</td>\n",
       "      <td>0.230108</td>\n",
       "      <td>55</td>\n",
       "      <td>-3.509549</td>\n",
       "      <td>-3.720099</td>\n",
       "      <td>-3.881995</td>\n",
       "      <td>-3.830726</td>\n",
       "      <td>-4.027582</td>\n",
       "      <td>-3.793990</td>\n",
       "      <td>0.173220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.262394</td>\n",
       "      <td>1.964590</td>\n",
       "      <td>0.069721</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(64, 4, 64, 4, 4, 32)</td>\n",
       "      <td>[0.0, 0.1, 0.1, 0.3, 0.2, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.683450</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.068708</td>\n",
       "      <td>0.881039</td>\n",
       "      <td>56</td>\n",
       "      <td>-3.042402</td>\n",
       "      <td>-3.743024</td>\n",
       "      <td>-3.109341</td>\n",
       "      <td>-3.579479</td>\n",
       "      <td>-6.030831</td>\n",
       "      <td>-3.901015</td>\n",
       "      <td>1.098037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1.819248</td>\n",
       "      <td>0.434627</td>\n",
       "      <td>0.066853</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(8, 64, 32, 16, 32, 32)</td>\n",
       "      <td>[0.3, 0.0, 0.3, 0.3, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.931859</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.073323</td>\n",
       "      <td>0.342279</td>\n",
       "      <td>57</td>\n",
       "      <td>-3.933999</td>\n",
       "      <td>-4.095889</td>\n",
       "      <td>-3.454255</td>\n",
       "      <td>-3.810474</td>\n",
       "      <td>-4.124846</td>\n",
       "      <td>-3.883893</td>\n",
       "      <td>0.243101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.340920</td>\n",
       "      <td>0.196845</td>\n",
       "      <td>0.068869</td>\n",
       "      <td>0.009164</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "      <td>(8, 4, 8, 16, 8, 4)</td>\n",
       "      <td>[0.1, 0.3, 0.2, 0.2, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.709024</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.138777</td>\n",
       "      <td>0.724627</td>\n",
       "      <td>60</td>\n",
       "      <td>-3.604376</td>\n",
       "      <td>-2.993532</td>\n",
       "      <td>-5.108695</td>\n",
       "      <td>-3.939392</td>\n",
       "      <td>-3.920404</td>\n",
       "      <td>-3.913280</td>\n",
       "      <td>0.688637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.262931</td>\n",
       "      <td>0.065133</td>\n",
       "      <td>0.073932</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 8, 64, 8, 32, 32)</td>\n",
       "      <td>[0.3, 0.0, 0.2, 0.3, 0.0, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-5.431517</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.160269</td>\n",
       "      <td>0.842450</td>\n",
       "      <td>61</td>\n",
       "      <td>-5.637015</td>\n",
       "      <td>-4.289149</td>\n",
       "      <td>-3.363489</td>\n",
       "      <td>-4.305180</td>\n",
       "      <td>-3.246187</td>\n",
       "      <td>-4.168204</td>\n",
       "      <td>0.858892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.517129</td>\n",
       "      <td>0.100984</td>\n",
       "      <td>0.075037</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(64, 4, 8, 8, 64, 8)</td>\n",
       "      <td>[0.0, 0.2, 0.0, 0.1, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.883612</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.188626</td>\n",
       "      <td>0.494258</td>\n",
       "      <td>62</td>\n",
       "      <td>-3.632736</td>\n",
       "      <td>-3.861969</td>\n",
       "      <td>-3.327079</td>\n",
       "      <td>-4.332283</td>\n",
       "      <td>-4.422741</td>\n",
       "      <td>-3.915362</td>\n",
       "      <td>0.414744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.310591</td>\n",
       "      <td>0.159938</td>\n",
       "      <td>0.068115</td>\n",
       "      <td>0.005472</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3</td>\n",
       "      <td>(8, 16, 4, 16, 16, 64)</td>\n",
       "      <td>[0.1, 0.0, 0.3, 0.0, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.868113</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.378028</td>\n",
       "      <td>0.732928</td>\n",
       "      <td>66</td>\n",
       "      <td>-3.780779</td>\n",
       "      <td>-2.866882</td>\n",
       "      <td>-4.889332</td>\n",
       "      <td>-4.257324</td>\n",
       "      <td>-5.248864</td>\n",
       "      <td>-4.208636</td>\n",
       "      <td>0.840376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.399842</td>\n",
       "      <td>0.570349</td>\n",
       "      <td>0.071272</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 4, 32, 8, 8, 4)</td>\n",
       "      <td>[0.3, 0.1, 0.0, 0.3, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.713081</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.629266</td>\n",
       "      <td>0.699482</td>\n",
       "      <td>67</td>\n",
       "      <td>-3.792214</td>\n",
       "      <td>-4.001750</td>\n",
       "      <td>-3.996552</td>\n",
       "      <td>-5.039602</td>\n",
       "      <td>-5.944006</td>\n",
       "      <td>-4.554825</td>\n",
       "      <td>0.820240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1.752546</td>\n",
       "      <td>0.231012</td>\n",
       "      <td>0.076944</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 64, 16, 64, 4, 16)</td>\n",
       "      <td>[0.1, 0.3, 0.3, 0.1, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.959479</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.641373</td>\n",
       "      <td>0.412405</td>\n",
       "      <td>68</td>\n",
       "      <td>-4.600227</td>\n",
       "      <td>-3.691846</td>\n",
       "      <td>-4.298586</td>\n",
       "      <td>-4.591487</td>\n",
       "      <td>-5.152407</td>\n",
       "      <td>-4.466911</td>\n",
       "      <td>0.475869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.769110</td>\n",
       "      <td>0.134735</td>\n",
       "      <td>0.079284</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(64, 8, 64, 32, 4, 64)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.2, 0.1, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.518302</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.710017</td>\n",
       "      <td>0.157673</td>\n",
       "      <td>69</td>\n",
       "      <td>-4.262499</td>\n",
       "      <td>-4.325011</td>\n",
       "      <td>-4.487673</td>\n",
       "      <td>-4.733686</td>\n",
       "      <td>-4.507604</td>\n",
       "      <td>-4.463295</td>\n",
       "      <td>0.164384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.372091</td>\n",
       "      <td>0.345872</td>\n",
       "      <td>0.057939</td>\n",
       "      <td>0.002211</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(32, 8, 8, 4, 16, 4)</td>\n",
       "      <td>[0.3, 0.0, 0.1, 0.1, 0.0, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.562274</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.735709</td>\n",
       "      <td>0.472276</td>\n",
       "      <td>70</td>\n",
       "      <td>-4.626635</td>\n",
       "      <td>-4.195252</td>\n",
       "      <td>-4.067453</td>\n",
       "      <td>-4.284728</td>\n",
       "      <td>-5.144372</td>\n",
       "      <td>-4.463688</td>\n",
       "      <td>0.387548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.520440</td>\n",
       "      <td>0.151489</td>\n",
       "      <td>0.080155</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 32, 8, 8, 32, 16)</td>\n",
       "      <td>[0.0, 0.0, 0.1, 0.3, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.315631</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.745196</td>\n",
       "      <td>0.339333</td>\n",
       "      <td>71</td>\n",
       "      <td>-3.899865</td>\n",
       "      <td>-4.994592</td>\n",
       "      <td>-4.869039</td>\n",
       "      <td>-4.365901</td>\n",
       "      <td>-4.432202</td>\n",
       "      <td>-4.512320</td>\n",
       "      <td>0.390587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4.577236</td>\n",
       "      <td>0.873900</td>\n",
       "      <td>0.070861</td>\n",
       "      <td>0.001916</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(64, 16, 4, 4, 4, 64)</td>\n",
       "      <td>[0.0, 0.3, 0.0, 0.1, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.147595</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.745275</td>\n",
       "      <td>0.434045</td>\n",
       "      <td>72</td>\n",
       "      <td>-5.032434</td>\n",
       "      <td>-4.783619</td>\n",
       "      <td>-4.198656</td>\n",
       "      <td>-3.924726</td>\n",
       "      <td>-4.367122</td>\n",
       "      <td>-4.461311</td>\n",
       "      <td>0.398960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>4.654827</td>\n",
       "      <td>1.034928</td>\n",
       "      <td>0.075963</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 32, 4, 32, 4, 32)</td>\n",
       "      <td>[0.1, 0.1, 0.0, 0.0, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.205419</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.819183</td>\n",
       "      <td>1.077468</td>\n",
       "      <td>73</td>\n",
       "      <td>-4.161263</td>\n",
       "      <td>-6.743170</td>\n",
       "      <td>-4.453614</td>\n",
       "      <td>-3.723066</td>\n",
       "      <td>-4.294684</td>\n",
       "      <td>-4.675159</td>\n",
       "      <td>1.062175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>4.841628</td>\n",
       "      <td>0.642366</td>\n",
       "      <td>0.071950</td>\n",
       "      <td>0.009444</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 8, 64, 64, 16, 4)</td>\n",
       "      <td>[0.2, 0.1, 0.0, 0.0, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.032173</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.875657</td>\n",
       "      <td>0.388905</td>\n",
       "      <td>74</td>\n",
       "      <td>-4.985263</td>\n",
       "      <td>-5.051864</td>\n",
       "      <td>-4.647574</td>\n",
       "      <td>-4.604249</td>\n",
       "      <td>-4.588098</td>\n",
       "      <td>-4.775410</td>\n",
       "      <td>0.200594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.870517</td>\n",
       "      <td>0.275743</td>\n",
       "      <td>0.065118</td>\n",
       "      <td>0.001494</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>(16, 4, 16, 8, 32, 8)</td>\n",
       "      <td>[0.1, 0.0, 0.3, 0.2, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.627315</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.918170</td>\n",
       "      <td>0.366687</td>\n",
       "      <td>75</td>\n",
       "      <td>-4.383821</td>\n",
       "      <td>-4.264166</td>\n",
       "      <td>-5.404635</td>\n",
       "      <td>-4.585191</td>\n",
       "      <td>-4.508484</td>\n",
       "      <td>-4.629259</td>\n",
       "      <td>0.402807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>4.626636</td>\n",
       "      <td>1.202737</td>\n",
       "      <td>0.072876</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 16, 64, 64, 64, 16)</td>\n",
       "      <td>[0.1, 0.2, 0.0, 0.3, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-6.522488</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.918541</td>\n",
       "      <td>0.844340</td>\n",
       "      <td>76</td>\n",
       "      <td>-6.403205</td>\n",
       "      <td>-3.755758</td>\n",
       "      <td>-3.805934</td>\n",
       "      <td>-4.307264</td>\n",
       "      <td>-5.557701</td>\n",
       "      <td>-4.765973</td>\n",
       "      <td>1.044987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>3.711105</td>\n",
       "      <td>0.864177</td>\n",
       "      <td>0.078757</td>\n",
       "      <td>0.005837</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 64, 32, 4, 4, 8)</td>\n",
       "      <td>[0.3, 0.3, 0.0, 0.1, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.027965</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.931777</td>\n",
       "      <td>0.226004</td>\n",
       "      <td>77</td>\n",
       "      <td>-5.064423</td>\n",
       "      <td>-4.292847</td>\n",
       "      <td>-4.596335</td>\n",
       "      <td>-4.934298</td>\n",
       "      <td>-4.610921</td>\n",
       "      <td>-4.699765</td>\n",
       "      <td>0.272820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.205330</td>\n",
       "      <td>0.662003</td>\n",
       "      <td>0.075952</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 4, 32, 16, 8, 64)</td>\n",
       "      <td>[0.0, 0.0, 0.3, 0.0, 0.3, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.209240</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.936507</td>\n",
       "      <td>0.429749</td>\n",
       "      <td>78</td>\n",
       "      <td>-4.749343</td>\n",
       "      <td>-4.827422</td>\n",
       "      <td>-3.885274</td>\n",
       "      <td>-4.718909</td>\n",
       "      <td>-5.010941</td>\n",
       "      <td>-4.638378</td>\n",
       "      <td>0.390001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.870194</td>\n",
       "      <td>0.267567</td>\n",
       "      <td>0.070096</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(32, 4, 4, 32, 8, 32)</td>\n",
       "      <td>[0.2, 0.3, 0.1, 0.0, 0.2, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.693189</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.945885</td>\n",
       "      <td>0.592438</td>\n",
       "      <td>79</td>\n",
       "      <td>-4.537680</td>\n",
       "      <td>-3.665157</td>\n",
       "      <td>-4.578929</td>\n",
       "      <td>-5.228545</td>\n",
       "      <td>-5.610092</td>\n",
       "      <td>-4.724081</td>\n",
       "      <td>0.665828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.615868</td>\n",
       "      <td>0.196203</td>\n",
       "      <td>0.078078</td>\n",
       "      <td>0.003747</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(64, 8, 32, 64, 8, 64)</td>\n",
       "      <td>[0.2, 0.3, 0.0, 0.3, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.366928</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.263866</td>\n",
       "      <td>0.220901</td>\n",
       "      <td>81</td>\n",
       "      <td>-5.123247</td>\n",
       "      <td>-4.947372</td>\n",
       "      <td>-4.975546</td>\n",
       "      <td>-5.205015</td>\n",
       "      <td>-5.030421</td>\n",
       "      <td>-5.056320</td>\n",
       "      <td>0.095556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.737277</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>0.083777</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 16, 4, 32, 64, 8)</td>\n",
       "      <td>[0.3, 0.3, 0.0, 0.0, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.464281</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.447851</td>\n",
       "      <td>0.084832</td>\n",
       "      <td>82</td>\n",
       "      <td>-5.809070</td>\n",
       "      <td>-4.933755</td>\n",
       "      <td>-5.593369</td>\n",
       "      <td>-5.546139</td>\n",
       "      <td>-5.232346</td>\n",
       "      <td>-5.422936</td>\n",
       "      <td>0.306251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6.096340</td>\n",
       "      <td>0.386804</td>\n",
       "      <td>0.069647</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(8, 4, 8, 32, 16, 32)</td>\n",
       "      <td>[0.1, 0.0, 0.3, 0.3, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.900061</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.517619</td>\n",
       "      <td>0.534699</td>\n",
       "      <td>83</td>\n",
       "      <td>-5.819024</td>\n",
       "      <td>-4.232330</td>\n",
       "      <td>-5.493301</td>\n",
       "      <td>-5.591993</td>\n",
       "      <td>-5.701148</td>\n",
       "      <td>-5.367559</td>\n",
       "      <td>0.577932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.274352</td>\n",
       "      <td>0.052584</td>\n",
       "      <td>0.077322</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 16, 8, 4, 32, 4)</td>\n",
       "      <td>[0.0, 0.3, 0.2, 0.2, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-5.245757</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.622228</td>\n",
       "      <td>1.165520</td>\n",
       "      <td>84</td>\n",
       "      <td>-4.948552</td>\n",
       "      <td>-4.412811</td>\n",
       "      <td>-5.256732</td>\n",
       "      <td>-7.782584</td>\n",
       "      <td>-4.657990</td>\n",
       "      <td>-5.411734</td>\n",
       "      <td>1.218644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.349942</td>\n",
       "      <td>0.441901</td>\n",
       "      <td>0.082150</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 64, 16, 4, 8, 64)</td>\n",
       "      <td>[0.3, 0.1, 0.3, 0.0, 0.1, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.407163</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.664937</td>\n",
       "      <td>0.325811</td>\n",
       "      <td>86</td>\n",
       "      <td>-5.151940</td>\n",
       "      <td>-5.793719</td>\n",
       "      <td>-5.084618</td>\n",
       "      <td>-5.509892</td>\n",
       "      <td>-5.740507</td>\n",
       "      <td>-5.456135</td>\n",
       "      <td>0.292670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>3.276229</td>\n",
       "      <td>0.806072</td>\n",
       "      <td>0.220423</td>\n",
       "      <td>0.285118</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 64, 8, 8, 64, 64)</td>\n",
       "      <td>[0.2, 0.2, 0.0, 0.2, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.294130</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.680199</td>\n",
       "      <td>0.625255</td>\n",
       "      <td>87</td>\n",
       "      <td>-5.511517</td>\n",
       "      <td>-6.286701</td>\n",
       "      <td>-5.946515</td>\n",
       "      <td>-5.070109</td>\n",
       "      <td>-5.157332</td>\n",
       "      <td>-5.594435</td>\n",
       "      <td>0.463906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.875356</td>\n",
       "      <td>1.411328</td>\n",
       "      <td>0.064165</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(8, 32, 4, 8, 16, 16)</td>\n",
       "      <td>[0.1, 0.0, 0.1, 0.0, 0.2, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.760444</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.733932</td>\n",
       "      <td>4.187808</td>\n",
       "      <td>88</td>\n",
       "      <td>-3.519809</td>\n",
       "      <td>-13.532279</td>\n",
       "      <td>-3.066280</td>\n",
       "      <td>-3.308044</td>\n",
       "      <td>-3.837000</td>\n",
       "      <td>-5.452682</td>\n",
       "      <td>4.047733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.669213</td>\n",
       "      <td>0.150287</td>\n",
       "      <td>0.066952</td>\n",
       "      <td>0.003482</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 16, 32, 4, 8, 8)</td>\n",
       "      <td>[0.2, 0.3, 0.0, 0.3, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.544407</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.037519</td>\n",
       "      <td>0.883102</td>\n",
       "      <td>89</td>\n",
       "      <td>-5.679337</td>\n",
       "      <td>-5.053432</td>\n",
       "      <td>-7.487936</td>\n",
       "      <td>-5.580757</td>\n",
       "      <td>-5.893959</td>\n",
       "      <td>-5.939084</td>\n",
       "      <td>0.822341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.952600</td>\n",
       "      <td>1.658953</td>\n",
       "      <td>0.077643</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 4, 4, 32, 64, 4)</td>\n",
       "      <td>[0.3, 0.3, 0.1, 0.2, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.724928</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.190416</td>\n",
       "      <td>3.475616</td>\n",
       "      <td>90</td>\n",
       "      <td>-5.013691</td>\n",
       "      <td>-4.084607</td>\n",
       "      <td>-4.033420</td>\n",
       "      <td>-4.245999</td>\n",
       "      <td>-13.772973</td>\n",
       "      <td>-6.230138</td>\n",
       "      <td>3.787870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2.814177</td>\n",
       "      <td>0.214926</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 32, 64, 4, 32, 4)</td>\n",
       "      <td>[0.2, 0.2, 0.2, 0.3, 0.3, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.616088</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.564017</td>\n",
       "      <td>0.729446</td>\n",
       "      <td>91</td>\n",
       "      <td>-5.788209</td>\n",
       "      <td>-6.216347</td>\n",
       "      <td>-6.913198</td>\n",
       "      <td>-7.088507</td>\n",
       "      <td>-5.635606</td>\n",
       "      <td>-6.328374</td>\n",
       "      <td>0.583794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>4.946305</td>\n",
       "      <td>0.890021</td>\n",
       "      <td>0.074963</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(16, 16, 16, 16, 4, 16)</td>\n",
       "      <td>[0.3, 0.0, 0.0, 0.3, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.973960</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.500001</td>\n",
       "      <td>4.313533</td>\n",
       "      <td>92</td>\n",
       "      <td>-4.826792</td>\n",
       "      <td>-5.119690</td>\n",
       "      <td>-5.563558</td>\n",
       "      <td>-16.146329</td>\n",
       "      <td>-4.837408</td>\n",
       "      <td>-7.298756</td>\n",
       "      <td>4.431876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5.057990</td>\n",
       "      <td>1.269600</td>\n",
       "      <td>0.065355</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 64, 4, 16, 4, 4)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-7.938048</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.148529</td>\n",
       "      <td>4.380331</td>\n",
       "      <td>93</td>\n",
       "      <td>-7.774635</td>\n",
       "      <td>-6.167044</td>\n",
       "      <td>-4.067231</td>\n",
       "      <td>-5.443891</td>\n",
       "      <td>-17.179002</td>\n",
       "      <td>-8.126360</td>\n",
       "      <td>4.681562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>6.307224</td>\n",
       "      <td>0.539358</td>\n",
       "      <td>0.077992</td>\n",
       "      <td>0.003392</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 4, 8, 32, 4, 4)</td>\n",
       "      <td>[0.1, 0.0, 0.0, 0.0, 0.3, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.654573</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.688800</td>\n",
       "      <td>5.616778</td>\n",
       "      <td>94</td>\n",
       "      <td>-5.493066</td>\n",
       "      <td>-16.939133</td>\n",
       "      <td>-16.907280</td>\n",
       "      <td>-6.184639</td>\n",
       "      <td>-6.661172</td>\n",
       "      <td>-10.437058</td>\n",
       "      <td>5.308939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5.765096</td>\n",
       "      <td>0.082333</td>\n",
       "      <td>0.067367</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 4, 4, 16, 16, 8)</td>\n",
       "      <td>[0.3, 0.2, 0.1, 0.3, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-9.025707</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.212495</td>\n",
       "      <td>2.713143</td>\n",
       "      <td>95</td>\n",
       "      <td>-9.846847</td>\n",
       "      <td>-8.126998</td>\n",
       "      <td>-11.089035</td>\n",
       "      <td>-16.278426</td>\n",
       "      <td>-10.789633</td>\n",
       "      <td>-11.226188</td>\n",
       "      <td>2.729178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5.990242</td>\n",
       "      <td>0.602887</td>\n",
       "      <td>0.066875</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 8, 4, 16, 4, 32)</td>\n",
       "      <td>[0.3, 0.3, 0.2, 0.3, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-9.293062</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.252686</td>\n",
       "      <td>3.258617</td>\n",
       "      <td>96</td>\n",
       "      <td>-9.939363</td>\n",
       "      <td>-8.807537</td>\n",
       "      <td>-14.355163</td>\n",
       "      <td>-7.149443</td>\n",
       "      <td>-16.180493</td>\n",
       "      <td>-11.286400</td>\n",
       "      <td>3.418864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>5.490373</td>\n",
       "      <td>0.065778</td>\n",
       "      <td>0.064155</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>(4, 4, 4, 4, 64, 64)</td>\n",
       "      <td>[0.2, 0.1, 0.2, 0.3, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-16.659912</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.724782</td>\n",
       "      <td>3.108373</td>\n",
       "      <td>97</td>\n",
       "      <td>-17.147026</td>\n",
       "      <td>-9.507247</td>\n",
       "      <td>-8.190365</td>\n",
       "      <td>-13.952026</td>\n",
       "      <td>-9.187030</td>\n",
       "      <td>-11.596739</td>\n",
       "      <td>3.409666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.117471</td>\n",
       "      <td>0.049489</td>\n",
       "      <td>0.057340</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(4, 4, 4, 16, 8, 16)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.3, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-9.094137</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.236092</td>\n",
       "      <td>2.430642</td>\n",
       "      <td>98</td>\n",
       "      <td>-9.211746</td>\n",
       "      <td>-10.256436</td>\n",
       "      <td>-13.314954</td>\n",
       "      <td>-11.208554</td>\n",
       "      <td>-16.838069</td>\n",
       "      <td>-12.165952</td>\n",
       "      <td>2.699595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>5.744885</td>\n",
       "      <td>0.223526</td>\n",
       "      <td>0.064530</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>(4, 4, 64, 8, 16, 64)</td>\n",
       "      <td>[0.2, 0.2, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-8.217524</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.360122</td>\n",
       "      <td>2.435588</td>\n",
       "      <td>99</td>\n",
       "      <td>-8.452629</td>\n",
       "      <td>-10.679656</td>\n",
       "      <td>-12.443422</td>\n",
       "      <td>-14.927909</td>\n",
       "      <td>-14.464075</td>\n",
       "      <td>-12.193538</td>\n",
       "      <td>2.407522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "90       1.381248      0.038405         0.068078        0.005896   \n",
       "47       2.399896      0.283920         0.072665        0.002338   \n",
       "6        2.196659      0.183173         0.065548        0.001259   \n",
       "44       1.448093      0.160615         0.075976        0.004562   \n",
       "8        1.277616      0.060783         0.070754        0.003336   \n",
       "63       2.156716      0.165973         0.071492        0.001492   \n",
       "83       1.747765      0.127198         0.065332        0.003197   \n",
       "10       1.272680      0.062594         0.078090        0.004779   \n",
       "50       1.446125      0.064014         0.076042        0.004591   \n",
       "88       1.121251      0.054270         0.064064        0.000459   \n",
       "97       2.074067      0.299716         0.068131        0.000985   \n",
       "91       1.484825      0.137363         0.075200        0.001080   \n",
       "61       1.545654      0.200134         0.069848        0.000691   \n",
       "15       1.407048      0.058881         0.081968        0.005464   \n",
       "2        1.290341      0.103828         0.069295        0.002260   \n",
       "40       1.530041      0.246018         0.079734        0.005308   \n",
       "22       1.502557      0.107721         0.079624        0.003246   \n",
       "54       1.379140      0.088124         0.079533        0.003351   \n",
       "86       1.781326      0.124139         0.078681        0.001120   \n",
       "14       2.078301      0.350160         0.069616        0.002228   \n",
       "56       1.577962      0.328636         0.085771        0.011449   \n",
       "39       1.647439      0.057403         0.074555        0.001031   \n",
       "43       2.262394      1.964590         0.069721        0.002387   \n",
       "46       1.819248      0.434627         0.066853        0.002264   \n",
       "75       1.340920      0.196845         0.068869        0.009164   \n",
       "41       1.262931      0.065133         0.073932        0.002217   \n",
       "37       1.517129      0.100984         0.075037        0.000743   \n",
       "36       1.310591      0.159938         0.068115        0.005472   \n",
       "42       2.399842      0.570349         0.071272        0.003106   \n",
       "69       1.752546      0.231012         0.076944        0.001937   \n",
       "29       2.769110      0.134735         0.079284        0.003135   \n",
       "32       5.372091      0.345872         0.057939        0.002211   \n",
       "93       1.520440      0.151489         0.080155        0.007016   \n",
       "35       4.577236      0.873900         0.070861        0.001916   \n",
       "96       4.654827      1.034928         0.075963        0.000651   \n",
       "55       4.841628      0.642366         0.071950        0.009444   \n",
       "28       5.870517      0.275743         0.065118        0.001494   \n",
       "51       4.626636      1.202737         0.072876        0.000783   \n",
       "72       3.711105      0.864177         0.078757        0.005837   \n",
       "52       4.205330      0.662003         0.075952        0.004268   \n",
       "20       1.870194      0.267567         0.070096        0.002482   \n",
       "31       2.615868      0.196203         0.078078        0.003747   \n",
       "7        3.737277      0.634409         0.083777        0.004083   \n",
       "34       6.096340      0.386804         0.069647        0.002039   \n",
       "57       1.274352      0.052584         0.077322        0.003825   \n",
       "1        3.349942      0.441901         0.082150        0.003621   \n",
       "98       3.276229      0.806072         0.220423        0.285118   \n",
       "81       2.875356      1.411328         0.064165        0.001089   \n",
       "19       5.669213      0.150287         0.066952        0.003482   \n",
       "66       2.952600      1.658953         0.077643        0.002434   \n",
       "76       2.814177      0.214926         0.072581        0.001316   \n",
       "59       4.946305      0.890021         0.074963        0.002848   \n",
       "74       5.057990      1.269600         0.065355        0.003223   \n",
       "65       6.307224      0.539358         0.077992        0.003392   \n",
       "99       5.765096      0.082333         0.067367        0.000889   \n",
       "95       5.990242      0.602887         0.066875        0.001736   \n",
       "89       5.490373      0.065778         0.064155        0.000709   \n",
       "5        5.117471      0.049489         0.057340        0.001454   \n",
       "58       5.744885      0.223526         0.064530        0.001768   \n",
       "\n",
       "   param_mlp__optimizer__learning_rate param_mlp__nlayers  \\\n",
       "90                               0.005                  3   \n",
       "47                               0.001                  4   \n",
       "6                                0.001                  3   \n",
       "44                               0.005                  5   \n",
       "8                                0.005                  4   \n",
       "63                               0.001                  5   \n",
       "83                               0.001                  3   \n",
       "10                               0.005                  5   \n",
       "50                               0.005                  5   \n",
       "88                               0.005                  3   \n",
       "97                               0.001                  4   \n",
       "91                               0.005                  5   \n",
       "61                               0.005                  4   \n",
       "15                               0.005                  6   \n",
       "2                                0.005                  4   \n",
       "40                               0.005                  6   \n",
       "22                               0.001                  6   \n",
       "54                               0.005                  6   \n",
       "86                               0.001                  6   \n",
       "14                               0.001                  3   \n",
       "56                               0.005                  6   \n",
       "39                               0.001                  6   \n",
       "43                               0.005                  4   \n",
       "46                               0.001                  3   \n",
       "75                               0.005                  3   \n",
       "41                               0.005                  5   \n",
       "37                               0.001                  6   \n",
       "36                               0.005                  3   \n",
       "42                               0.001                  4   \n",
       "69                               0.001                  6   \n",
       "29                              0.0001                  6   \n",
       "32                              0.0001                  1   \n",
       "93                               0.001                  5   \n",
       "35                              0.0001                  4   \n",
       "96                              0.0001                  6   \n",
       "55                              0.0001                  4   \n",
       "28                              0.0001                  2   \n",
       "51                              0.0001                  5   \n",
       "72                              0.0001                  5   \n",
       "52                              0.0001                  5   \n",
       "20                               0.001                  4   \n",
       "31                              0.0001                  6   \n",
       "7                               0.0001                  6   \n",
       "34                              0.0001                  4   \n",
       "57                               0.005                  6   \n",
       "1                               0.0001                  6   \n",
       "98                              0.0001                  6   \n",
       "81                               0.001                  3   \n",
       "19                              0.0001                  3   \n",
       "66                               0.001                  6   \n",
       "76                              0.0001                  5   \n",
       "59                              0.0001                  5   \n",
       "74                              0.0001                  3   \n",
       "65                              0.0001                  6   \n",
       "99                              0.0001                  3   \n",
       "95                              0.0001                  3   \n",
       "89                              0.0001                  3   \n",
       "5                               0.0001                  1   \n",
       "58                              0.0001                  2   \n",
       "\n",
       "          param_mlp__hiddens             param_mlp__dropouts  \\\n",
       "90      (4, 64, 8, 4, 8, 16)  [0.1, 0.0, 0.2, 0.0, 0.2, 0.0]   \n",
       "47      (4, 4, 32, 8, 8, 16)  [0.0, 0.1, 0.0, 0.1, 0.2, 0.2]   \n",
       "6      (4, 32, 16, 8, 32, 4)  [0.1, 0.2, 0.2, 0.0, 0.1, 0.2]   \n",
       "44    (4, 64, 8, 32, 16, 16)  [0.2, 0.0, 0.1, 0.0, 0.3, 0.1]   \n",
       "8     (16, 16, 32, 8, 32, 4)  [0.2, 0.1, 0.1, 0.0, 0.3, 0.3]   \n",
       "63      (4, 16, 32, 8, 4, 8)  [0.0, 0.0, 0.0, 0.3, 0.0, 0.3]   \n",
       "83      (4, 64, 32, 8, 8, 8)  [0.0, 0.0, 0.0, 0.3, 0.3, 0.0]   \n",
       "10    (64, 16, 64, 4, 32, 8)  [0.0, 0.1, 0.0, 0.0, 0.3, 0.3]   \n",
       "50     (8, 32, 16, 64, 4, 4)  [0.0, 0.2, 0.2, 0.2, 0.2, 0.0]   \n",
       "88    (64, 4, 32, 4, 32, 32)  [0.1, 0.2, 0.0, 0.1, 0.0, 0.1]   \n",
       "97     (8, 16, 4, 16, 4, 16)  [0.0, 0.1, 0.1, 0.0, 0.0, 0.3]   \n",
       "91     (8, 16, 8, 32, 16, 8)  [0.0, 0.2, 0.2, 0.3, 0.0, 0.1]   \n",
       "61     (4, 4, 8, 16, 64, 16)  [0.0, 0.3, 0.2, 0.2, 0.0, 0.1]   \n",
       "15   (16, 64, 8, 32, 64, 16)  [0.2, 0.3, 0.0, 0.2, 0.1, 0.2]   \n",
       "2   (16, 32, 64, 32, 16, 64)  [0.2, 0.2, 0.2, 0.0, 0.3, 0.0]   \n",
       "40    (16, 16, 8, 16, 64, 8)  [0.0, 0.3, 0.3, 0.2, 0.0, 0.0]   \n",
       "22  (32, 16, 64, 64, 32, 32)  [0.0, 0.1, 0.0, 0.3, 0.1, 0.0]   \n",
       "54      (8, 8, 32, 8, 8, 16)  [0.0, 0.1, 0.0, 0.2, 0.2, 0.0]   \n",
       "86   (4, 32, 16, 32, 64, 32)  [0.0, 0.2, 0.0, 0.0, 0.0, 0.3]   \n",
       "14    (16, 8, 16, 16, 16, 4)  [0.1, 0.2, 0.3, 0.0, 0.3, 0.3]   \n",
       "56   (32, 16, 32, 16, 64, 8)  [0.1, 0.3, 0.3, 0.3, 0.1, 0.0]   \n",
       "39     (8, 64, 8, 16, 16, 4)  [0.0, 0.0, 0.1, 0.0, 0.1, 0.1]   \n",
       "43     (64, 4, 64, 4, 4, 32)  [0.0, 0.1, 0.1, 0.3, 0.2, 0.1]   \n",
       "46   (8, 64, 32, 16, 32, 32)  [0.3, 0.0, 0.3, 0.3, 0.2, 0.2]   \n",
       "75       (8, 4, 8, 16, 8, 4)  [0.1, 0.3, 0.2, 0.2, 0.3, 0.3]   \n",
       "41     (8, 8, 64, 8, 32, 32)  [0.3, 0.0, 0.2, 0.3, 0.0, 0.2]   \n",
       "37      (64, 4, 8, 8, 64, 8)  [0.0, 0.2, 0.0, 0.1, 0.1, 0.2]   \n",
       "36    (8, 16, 4, 16, 16, 64)  [0.1, 0.0, 0.3, 0.0, 0.0, 0.3]   \n",
       "42       (4, 4, 32, 8, 8, 4)  [0.3, 0.1, 0.0, 0.3, 0.1, 0.2]   \n",
       "69    (8, 64, 16, 64, 4, 16)  [0.1, 0.3, 0.3, 0.1, 0.1, 0.0]   \n",
       "29    (64, 8, 64, 32, 4, 64)  [0.0, 0.1, 0.0, 0.2, 0.1, 0.3]   \n",
       "32      (32, 8, 8, 4, 16, 4)  [0.3, 0.0, 0.1, 0.1, 0.0, 0.2]   \n",
       "93    (32, 32, 8, 8, 32, 16)  [0.0, 0.0, 0.1, 0.3, 0.1, 0.0]   \n",
       "35     (64, 16, 4, 4, 4, 64)  [0.0, 0.3, 0.0, 0.1, 0.0, 0.0]   \n",
       "96     (8, 32, 4, 32, 4, 32)  [0.1, 0.1, 0.0, 0.0, 0.0, 0.1]   \n",
       "55     (4, 8, 64, 64, 16, 4)  [0.2, 0.1, 0.0, 0.0, 0.2, 0.2]   \n",
       "28     (16, 4, 16, 8, 32, 8)  [0.1, 0.0, 0.3, 0.2, 0.0, 0.0]   \n",
       "51   (4, 16, 64, 64, 64, 16)  [0.1, 0.2, 0.0, 0.3, 0.1, 0.2]   \n",
       "72     (32, 64, 32, 4, 4, 8)  [0.3, 0.3, 0.0, 0.1, 0.0, 0.1]   \n",
       "52    (32, 4, 32, 16, 8, 64)  [0.0, 0.0, 0.3, 0.0, 0.3, 0.2]   \n",
       "20     (32, 4, 4, 32, 8, 32)  [0.2, 0.3, 0.1, 0.0, 0.2, 0.3]   \n",
       "31    (64, 8, 32, 64, 8, 64)  [0.2, 0.3, 0.0, 0.3, 0.0, 0.0]   \n",
       "7     (16, 16, 4, 32, 64, 8)  [0.3, 0.3, 0.0, 0.0, 0.3, 0.0]   \n",
       "34     (8, 4, 8, 32, 16, 32)  [0.1, 0.0, 0.3, 0.3, 0.3, 0.0]   \n",
       "57     (16, 16, 8, 4, 32, 4)  [0.0, 0.3, 0.2, 0.2, 0.0, 0.3]   \n",
       "1     (16, 64, 16, 4, 8, 64)  [0.3, 0.1, 0.3, 0.0, 0.1, 0.1]   \n",
       "98     (8, 64, 8, 8, 64, 64)  [0.2, 0.2, 0.0, 0.2, 0.1, 0.0]   \n",
       "81     (8, 32, 4, 8, 16, 16)  [0.1, 0.0, 0.1, 0.0, 0.2, 0.3]   \n",
       "19      (4, 16, 32, 4, 8, 8)  [0.2, 0.3, 0.0, 0.3, 0.0, 0.1]   \n",
       "66      (8, 4, 4, 32, 64, 4)  [0.3, 0.3, 0.1, 0.2, 0.3, 0.0]   \n",
       "76    (32, 32, 64, 4, 32, 4)  [0.2, 0.2, 0.2, 0.3, 0.3, 0.3]   \n",
       "59   (16, 16, 16, 16, 4, 16)  [0.3, 0.0, 0.0, 0.3, 0.0, 0.0]   \n",
       "74      (4, 64, 4, 16, 4, 4)  [0.0, 0.0, 0.0, 0.0, 0.1, 0.0]   \n",
       "65       (8, 4, 8, 32, 4, 4)  [0.1, 0.0, 0.0, 0.0, 0.3, 0.1]   \n",
       "99      (4, 4, 4, 16, 16, 8)  [0.3, 0.2, 0.1, 0.3, 0.0, 0.1]   \n",
       "95      (4, 8, 4, 16, 4, 32)  [0.3, 0.3, 0.2, 0.3, 0.0, 0.0]   \n",
       "89      (4, 4, 4, 4, 64, 64)  [0.2, 0.1, 0.2, 0.3, 0.2, 0.0]   \n",
       "5       (4, 4, 4, 16, 8, 16)  [0.0, 0.0, 0.0, 0.3, 0.2, 0.2]   \n",
       "58     (4, 4, 64, 8, 16, 64)  [0.2, 0.2, 0.0, 0.0, 0.0, 0.0]   \n",
       "\n",
       "                                               params  split0_test_score  ...  \\\n",
       "90  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.444679  ...   \n",
       "47  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.502558  ...   \n",
       "6   {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.510855  ...   \n",
       "44  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.207207  ...   \n",
       "8   {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.679821  ...   \n",
       "63  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.738486  ...   \n",
       "83  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.356810  ...   \n",
       "10  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.984387  ...   \n",
       "50  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.772745  ...   \n",
       "88  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.801082  ...   \n",
       "97  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.748797  ...   \n",
       "91  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.194103  ...   \n",
       "61  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.032763  ...   \n",
       "15  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.286118  ...   \n",
       "2   {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.278897  ...   \n",
       "40  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.242633  ...   \n",
       "22  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.260488  ...   \n",
       "54  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.836822  ...   \n",
       "86  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.841363  ...   \n",
       "14  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.233727  ...   \n",
       "56  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.929504  ...   \n",
       "39  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.986691  ...   \n",
       "43  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.683450  ...   \n",
       "46  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.931859  ...   \n",
       "75  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.709024  ...   \n",
       "41  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -5.431517  ...   \n",
       "37  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.883612  ...   \n",
       "36  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.868113  ...   \n",
       "42  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.713081  ...   \n",
       "69  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.959479  ...   \n",
       "29  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.518302  ...   \n",
       "32  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.562274  ...   \n",
       "93  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.315631  ...   \n",
       "35  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.147595  ...   \n",
       "96  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.205419  ...   \n",
       "55  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.032173  ...   \n",
       "28  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.627315  ...   \n",
       "51  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -6.522488  ...   \n",
       "72  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.027965  ...   \n",
       "52  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.209240  ...   \n",
       "20  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.693189  ...   \n",
       "31  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.366928  ...   \n",
       "7   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.464281  ...   \n",
       "34  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.900061  ...   \n",
       "57  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -5.245757  ...   \n",
       "1   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.407163  ...   \n",
       "98  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.294130  ...   \n",
       "81  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.760444  ...   \n",
       "19  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.544407  ...   \n",
       "66  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.724928  ...   \n",
       "76  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.616088  ...   \n",
       "59  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.973960  ...   \n",
       "74  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -7.938048  ...   \n",
       "65  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.654573  ...   \n",
       "99  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -9.025707  ...   \n",
       "95  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -9.293062  ...   \n",
       "89  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...         -16.659912  ...   \n",
       "5   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -9.094137  ...   \n",
       "58  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -8.217524  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "90        -3.245788        0.184700                1           -3.123399   \n",
       "47        -3.306568        0.217319                2           -3.202178   \n",
       "6         -3.390546        0.302877                3           -3.142341   \n",
       "44        -3.451918        0.515716                5           -3.128185   \n",
       "8         -3.457536        0.185252                7           -3.400886   \n",
       "63        -3.482312        0.314462                9           -3.206298   \n",
       "83        -3.491877        0.297740               10           -3.032396   \n",
       "10        -3.511034        0.294358               12           -3.639168   \n",
       "50        -3.548365        0.294348               14           -3.605911   \n",
       "88        -3.600028        0.317610               22           -3.483200   \n",
       "97        -3.669659        0.471889               28           -3.303509   \n",
       "91        -3.683814        0.474299               30           -2.969188   \n",
       "61        -3.724189        0.198925               34           -4.076120   \n",
       "15        -3.727956        0.334774               35           -3.708094   \n",
       "2         -3.730299        0.433099               37           -3.993858   \n",
       "40        -3.836662        0.221970               45           -4.394014   \n",
       "22        -3.864190        0.312776               47           -4.088623   \n",
       "54        -3.921562        0.382886               49           -3.577503   \n",
       "86        -3.946563        0.250703               50           -3.658225   \n",
       "14        -3.979165        0.257731               52           -3.806590   \n",
       "56        -3.998526        0.731424               54           -4.026567   \n",
       "39        -4.019573        0.230108               55           -3.509549   \n",
       "43        -4.068708        0.881039               56           -3.042402   \n",
       "46        -4.073323        0.342279               57           -3.933999   \n",
       "75        -4.138777        0.724627               60           -3.604376   \n",
       "41        -4.160269        0.842450               61           -5.637015   \n",
       "37        -4.188626        0.494258               62           -3.632736   \n",
       "36        -4.378028        0.732928               66           -3.780779   \n",
       "42        -4.629266        0.699482               67           -3.792214   \n",
       "69        -4.641373        0.412405               68           -4.600227   \n",
       "29        -4.710017        0.157673               69           -4.262499   \n",
       "32        -4.735709        0.472276               70           -4.626635   \n",
       "93        -4.745196        0.339333               71           -3.899865   \n",
       "35        -4.745275        0.434045               72           -5.032434   \n",
       "96        -4.819183        1.077468               73           -4.161263   \n",
       "55        -4.875657        0.388905               74           -4.985263   \n",
       "28        -4.918170        0.366687               75           -4.383821   \n",
       "51        -4.918541        0.844340               76           -6.403205   \n",
       "72        -4.931777        0.226004               77           -5.064423   \n",
       "52        -4.936507        0.429749               78           -4.749343   \n",
       "20        -4.945885        0.592438               79           -4.537680   \n",
       "31        -5.263866        0.220901               81           -5.123247   \n",
       "7         -5.447851        0.084832               82           -5.809070   \n",
       "34        -5.517619        0.534699               83           -5.819024   \n",
       "57        -5.622228        1.165520               84           -4.948552   \n",
       "1         -5.664937        0.325811               86           -5.151940   \n",
       "98        -5.680199        0.625255               87           -5.511517   \n",
       "81        -5.733932        4.187808               88           -3.519809   \n",
       "19        -6.037519        0.883102               89           -5.679337   \n",
       "66        -6.190416        3.475616               90           -5.013691   \n",
       "76        -6.564017        0.729446               91           -5.788209   \n",
       "59        -7.500001        4.313533               92           -4.826792   \n",
       "74        -8.148529        4.380331               93           -7.774635   \n",
       "65       -10.688800        5.616778               94           -5.493066   \n",
       "99       -11.212495        2.713143               95           -9.846847   \n",
       "95       -11.252686        3.258617               96           -9.939363   \n",
       "89       -11.724782        3.108373               97          -17.147026   \n",
       "5        -12.236092        2.430642               98           -9.211746   \n",
       "58       -12.360122        2.435588               99           -8.452629   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "90           -2.881489           -3.200226           -3.204078   \n",
       "47           -3.265401           -3.032918           -3.439771   \n",
       "6            -3.310471           -3.101906           -3.179767   \n",
       "44           -3.862343           -3.072557           -3.394035   \n",
       "8            -2.870856           -3.252017           -3.121352   \n",
       "63           -3.450732           -3.505153           -3.251069   \n",
       "83           -3.292912           -3.188334           -3.490692   \n",
       "10           -3.225021           -3.512329           -3.147330   \n",
       "50           -3.195032           -3.059955           -3.527445   \n",
       "88           -3.011451           -3.564499           -3.021721   \n",
       "97           -3.469351           -3.097847           -3.089463   \n",
       "91           -2.899978           -3.489649           -4.435165   \n",
       "61           -3.204912           -3.280440           -3.612611   \n",
       "15           -3.132101           -3.468539           -3.515535   \n",
       "2            -3.169882           -2.808735           -4.060848   \n",
       "40           -3.067084           -3.593900           -3.970299   \n",
       "22           -3.439558           -3.575928           -3.251769   \n",
       "54           -3.824137           -3.555268           -3.613887   \n",
       "86           -3.306484           -3.938358           -4.235332   \n",
       "14           -3.512726           -3.832004           -3.933008   \n",
       "56           -3.470867           -3.408566           -3.492128   \n",
       "39           -3.720099           -3.881995           -3.830726   \n",
       "43           -3.743024           -3.109341           -3.579479   \n",
       "46           -4.095889           -3.454255           -3.810474   \n",
       "75           -2.993532           -5.108695           -3.939392   \n",
       "41           -4.289149           -3.363489           -4.305180   \n",
       "37           -3.861969           -3.327079           -4.332283   \n",
       "36           -2.866882           -4.889332           -4.257324   \n",
       "42           -4.001750           -3.996552           -5.039602   \n",
       "69           -3.691846           -4.298586           -4.591487   \n",
       "29           -4.325011           -4.487673           -4.733686   \n",
       "32           -4.195252           -4.067453           -4.284728   \n",
       "93           -4.994592           -4.869039           -4.365901   \n",
       "35           -4.783619           -4.198656           -3.924726   \n",
       "96           -6.743170           -4.453614           -3.723066   \n",
       "55           -5.051864           -4.647574           -4.604249   \n",
       "28           -4.264166           -5.404635           -4.585191   \n",
       "51           -3.755758           -3.805934           -4.307264   \n",
       "72           -4.292847           -4.596335           -4.934298   \n",
       "52           -4.827422           -3.885274           -4.718909   \n",
       "20           -3.665157           -4.578929           -5.228545   \n",
       "31           -4.947372           -4.975546           -5.205015   \n",
       "7            -4.933755           -5.593369           -5.546139   \n",
       "34           -4.232330           -5.493301           -5.591993   \n",
       "57           -4.412811           -5.256732           -7.782584   \n",
       "1            -5.793719           -5.084618           -5.509892   \n",
       "98           -6.286701           -5.946515           -5.070109   \n",
       "81          -13.532279           -3.066280           -3.308044   \n",
       "19           -5.053432           -7.487936           -5.580757   \n",
       "66           -4.084607           -4.033420           -4.245999   \n",
       "76           -6.216347           -6.913198           -7.088507   \n",
       "59           -5.119690           -5.563558          -16.146329   \n",
       "74           -6.167044           -4.067231           -5.443891   \n",
       "65          -16.939133          -16.907280           -6.184639   \n",
       "99           -8.126998          -11.089035          -16.278426   \n",
       "95           -8.807537          -14.355163           -7.149443   \n",
       "89           -9.507247           -8.190365          -13.952026   \n",
       "5           -10.256436          -13.314954          -11.208554   \n",
       "58          -10.679656          -12.443422          -14.927909   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "90           -2.883873         -3.058613         0.146505  \n",
       "47           -3.160560         -3.220165         0.133575  \n",
       "6            -3.135069         -3.173911         0.072626  \n",
       "44           -3.495929         -3.390610         0.284134  \n",
       "8            -3.304009         -3.189824         0.183239  \n",
       "63           -3.434089         -3.369468         0.118180  \n",
       "83           -3.274313         -3.255729         0.149235  \n",
       "10           -2.962338         -3.297237         0.246181  \n",
       "50           -3.104705         -3.298610         0.224536  \n",
       "88           -3.536013         -3.323377         0.251870  \n",
       "97           -4.086803         -3.409395         0.366908  \n",
       "91           -3.310104         -3.420817         0.551568  \n",
       "61           -3.803830         -3.595583         0.324503  \n",
       "15           -3.352134         -3.435281         0.190161  \n",
       "2            -3.607992         -3.528263         0.480196  \n",
       "40           -3.805948         -3.766249         0.437227  \n",
       "22           -3.628871         -3.596950         0.278253  \n",
       "54           -3.733543         -3.660868         0.102222  \n",
       "86           -3.827102         -3.793100         0.307510  \n",
       "14           -3.433693         -3.703604         0.194425  \n",
       "56           -5.077090         -3.895044         0.631421  \n",
       "39           -4.027582         -3.793990         0.173220  \n",
       "43           -6.030831         -3.901015         1.098037  \n",
       "46           -4.124846         -3.883893         0.243101  \n",
       "75           -3.920404         -3.913280         0.688637  \n",
       "41           -3.246187         -4.168204         0.858892  \n",
       "37           -4.422741         -3.915362         0.414744  \n",
       "36           -5.248864         -4.208636         0.840376  \n",
       "42           -5.944006         -4.554825         0.820240  \n",
       "69           -5.152407         -4.466911         0.475869  \n",
       "29           -4.507604         -4.463295         0.164384  \n",
       "32           -5.144372         -4.463688         0.387548  \n",
       "93           -4.432202         -4.512320         0.390587  \n",
       "35           -4.367122         -4.461311         0.398960  \n",
       "96           -4.294684         -4.675159         1.062175  \n",
       "55           -4.588098         -4.775410         0.200594  \n",
       "28           -4.508484         -4.629259         0.402807  \n",
       "51           -5.557701         -4.765973         1.044987  \n",
       "72           -4.610921         -4.699765         0.272820  \n",
       "52           -5.010941         -4.638378         0.390001  \n",
       "20           -5.610092         -4.724081         0.665828  \n",
       "31           -5.030421         -5.056320         0.095556  \n",
       "7            -5.232346         -5.422936         0.306251  \n",
       "34           -5.701148         -5.367559         0.577932  \n",
       "57           -4.657990         -5.411734         1.218644  \n",
       "1            -5.740507         -5.456135         0.292670  \n",
       "98           -5.157332         -5.594435         0.463906  \n",
       "81           -3.837000         -5.452682         4.047733  \n",
       "19           -5.893959         -5.939084         0.822341  \n",
       "66          -13.772973         -6.230138         3.787870  \n",
       "76           -5.635606         -6.328374         0.583794  \n",
       "59           -4.837408         -7.298756         4.431876  \n",
       "74          -17.179002         -8.126360         4.681562  \n",
       "65           -6.661172        -10.437058         5.308939  \n",
       "99          -10.789633        -11.226188         2.729178  \n",
       "95          -16.180493        -11.286400         3.418864  \n",
       "89           -9.187030        -11.596739         3.409666  \n",
       "5           -16.838069        -12.165952         2.699595  \n",
       "58          -14.464075        -12.193538         2.407522  \n",
       "\n",
       "[59 rows x 24 columns]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[no_overfit].sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_results[no_overfit]\n",
    "    .sort_values(\"mean_test_score\", ascending=False)\n",
    "    .to_csv(\"best_model_2.csv\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>1.381248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.038405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.068078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.005896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__optimizer__learning_rate</th>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__nlayers</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__hiddens</th>\n",
       "      <td>(4, 64, 8, 4, 8, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__dropouts</th>\n",
       "      <td>[0.1 0.  0.2 0.  0.2 0. ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>-3.444679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>-3.440077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>-3.266266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>-2.978596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>-3.099319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>-3.245788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.1847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>-3.123399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>-2.881489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>-3.200226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>-3.204078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>-2.883873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>-3.058613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.146505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     0\n",
       "mean_fit_time                                                                 1.381248\n",
       "std_fit_time                                                                  0.038405\n",
       "mean_score_time                                                               0.068078\n",
       "std_score_time                                                                0.005896\n",
       "param_mlp__optimizer__learning_rate                                              0.005\n",
       "param_mlp__nlayers                                                                   3\n",
       "param_mlp__hiddens                                                (4, 64, 8, 4, 8, 16)\n",
       "param_mlp__dropouts                                          [0.1 0.  0.2 0.  0.2 0. ]\n",
       "params                               {'mlp__optimizer__learning_rate': 0.005, 'mlp_...\n",
       "split0_test_score                                                            -3.444679\n",
       "split1_test_score                                                            -3.440077\n",
       "split2_test_score                                                            -3.266266\n",
       "split3_test_score                                                            -2.978596\n",
       "split4_test_score                                                            -3.099319\n",
       "mean_test_score                                                              -3.245788\n",
       "std_test_score                                                                  0.1847\n",
       "rank_test_score                                                                      1\n",
       "split0_train_score                                                           -3.123399\n",
       "split1_train_score                                                           -2.881489\n",
       "split2_train_score                                                           -3.200226\n",
       "split3_train_score                                                           -3.204078\n",
       "split4_train_score                                                           -2.883873\n",
       "mean_train_score                                                             -3.058613\n",
       "std_train_score                                                               0.146505"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = pd.read_csv(\"best_model_2.csv\")\n",
    "best_model.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = KerasRegressor(\n",
    "    twoLayerFeedForward, \n",
    "    epochs=200, \n",
    "    loss=\"mse\",\n",
    "    callbacks=[callback],\n",
    "    validation_split=0.2,\n",
    "    nlayers=3,\n",
    "    dropouts=[0.2, 0., 0.],\n",
    "    hiddens=(4, 4, 32),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipe = Pipeline(\n",
    "    [\n",
    "        ('scaler', PowerTransformer()),\n",
    "        (\"principal_components\", PCA(n_components=20, svd_solver = 'full')),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_20 (Dense)            (None, 4)                 84        \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 64)                320       \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 8)                 520       \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 933\n",
      "Trainable params: 933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp = twoLayerFeedForward(\n",
    "    hiddens=(4, 64, 8,),\n",
    "    dropouts=[0.1, 0., 0.2],\n",
    "    nlayers=3,\n",
    "    meta={\"X_shape_\":(0, 20)}\n",
    ")\n",
    "mlp.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001), loss=\"mae\")\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = preprocessing_pipe.fit_transform(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9a5b56afd0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=6,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "mlp.fit(new_df, y.values, epochs=200, validation_split=0.2, callbacks=[callback], verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9a607dfe80>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGwCAYAAABcnuQpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAACIHElEQVR4nO3dd3wUdf7H8dfsbrLpjZBGAoQQmtJR7IIdFcUuiqKevZ/eHffT89Q7PSyncif2s54FK9jOgg1BRSlSBJQkBAghIYSQCkl2d+b3x7JLQjKTspvs7uzn+XjwIPnu7O73+54N+TDzne8omqZpCCGEEEKEKEugOyCEEEII4QspZoQQQggR0qSYEUIIIURIk2JGCCGEECFNihkhhBBChDQpZoQQQggR0qSYEUIIIURIswW6Az1NVVW2b99OfHw8iqIEujtCCCGE6ARN06irqyMrKwuLxfjYi+mLme3bt5OTkxPobgghhBCiG0pKSsjOzjbcxvTFTHx8POAOIyEhwa+v7Tnq05mqMdxINsYkH32SjTHJR59kYyzU8qmtrSUnJ8f7e9xIQIuZb7/9locffpgVK1ZQVlbG/PnzmTZtWqttNmzYwKxZs1i0aBGqqnLQQQfx1ltv0b9//069h+fUUkJCgt+LGZfLhcViIT4+HqvV6tfXDnWSjTHJR59kY0zy0SfZGAvVfDozRSSgpVlDQwOjR49m7ty57T5eVFTEUUcdxbBhw/jmm29YvXo1d911F1FRUb3cUyGEEEIEq4AemZkyZQpTpkzRffzOO+/k1FNP5aGHHvK2DRo0qDe6JoQQQogQEbRzZlRV5eOPP+ZPf/oTJ598Mj///DO5ubn83//9X5tTUS01NTXR1NTk/b62thZwH15zuVyA+5CVxWJBVVVa3jRcr91isaAoSrvbZ2RkoGma97U923vG0JJeu9VqRdO0Vu2evui1d7bvXR2Tp73leLozJovFQnp6eqtsQn1M/txPAGlpad58zDAmf+0nTdNaZWOGMflzP3ny8TDDmA7sS3fHdGA2ZhhTR+1dGZOmaaSnp3dprIEeU2cFbTFTUVFBfX09DzzwAPfddx8PPvggn376KWeffTZff/01xx57bLvPmz17Nvfee2+b9qKiIuLi4gBITEwkMzOTHTt2UFNT490mNTWV1NRUSktLaWho8LZnZGSQlJTE5s2baW5u9rZnZ2eTlJTExo0bW4Wem5uLzWajoKCgVR/y8/NxOp0UFxd72ywWC0OGDKGhoYFt27Z52yMjIxk0aBA1NTWUl5d722NjY8nJyaGqqorKykpvuz/HFBcXR1FRkc9jioiIoLCw0FRj8td+qqiooKamhoqKCtOMyV/7adOmTaiq6s3GDGPqif1UXV1tujF1Zj/t3LnT+4svLi6OtLQ07+8Lj6amJpKTkykrK2Pv3r2txpqQkEBJSQkOh8Pbnp6eTmxsLMXFxa1+qWZnZ2O1WtmyZUurMQ0YMACXy9Wq74qikJubS0NDAzt27PC2R0REkJOTQ21tbav9ER0dTWZmJrt372b37t3edr0xJScn+21Me/fuDYoxpaamkpaWpvvZO7CPRhSt5SgDSFGUVhOAt2/fTr9+/Zg+fTqvv/66d7szzjiD2NhY3njjjXZfp70jM54fAs8EYH9VvgBbtmwhJyen1cxws1Xz3RmToigUFxfTv39/7zahPiZ/7ien08nmzZsZMGCAt3+hPiZ/7SeHw8GWLVu82ZhhTP7cT6qqsmXLFgYOHIjNZjPFmA7sS3vtiqJQVlZGdXV1q9fxHM3zcDqd2Gy2Nu2ebYGgam/vV3BX2rv6np582nvtQIwpOTm51dE02P8Zq66uJjk5mZqamg4v4AnaIzOpqanYbDZGjBjRqn348OEsWbJE93l2ux273d6m3Wq1tpm93bIA6U67y+WiubkZi8XS7sxwvdni7bUritKldl/73lF7V/reXrvL5cLhcLSbTaiOCfy3nxRFwel0tsknlMfkz/b2sgnlMfl7PzmdTu8vE7OMqaP2srIyampqSE9PJyYmxjv+ljRNo6mpCbvd3u7j4S6Y8tE0jT179niPwGZmZrbZRu+z156gLWYiIyM55JBD+O2331q1b9y4kQEDBgSoV0IIIXqby+WiurqatLQ0+vTpo7ud53/+UVFRAf9lHYyCLZ/o6GjAPa0kLS3Np8vFA1rM1NfXt5pTUVxczKpVq0hJSaF///788Y9/5IILLuCYY45h8uTJfPrpp3z44Yd88803geu0EEKIXuWZCxITExPgngh/8+xTh8MRusXM8uXLmTx5svf72267DYCZM2fy0ksvcdZZZ/H0008ze/Zsbr75ZoYOHcq7777LUUcdFagut2KxWMjOzu7SobBwIdkYk3z0STbGwjmfzhxNiIyM7IWehK5gy8dfR4iCZgJwT6mtrSUxMbFTE4iEEEIEn8bGRoqLi8nNzZVFU03GaN925fd3+JX2fuRyudi4cWObWf1CsumI5KNPsjEm+ejTNI3GxsZ2r5oR5s5HihkfdWVRn3Aj2RiTfPRJNsYkH31m/EV9oEmTJnHrrbd267lmzSdor2YKei4HVJdi21MB5Ae6N0IIIYJMR/NBPPNDu+q9994jIiKim70yJylmuuvr+7EueYyU/PNh9JGB7o0QQoggU1ZW5v36zTff5K9//Wur5UY8lyZ7OByOThUpKSkp/uukSchppu5K6AdAIrVheVVBRywWC7m5uZKNDslHn2RjTPJx0zSNPc3ONn9cirXddn/+6eypmoyMDO+fxMREFEXxft/Y2EhSUhJvvfUWkyZNIioqildffZVdu3Yxffp0srOziYmJYeTIkW1WvD/wNNPAgQP5xz/+wRVXXEF8fDz9+/fn2WefbbdP7S0qawZyZKa7EnMAsNSVdbBh+GpvyWyxn+SjT7IxJvnAXoeLEX/9LCDvvf5vJxMT6Z99MGvWLB555BFefPFF7HY7jY2NjB8/nlmzZpGQkMDHH3/MJZdcwqBBg5g4caLu6zzyyCP8/e9/54477uCdd97huuuu45hjjmHYsGGttguGxfJ6QniX9r5IdB+Zce3eIpPx2qGqKgUFBZKNDslHn2RjTPIxl1tvvZWzzz6b3NxcsrKy6NevH3/4wx8YM2YMgwYN4qabbuLkk0/m7bffNnydU089leuvv57Bgwcza9YsUlNT211gtrGxsYdGElhS3ndXYjYAtqZqXI69YI0LcIeEECJ8REdYWf+3k1u1uS89biIqqmfvPRQd0f2Vag80YcKEVt+7XC4eeOAB3nzzTUpLS703T46NjTV8nVGjRnm/9pzO8tz3KBxIMdNdUUloEbEojgaoLYWooYHukRBChA1FUdqc6tE0DYvqJCrSFjKnUw4sUh555BEee+wx5syZw8iRI4mNjeXWW2+lubnZ8HUOnDjsueN5uJDTTN2lKN5TTdSWBrYvQgghTGHx4sWceeaZzJgxg9GjRzNo0CAKCgoC3a2gJ8WML/adarJIMdOGxWIhPz8/7K+40CP56JNsjEk+xkL9dgeDBw9m4cKFfP/992zYsIFrrrmG8vJyv71+qOejR34afJHgLmbkyEz7nE5noLsQ1CQffZKNMclHX6ivcHvXXXcxbtw4Tj75ZCZNmkRGRgbTpk3z2+uHej56ZM6MD7SELBRAqy4hNM7O9h5VVSkuLiY/P9+n27qbleSjT7IxJvkYa2pqCsqjD5dddhmXXXaZ9/uBAwe2W1ikpKSwYMECw9c68CqlzZs3t9lm1apV7T43WPPxlRyZ8cW+00yKHJkRQgghAkaKGR9o+1YBpmZbYDsihBBChDEpZrrppe+KmfH2viMytdvBpOchfSETFI1JPvokG2OSj75QuSQ7UMyaj8yZ6SanqrGyJg6icK8101gN0cmB7lbQsFqtDBkyJNDdCFqSjz7Jxpjko09RFFPOB/EXM+cj5X03ZSZG00QkNUqCu0FONbWiaRr19fWmnTnvK8lHn2RjTPLRp2kaLpdLstFh5nykmOmmjER3dbtd6+NuqJFJwC2pqsq2bdvCagXKrpB89Ek2xiQfYx2tlBvuzJqPFDPdlLmvmNnmSnE31JQEsDdCCCFE+JJippvS4u1YFNimpbob5PJsIYQQIiCkmOkmm9VCWrydMs1zZEbmzLSkKAqRkZGmnTnvK8lHn2RjTPIxZrYrvSZNmsStt97q/X7gwIHMmTPH8DmKouguvNeVfIxeJ9iYa6/3sozEaLZ7jszInJlWLBYLgwYNMt0/LP4i+eiTbIxJPvoURcFutwdNoTd16lROOOGEdh/74YcfUBSFlStXduk1ly1bxtVXX92t/ujlc8899zBmzJg225eVlTFlypRuvVdvk58GH2QkRrWYACxHZlrSNI3q6mpTzpr3B8lHn2RjTPLRp2kaTqczaLL53e9+x1dffcWWLVvaPPbCCy8wZswYxo0b16XX7Nu3LzExMd3qT1fzycjIwG63d+u9epsUMz7ISLDvL2bqtoPqCmyHgoiqqpSXl8sVFzokH32SjTHJx5jD4Qh0F7xOP/100tLSeOmll1q179mzhzfffJNp06Yxffp0srOziYmJYeTIkbzxxhuGr3ngaaaCggKOOeYYoqKiGDFiBAsXLmzznFmzZjFkyBBiY2PJz8/nrrvu8ub00ksvce+997J69WoURUFRFG9/DzzNtHbtWo477jiio6Pp06cPV199NfX19d7HL7vsMqZNm8Y///lPMjMz6dOnDzfccEOv7BNZNM8HmYlRVJCMCwtW1Qn1OyAhK9DdEkII89M0cOxp29bcCBYX9OSppoiYTr2+zWbj0ksv5aWXXuKvf/2r9/TO22+/TXNzM1deeSVvvPEGs2bNIiEhgY8//phLLrmEQYMGMXHixA5fX1VVzj77bFJTU1m6dCm1tbWt5td4xMfH89JLL5GZmcmKFSu48cYbSUhI4E9/+hMXXHABv/zyC59++ilffPEFAImJiW1eY8+ePZxyyikcdthhLFu2jIqKCq688kpuvPHGVsXa119/TWZmJl9//TWFhYVccMEFjBkzhquuuqrD8fhCihkfZCREoWKhypJKX7XCPW9GihkhhOh5jj3wj9b/3ipAdG+89x3bITK2U5teccUVPPzww3zzzTdMnjwZcJ9iOvvss+nXrx9/+MMfvNvedNNNfPrpp7z99tudKma++OILNmzYwObNm8nOdt/4+B//+EebeS5/+ctfAPdppoyMDDZt2sRbb73Fn/70J6Kjo4mLi8Nms5GRkaH7Xq+99hp79+7llVdeITbWPfa5c+cydepUHnzwQdLT0wFITk5m7ty5WK1Whg0bxmmnncaXX34pxUwwy0xy/9iUkUJfKtxrzeQcEuBeBQdFUYiNjQ2aiXjBRvLRJ9kYk3xCy7BhwzjiiCN44YUXmDx5MkVFRSxevJjPP/8cl8vFAw88wJtvvklpaSlNTU00NTV5i4WObNiwgf79+3sLGYDDDz+8zXbvvPMOc+bMobCwkPr6epxOJwkJCV0ax4YNGxg9enSrvh155JGoqspvv/3mLWYOOuggrFard5vMzEzWrl3bpffqDilmfJC1r5jZ4kxhlAVZa6YFi8VCTk5OoLsRtCQffZKNMclnn4gY9xGSQL13F/zud7/jxhtv5IknnuDFF19kwIABHH/88Tz88MM89thjzJkzh5EjRxIbG8utt97a6VV625vIe2CRu3TpUi688ELuvfdeTj75ZBITE5k3bx6PPPJIl8agaZpuAd2yPSIios1jvTG/S4oZH/SNi0QBStU+7qnUckWTl6qqVFVVkZKSIpeQtkPy0SfZGJN89lGUNqd6PFfr2Gy2oDpydf7553PLLbfw+uuv8/LLL3PVVVehKAqLFy/mzDPPZMaMGYB73xYUFDB8+PBOve6IESPYunUr27dvJyvLfcrthx9+aLXNd999x4ABA7jzzju9+WzevLnVNpGRkbhcxhewjBgxgpdffpmGhgbv0ZnvvvsOi8USFDc+DeOfBN/ZLArJ0VZK5fLsNjRNo7KyMmgukQw2ko8+ycaY5GPM6XQGugttxMXFccEFF3DHHXewfft2LrvsMgAGDx7MwoUL+f7779mwYQPXXHMN5eXlnX7dE044gaFDh3LppZeyevVqFi9ezJ133tlqm8GDB7N161bmzZtHUVER//73v9sshDdw4ECKi4tZtWoVlZWVNDU1tXmviy++mKioKGbOnMkvv/zC119/zU033cQll1ziPcUUSFLM+Cg11kaZFDNCCCEM/O53v2P37t2ccMIJ9O/fH4C77rqLcePGcfLJJzNp0iQyMjKYNm1ap1/TYrEwf/58mpqaOPTQQ7nyyiu5//77W21z5pln8vvf/54bb7yRsWPHsnTpUu+EYI9zzjmHU045hcmTJ9O3b992Lw+PiYnhs88+o6qqikMOOYRzzz2X448/nrlz53Y9jB6gaCYv72tra0lMTKSmpqbLE5464nK5uOSZxVSXrON/9jsgti/8sdCv7xGqXC4XBQUF5Ofnt5oMJtwkH32SjbFwzKexsZHi4mJyc3OJiorS3U7TNBobG4mKigqq00zBIhjzMdq3Xfn9LUdmfKAoCv2SY/YvnNewExyNge1UkFAUhcTExKD5gQk2ko8+ycaY5GMsXAq87jJrPlLM+MBisTA4qw/VxNGk7Kso5YomwJ1NZmZmeE9QNCD56JNsjEk++uQmnMbMnI/8NPhAVVWitEZAodKy74aTUswA7mzKyspkyXUdko8+ycaY5KNP0zSam5tlcrQOM+cjxYwPNE0jVnGvByCTgFvTNI2amhpT/tD4g+SjT7IxJvkY6+gS43Bn1nykmPFRaox7qZ5iZ7K7oUaOzAghRE+QAs58/LVPpZjxUZ99xUypmuJuqCkJYG+EEMJ8PKvK7tmzp4MtRajx7NMDVw7uKlkB2AeKopCZ3pfUuG2U7pU5My0pikJqaqopJ5r5g+SjT7IxFo75WK1WkpKSqKioANxrnrQ3fs8Kt42NjWGVT2cFUz6aprFnzx4qKipISkry+SorKWZ8YLFYSE1NJTMxmrI9MmemJU82on2Sjz7Jxli45uO5o7OnoBHmkJSUZHi37s6SYsYHqqpSWlpKRoKdou0tihlNc983JIx5sunXr59cQtoOyUefZGMsXPNRFIXMzEzS0tJwOBztbqOqKjt27CA9PT2ssumsYMsnIiLCb+veSDHjA03TaGhoICMhisWeq5ma66GxBqKTAtq3QPNkIxP22if56JNsjIV7PlarVfcXoMvloqmpCbvdbtrF4Xxh5nwCX5qZQGZSFI3Yqbcmuhtk3owQQgjRa6SY8YOMBPfqv5XKvvPYMm9GCCGE6DVSzPjAYrGQkZFBZlI0AKWycJ6XJ5tgOC8bjCQffZKNMclHn2RjzMz5mG9EvUhRFJKSksjaV8wUO5LcD0gx480m0Jf/BSvJR59kY0zy0SfZGDNzPgEtZr799lumTp1KVlYWiqKwYMEC3W2vueYaFEVhzpw5vda/jqiqyqZNm0iLiwSgxLXvyIzMmfFmI/ePaZ/ko0+yMSb56JNsjJk5n4AWMw0NDYwePZq5c+cabrdgwQJ+/PFHsrKyeqlnneO5aVekzUKf2Ei5P1MLZr6hmT9IPvokG2OSjz7JxpiZ8wnopdlTpkxhypQphtuUlpZy44038tlnn3Haaaf1Us+6LiMxilJZOE8IIYTodUG9zoyqqlxyySX88Y9/5KCDDurUc5qammhqavJ+X1tbC7ivr/fcLVRRFCwWC6qqtqpQ9dotFguKorRp93ztcrnISLCzYd/CeVrtdlBdqAcUv55JVwce4rNarWia1qrd0xe99s72vatj8rQfeGdVvb7rtXvyafk6oT4mf+8nVVU7/EyG2pj8tZ9aZmOWMXWm750ZkycfVVUNxxpKYzqwL90d04HZmGFMHbV3ZUwulwtN09r82xzMY+qsoC5mHnzwQWw2GzfffHOnnzN79mzuvffeNu1FRUXExcUBkJiYSGZmJjt27KCmpsa7TWpqKqmpqZSWltLQ0OBtz8jIICkpic2bN9Pc3Oxt79evH9nZ2WzevJlorYkdJKNiwaI6cNWWU1Be16oP+fn5OJ1OiouLvW0Wi4UhQ4bQ0NDAtm37j+hERkYyaNAgampqKC8v97bHxsaSk5NDVVUVlZWV3nZ/jSk7O5u4uDiKiopafZByc3Ox2WwUFBR0akz5+fn06dOHoqIi72SzUB+TP/dTZWUlTqfTm48ZxuSv/bR58+ZW2ZhhTP7cT57765SUlJhmTP7aT55sqqur6du3rynG5M/9pGka6enpACExpi1bttBZihYkJ88URWH+/PlMmzYNgBUrVnDaaaexcuVK71yZgQMHcuutt3Lrrbfqvk57R2Y84SYkJHjfy9+V71OLivjn5wX8HHcLyc6daL/7AjVrXKu+BWvlG47/Q5ExyZhkTDImGVNwj6m6uprk5GRqamq8v7/1BO2RmcWLF1NRUUH//v29bS6Xi9tvv505c+awefPmdp9nt9ux2+1t2ttbAtuz8w7U2XaXy0VRURF5eXlkJcUAsNPSl2R2otSWYs05pN3XaW8ZaUVRutTua987atdb6rqz7S2zOfCxUB0T+G8/aZrWbj6hPCZ/tQPtZhPKY/Lnfmr5s9Wyvaf6HkqfvQOzMcOYfG1v+Z4ul4uCgoJ2/11ub3uPYBtTe4K2mLnkkks44YQTWrWdfPLJXHLJJVx++eUB6lVbnqo0I9G9CvA2NYUhIJdn0/48GrGf5KNPsjEm+eiTbIyZNZ+AFjP19fUUFhZ6vy8uLmbVqlWkpKTQv39/+vTp02r7iIgIMjIyGDp0aG93tUOZiZ6F85JBQa5oEkIIIXpJQIuZ5cuXM3nyZO/3t912GwAzZ87kpZdeClCvusdzf6atzhSIQIoZIYQQopcEtJiZNGlSlxbv0ZsnEygWi4Xc3FwsFgvRVoXkmAjKGlPcD4Z5MdMyG9GW5KNPsjEm+eiTbIyZOR/zjaiX2Wz768GMxGhKtX13zpY5M62yEW1JPvokG2OSjz7JxphZ85FixgeqqlJQUOCdUJWZGLX/lgb1O8DZZPBsczswG9Ga5KNPsjEm+eiTbIyZOR8pZvwoIzGKKuJxWvZdGl67PbAdEkIIIcKAFDN+lJUYBSjstvV1N4T5vBkhhBCiN0gx40cZ+y7P3qHsK2Zk3owQQgjR46SY8YHF4r4HkWdmeGaLhfMAqCkJVNcC7sBsRGuSjz7Jxpjko0+yMWbmfMw3ol7mdDq9X3tWAS5uTnY31IT3kZmW2Yi2JB99ko0xyUefZGPMrPlIMeMDVVUpLi5udTUTwGanp5gJ3zkzB2YjWpN89Ek2xiQffZKNMTPnI8WMH8VE2kiMjqBM23eaSa5mEkIIIXqcFDN+lpkYxXbPWjO14XtkRgghhOgtUsz46MCJVBmJUZR7jsw01kBTfQB6FRzMOMnMnyQffZKNMclHn2RjzKz5mHNUvcRqtTJkyBCsVqu3LTMxinpiaLLGuhvqygLUu8BqLxuxn+SjT7IxJvnok2yMmTkfKWZ8oGka9fX1rW6WmblvrZlqW3jfo6m9bMR+ko8+ycaY5KNPsjFm5nykmPGBqqps27at1cxwz+XZFcq+YiZML89uLxuxn+SjT7IxJvnok2yMmTkfKWb8zHN5dqlr3+XZckWTEEII0aOkmPEzTzFT7EhyN4TpaSYhhBCit0gx4wNFUYiMjERRFG+b5/5MW7zFTHgemWkvG7Gf5KNPsjEm+eiTbIyZOR9boDsQyiwWC4MGDWrVFme3ER9lo6zZs9ZMeBYz7WUj9pN89Ek2xiQffZKNMTPnI0dmfKBpGtXV1W1mhmcmRrVYBTg8F87Ty0a4ST76JBtjko8+ycaYmfORYsYHqqpSXl7eZmZ4RmL0/oXz9u6G5j0B6F1g6WUj3CQffZKNMclHn2RjzMz5SDHTAzIToqglhmaLe/5MuC6cJ4QQQvQGKWZ6QGZSFKBQHZHmbpArmoQQQogeI8WMDxRFITY2ts3M8Ezvwnn7JgGH4cJ5etkIN8lHn2RjTPLRJ9kYM3M+Usz4wGKxkJOT0+bGXZ5bGmz3LpwXfsWMXjbCTfLRJ9kYk3z0STbGzJyP+UbUi1RVpbKyss1kqqykfQvnNSe6G8Lw8my9bISb5KNPsjEm+eiTbIyZOR8pZnygaRqVlZXtXJq9b+E8Z/je0kAvG+Em+eiTbIxJPvokG2NmzkeKmR4Qa7eRFBPB9jBfa0YIIYToDVLM9JCsxGjKtfBeBVgIIYToDVLM+EBRFBITE9udGZ6VFL1/FeA9u8DR2Mu9CyyjbITkY0SyMSb56JNsjJk5HylmfGCxWMjMzGx3Zni/pChqiMVhsbsb6sLr6IxRNkLyMSLZGJN89Ek2xsycj/lG1ItUVaWsrKzdmeFZSdGAwm7bvoXzwmytGaNshORjRLIxJvnok2yMmTkfKWZ8oGkaNTU17c4MdxczsIPwnDdjlI2QfIxINsYkH32SjTEz5yPFTA/xFDPbnEnuhjBcOE8IIYToDVLM9BDvwnmOJHdDmB2ZEUIIIXqLFDM+UBSF1NTUdmeGp8VHYbUolKqetWbCq5gxykZIPkYkG2OSjz7JxpiZ87EFugOhzGKxkJqa2u5jVotCRkIUZbXhuXCeUTZC8jEi2RiTfPRJNsbMnI8cmfGBqqqUlJTozgzvlxRNuRaeR2Y6yibcST76JBtjko8+ycaYmfORYsYHmqbR0NCgOzM8Kylq/8J5DTvB2dSLvQusjrIJd5KPPsnGmOSjT7IxZuZ8pJjpQVlJ0ewmHocS6W6oKwtsh4QQQggTkmKmB3kWzquy9nU3hNnCeUIIIURvkGLGBxaLhYyMDN2lofvtW2umnPCbN9NRNuFO8tEn2RiTfPRJNsbMnI/5RtSLFEUhKSlJ9zI3z8J5JWG4cF5H2YQ7yUefZGNM8tEn2Rgzcz5SzPhAVVU2bdqkOzM8c9/CeVudye6GMDoy01E24U7y0SfZGJN89Ek2xsycjxQzPtA0jebmZt2Z4QlREcTbbWzXPPdnCp8jMx1lE+4kH32SjTHJR59kY8zM+Ugx08OyWq01Ez7FjBBCCNFbpJjpYa3Wmgmj00xCCCFEb5FixgcWi4Xs7GzDmeGtjszUV4CzuZd6F1idySacST76JBtjko8+ycaYmfMJ6Ii+/fZbpk6dSlZWFoqisGDBAu9jDoeDWbNmMXLkSGJjY8nKyuLSSy9l+/bgObqhKApxcXGGM8OzkqLZRQJOJQLQwmbhvM5kE84kH32SjTHJR59kY8zM+QS0mGloaGD06NHMnTu3zWN79uxh5cqV3HXXXaxcuZL33nuPjRs3csYZZwSgp+1zuVxs3LgRl8ulu02/fQvn7bLsu7lXmJxq6kw24Uzy0SfZGJN89Ek2xsycT0Dvmj1lyhSmTJnS7mOJiYksXLiwVdvjjz/OoYceytatW+nfv39vdLFDHV3i5llrpkxLIZ2ysJoEbMbL//xJ8tEn2RiTfPRJNsbMmk9Ai5muqqmp8S76o6epqYmmpv03dKytrQXcFamnGlUUBYvFgqqqrS5R02u3WCwoitKm3fP1gVWu53ykqqpkxLvvy7TVmcQYC2i121EP2N5qtaJpWqsPmacveu2d7XtXx+RpNxpTZ9o9+bR8nVAfk7/3k6qqHX4mQ21M/tpPLbMxy5g60/fOjMmTj6qqhmMNpTEd2JfujunAbMwwpo7auzIml8uFpmlt/m0O5jF1VsgUM42Njfz5z3/moosuIiEhQXe72bNnc++997ZpLyoqIi4uDnAf9cnMzGTHjh3U1NR4t0lNTSU1NZXS0lIaGhq87RkZGSQlJbF582aam/dP4M3KygKguLi41Y7Izc3FZrNRUFCAU9VQgO1qCljAtbuEwoIC77YWi4UhQ4bQ0NDAtm3bvO2RkZEMGjSImpoaysvLve2xsbHk5ORQVVVFZWWlt91fY8rOziYuLo6ioqJWH6SWY2opPz8fp9NJcXFxqzHl5eXhcDgoLCz0/pCE+pj8uZ8qKiqoqqry5mOGMflrPxUXF7fKxgxj8ud+UlWVqqoqtm7dSl5eninG5K/95MmmqqqK9PR0U4zJn/vJU+h5Fs8L9jFt2bKFzlK0IFk9R1EU5s+fz7Rp09o85nA4OO+889i6dSvffPONYTHT3pEZT7ie5/mr8lUUBYfDgdVqbTWh6sAK98gHv+akhg/4W8TLaMOnop77cqs+B7ry7Yn/oVgsFpqamrDZbN5sQn1M/txPLpeL5uZmIiIiUBTFFGPy135yOp04HA5vNmYYkz/3k6ZpOBwOIiMj5cjMAe0HZmOGMXXU3pUxaZqG0+kkMjKyzcJ5wTim6upqkpOTqampMfy9DyFwZMbhcHD++edTXFzMV1991eGA7HY7dru9TbvVasVqtbZq8+y8A3W2XdM0bDabN/j23hP2XZ5d7748W6nd3qYf4N6pXWn3te8dtbf3nl1p1zSNiIiIdrMJ1TGBf/dTZGRkm3xCfUz+aPf85+DAbEJ9TL62e/quaVqrIs8MY+pse0d9PDAbM4zJ1/aW7+nJp2VGRtt7BNuY2t2201sGgKeQKSgo4IsvvqBPnz6B7lIrqqpSUFDQ4Xm9rKRoyry3NAiPq5k6m024knz0STbGJB99ko0xM+cT0CMz9fX1FBYWer8vLi5m1apVpKSkkJWVxbnnnsvKlSv56KOPcLlc3nNzKSkpREZGBqrbXdYvKZofPQvn1ZWDywHWiMB2SgghhDCJgBYzy5cvZ/Lkyd7vb7vtNgBmzpzJPffcwwcffADAmDFjWj3v66+/ZtKkSb3VTZ9lJUVTSQJObNhwuguapJxAd0sIIYQwhYAWM5MmTTK8e2eQzE32WVZSNBoWdln6kK7ucJ9qkmJGCCGE8IugnjMT7CwWC/n5+R1OUspKigJgu5rsbgiDhfM6m024knz0STbGJB99ko0xM+djvhH1MqfT2eE2/fatAlzi8hQz4TEJuDPZhDPJR59kY0zy0SfZGDNrPlLM+EBVVYqLizucGZ4YHUF0hJXt3iuazH9kprPZhCvJR59kY0zy0SfZGDNzPlLM9AJFUchKiqLcc0VTGBQzQgghRG+RYqaXuNea8RQz4XGaSQghhOgNUsz4qLMTqfolRbc4MhMexYwZJ5n5k+SjT7IxJvnok2yMmTWfoL+dQTCzWq0MGTKkU9tmJUXzlWfOTF05uJxgNW/8XckmHEk++iQbY5KPPsnGmJnzMWeJ1ks0TaO+vr5T6+G4F85LxIkVNBfU7+iFHgZOV7IJR5KPPsnGmOSjT7IxZuZ8pJjxgaqqbNu2rVMzw7OSolCxsEsJj1NNXckmHEk++iQbY5KPPsnGmJnzkWKml3jWmikNo4XzhBBCiN4gxUwvyUj0rAIcHkdmhBBCiN4ixYwPFEUhMjISRVE63NZus5IaZw+bhfO6kk04knz0STbGJB99ko0xM+dj3stpeoHFYmHQoEGd3r5fUhTlZeGxcF5Xswk3ko8+ycaY5KNPsjFm5nzkyIwPNE2jurq60zPDw2nhvK5mE24kH32SjTHJR59kY8zM+Ugx4wNVVSkvL+/0zPCslgvn1Zj7yExXswk3ko8+ycaY5KNPsjFm5nykmOlFWUnRlGhp7m9qS8GxN7AdEkIIIUxAiple1C8pikoSqFPiAA12FQa6S0IIIUTIk2LGB4qiEBsb2+mZ4VlJ0YDCJvq5G3b+1nOdC7CuZhNuJB99ko0xyUefZGPMzPlIMeMDi8VCTk5Op2/clbVv4bwNzix3g4mLma5mE24kH32SjTHJR59kY8zM+ZhvRL1IVVUqKys7PZmqT2wkkTYLBeq+YqbSvMVMV7MJN5KPPsnGmOSjT7IxZuZ8pJjxgaZpVFZWdvoyN0VRyEqMokjznGba2IO9C6yuZhNuJB99ko0xyUefZGPMzPlIMdPLspKiKfQUM7sKweUMbIeEEEKIECfFTC/LSoqmVOuDwxIFqgN2Fwe6S0IIIURIk2LGB4qikJiY2KWZ4VlJ0WhYqLD3dzeYdBJwd7IJJ5KPPsnGmOSjT7IxZuZ8pJjxgcViITMzs0szw/slue+evcWS427Y+WtPdC3gupNNOJF89Ek2xiQffZKNMTPnY74R9SJVVSkrK+vSzHDP5dm/OTPdDZXmnATcnWzCieSjT7IxJvnok2yMmTkfKWZ8oGkaNTU1XZoZ7ilmVu5NdzeY9DRTd7IJJ5KPPsnGmOSjT7IxZuZ8pJjpZdnJ0SgKrHdkuBsqC8CEVbIQQgjRW6SY6WV2m5WsxGi2aOloig0cDVC7LdDdEkIIIUKWFDM+UBSF1NTULs8MH9AnBic26mI9VzSZb95Md7MJF5KPPsnGmOSjT7IxZuZ8pJjxgcViITU1tcszwwf0iQWg3D7Q3WDC2xp0N5twIfnok2yMST76JBtjZs7HfCPqRaqqUlJS0uWZ4QP6xABQbOK7Z3c3m3Ah+eiTbIxJPvokG2NmzkeKGR9omkZDQ0OXZ4YP3FfM/NK8bxKwCYuZ7mYTLiQffZKNMclHn2RjzMz5SDETAP1T3KeZltX3dTdU/gYm/HAJIYQQvUGKmQDwnGZatbcvGgrs3Q0NlQHulRBCCBGapJjxgcViISMjo8uTqWLtNvrG22nEjiM+291osknA3c0mXEg++iQbY5KPPsnGmJnzMd+IepGiKCQlJXXrMrcBKe6jM7tjct0NJrtHky/ZhAPJR59kY0zy0SfZGDNzPlLM+EBVVTZt2tStmeGey7O3R5hzrRlfsgkHko8+ycaY5KNPsjFm5nykmPGBpmk0Nzd3a2a4Z95Mgbrv8myTnWbyJZtwIPnok2yMST76JBtjZs5HipkA8RQzqxs9N5w015EZIYQQordIMRMgntNMS+tS3Q1126GxJoA9EkIIIUKTFDM+sFgsZGdnd2tmuGfhvKI6G1rsvqMzlQX+7F5A+ZJNOJB89Ek2xiQffZKNMTPnY74R9SJFUYiLi+vWzPCkmEgSoyMA2JM02N1oopWAfckmHEg++iQbY5KPPsnGmJnzkWLGBy6Xi40bN+Jyubr1fM+8mV1RA90NJro829dszE7y0SfZGJN89Ek2xsycjxQzPvLlEjfPvJmtVs/CeeaaBGzGy//8SfLRJ9kYk3z0STbGzJpPl4qZn376qVVFd+DlXU1NTbz11lv+6VkY8Cyc95szy91gotNMQgghRG/pUjFz+OGHs2vXLu/3iYmJbNq0yft9dXU106dP91/vTM5zmmnl3n0TgKu3gGNvAHskhBBChJ4uFTMHHolpb+EdMy7Go8disZCbm9vtmeGe00xrqiMhKhE0FXYV+rOLAeNrNmYn+eiTbIxJPvokG2NmzsfvI+rKLOlvv/2WqVOnkpWVhaIoLFiwoNXjmqZxzz33kJWVRXR0NJMmTWLdunV+7rFvbDZbt5/ruTy7tLoRNXWIu9FEp5p8ySYcSD76JBtjko8+ycaYWfMJaHnW0NDA6NGjmTt3bruPP/TQQzz66KPMnTuXZcuWkZGRwYknnkhdXV0v97R9qqpSUFDQ7QlVfePtREdYUTVoiM9zN5pkErCv2Zid5KNPsjEm+eiTbIyZOZ8ul2jr16+nvLwccB85+fXXX6mvrwegsrKyS681ZcoUpkyZ0u5jmqYxZ84c7rzzTs4++2wAXn75ZdLT03n99de55ppr2n1eU1MTTU1N3u9ra2sB9yVpnsnLiqJgsVhQVbXVaTG9dovFgqIobdo9Xx94mZvnEN6BH5j22vunRPPbjnp22AcQD6gVv6K5XN6+aJrWavuu9r2rY/K0+zKmlvm0fJ1QH5PVatXte3fGpKpqh5/JUBuTv/ZTy2zMMqbO9L0zY/Lko6qq4VhDaUwH9qW7YzowGzOMqaP2rozJ5XKhaVqbf5uDeUyd1eVi5vjjj2/1pqeffrq3U5qm+W0xnuLiYsrLyznppJO8bXa7nWOPPZbvv/9et5iZPXs29957b5v2oqIi4uLiAPfE5czMTHbs2EFNzf5bCKSmppKamkppaSkNDQ3e9oyMDJKSkti8eTPNzc3e9qysLG9fW2aSm5uLzWajoKD1ir75+fk4nU6Ki4v3v2eUxm/ARmcGgwHH9rUUFxQQGRnJoEGDqKmp8RaPALGxseTk5FBVVdWqePTXmLKzs4mLi6OoqKjVB6krY7JYLOTl5eFwOCgsLPT+kIT6mIYMGUJDQwPbtm3ztnd3TBUVFVRVVXnzMcOY/LWfiouLW2VjhjH5cz+pqkpVVRVbt24lLy/PFGPy137yZFNVVUV6eropxuTP/eQp9Dx3zw72MW3ZsoXOUrQuzNjt7AsPGDCg0x3wdkRRmD9/PtOmTQPg+++/58gjj6S0tNRbNABcffXVbNmyhc8++6zd12nvyIwn3ISEBO97+evITGFhIYMGDcJqtbbaHjpXzc/+5Ff+s2Qzvx8fyS3rzkWzRKD+XymKNSJoqvmujslj48aN5OXlebMJpv+hdGdM/vwfisPhoKCggMGDB2O1Wk0xJn/tp+bmZgoLC73ZmGFM/j4yU1hYSH5+PhEREaYY04F98eXITMtszDCmjtq7emSmqKiI/Pz8NgcegnFM1dXVJCcnU1NT4/39radLR2Y6U6SsWrWqW8WMngMD7+joj91ux263t2m3Wq2tCg7Yv/MO1Nl2TdPIz8/3Bt/ee7anZXtuX/fRojX1CWCLRnHuxVpTAqnuWxwoitLu6/ja947aO9N3o3ZN0xgyZEi72YTqmEC/710dk81mY+jQoW3yCeUx+as9IiKi3WxCeUz+3E8Wi8WbT8v2nup7KH32DszGDGPytf3A/2jr/bvc3vYewTamdrft9JYGampqePLJJxk3bhzjx4/3x0uSkZEB0OoQFkBFRQXp6el+eQ9/cDqdPj1/4L7LszdX7YXUfHdjpTmuaPI1G7OTfPRJNsYkH32SjTGz5uNTMfPVV18xY8YMMjMzefzxxzn11FNZvny5XzqWm5tLRkYGCxcu9LY1NzezaNEijjjiCL+8h69UVaW4uLhLk5QO1H/fKsAlVXtRU4e6G01wjyZ/ZGNmko8+ycaY5KNPsjFm5ny6PAF427ZtvPTSS7zwwgs0NDRw/vnn43A4ePfddxkxYkSXXqu+vp7Cwv2LxBUXF7Nq1SpSUlLo378/t956K//4xz/Iz88nPz+ff/zjH8TExHDRRRd1tdtBKyspmgirQrNLpS4+l0SAnea4PFsIIYToDV0qZk499VSWLFnC6aefzuOPP84pp5yC1Wrl6aef7tabL1++nMmTJ3u/v+222wCYOXMmL730En/605/Yu3cv119/Pbt372bixIl8/vnnxMfHd+v9gpHVopCTHMOmygbKIga4ixmTnGYSQgghekOXipnPP/+cm2++meuuu478/Hyf33zSpEmGtz9QFIV77rmHe+65x+f36ildmaCkZ0AfdzFTqGUzDNxHZjQN/HSZe6D4Ixszk3z0STbGJB99ko0xs+bTpVEtXryYuro6JkyYwMSJE5k7dy47d+7sqb4FPavVypAhQ3RnuneW5x5N6xr7gNUOjgaoWO+PLgaMv7IxK8lHn2RjTPLRJ9kYM3M+Xb5r9nPPPUdZWRnXXHMN8+bNo1+/fqiqysKFC4PmNgO9RdM06uvrfb65pufu2cVVzZC377Tbhg997V5A+Ssbs5J89Ek2xiQffZKNMTPn063jTTExMVxxxRUsWbKEtWvXcvvtt/PAAw+QlpbGGWec4e8+Bi1VVb0rcvrCc3n2lqo9MHxffus/8LV7AeWvbMxK8tEn2RiTfPRJNsbMnI/PJ8+GDh3KQw89xLZt25g3b57fbmcQTvrvOzKzZVcD2tApYLFBxTqoLOzgmUIIIYTo0gTgK664osNt+vTp0+3OhKvs5GgsCuxpdrHTFUNa7jFQ9BVseB+Ovj3Q3RNCCCGCWpeOzLz00kt8/fXXVFdXs3v37nb/VFdX91BXg4+iKERGRvp8NMpus5KZGA3A1l3mONXkr2zMSvLRJ9kYk3z0STbGzJxPl240ef311zNv3jz69+/PFVdcwYwZM0hJSenJ/vmstraWxMTETt2oKpAu/s9SvivcxT/PG825Q+3wyBDQVLhlDST7715XQgghRCjoyu/vLh2ZefLJJykrK2PWrFl8+OGH5OTkcP755/PZZ5+ZcnZ0RzRNo7q62i9j75/ingS8dVcDxPWF/vtu2RCiVzX5Mxszknz0STbGJB99ko0xM+fT5QnAdrud6dOns3DhQtavX89BBx3E9ddfz4ABA6ivr++JPgYtVVUpLy/3y8zwgfsmAW/etcfdMOJM99/r3/f5tQPBn9mYkeSjT7IxJvnok2yMmTkfn65mUhQFRVHQNM2U4fSmAS0vzwYYfrr7720/Qe32APVKCCGECH5dLmaampp44403OPHEExk6dChr165l7ty5bN26lbi4uJ7oY1gY0OLybAASsiD7UPfXGz4KUK+EEEKI4NelYub6668nMzOTBx98kNNPP51t27bx9ttvc+qpp5r2fg9GFEUhNjbWLzPDPcVM9R4HNXsc7sYR+65q2hB6VzX5Mxszknz0STbGJB99ko0xM+fTpauZLBYL/fv3Z+zYsYZhvPfee37pnD+EytVMAIfc/wU765r44MYjGZWdBLu3wL9GgWKB2ze6JwYLIYQQYaDHrma69NJLmTx5MklJSSQmJur+CReqqlJZWem3+UJtJgEnD4DMMe5LtH8NrVNN/s7GbCQffZKNMclHn2RjzMz5dGkF4JdeeqmHuhGaNE2jsrKS5ORkv7zegD6xLNu82315tseIM6BslftU04TL/fI+vcHf2ZiN5KNPsjEm+eiTbIyZOZ/wm+gSxAakHHBkBmD4vku0i7+FPVUB6JUQQggR3KSYCSIDUvddnt3yyEzqYEg7CFQn/PZJgHomhBBCBC8pZnygKAqJiYl+mxnuOTKzpeWRGQjJq5r8nY3ZSD76JBtjko8+ycaYmfORYsYHFouFzMxMv12WPnDfwnkVdU3UNjr2P+C58WTRV9BY65f36mn+zsZsJB99ko0xyUefZGPMzPmYb0S9SFVVysrK/DYzPDEmgsFp7oUH/7embP8DacOhz2BwNUPB5355r57m72zMRvLRJ9kYk3z0STbGzJyPFDM+0DSNmpoav9606/wJ2QDMW1ayv1FR9h+dCZF7NfVENmYi+eiTbIxJPvokG2NmzkeKmSBz9rhsbBaFVSXVbNxRt/8Bz40nCxZCc0P7TxZCCCHCkBQzQSY1zs7xw9MAeLPl0ZnM0ZA8EJx7YfUbgemcEEIIEYSkmPGBoiikpqb6fWb4BYfkAPDeym00OV2eN4PDbnB/vfhRcDb59T39raeyMQvJR59kY0zy0SfZGDNzPlLM+MBisZCamur3meHH5PclIyGK3XscfLG+Yv8D4y6F+CyoLYWVr/j1Pf2tp7IxC8lHn2RjTPLRJ9kYM3M+5htRL1JVlZKSEr/PDLdZLZw73j0R+M3lLU41RUTB0be5v178KDga/fq+/tRT2ZiF5KNPsjEm+eiTbIyZOR8pZnygaRoNDQ09MjP8/AnuU02LC3aybXeLRfTGXQoJ/aBue1AfnenJbMxA8tEn2RiTfPRJNsbMnI8UM0Gqf58Yjsjrg6bBOyu27X/AZt9/dGZJcB+dEUIIIXqDFDNBzDMR+O3l23CpLSrpsZdAQjbUlcGKlwLTOSGEECJISDHjA4vFQkZGRo9Npjr5oAwSomyUVu/lu8LK/Q/Y7HDM7e6vlzwGjr098v6+6OlsQp3ko0+yMSb56JNsjJk5H/ONqBcpikJSUlKPXeYWFWHlrLH9gAMmAgOMmQGJOVBfHpRHZ3o6m1An+eiTbIxJPvokG2NmzkeKGR+oqsqmTZt6dGb4+ftONS1ct4Oqhub9D9gi4ejgPTrTG9mEMslHn2RjTPLRJ9kYM3M+Usz4QNM0mpube3Rm+EFZiRzcL4Fml8r8n0tbPzjmYkjsD/U7YPkLPdaH7uiNbEKZ5KNPsjEm+eiTbIyZOR8pZkLABfsu035rWUnrD6EtEo75g/vrJXOgeU/bJwshhBAmJ8VMCDhjTD/sNgu/7ahj9baa1g+OuQiS+kNDRdAdnRFCCCF6gxQzPrBYLGRnZ/f4zPDE6AhOHZkJwJvLtrZ+0BoBx/zR/fV3c4Lmjtq9lU2oknz0STbGJB99ko0xM+djvhH1IkVRiIuL65WZ4Z4VgT9YtZ2GJmfrB0dPd99Ru2En/PcsqN3e4/3pSG9mE4okH32SjTHJR59kY8zM+Ugx4wOXy8XGjRtxuVw9/l6HDUphYJ8YGppdzFt2wGXa1gg4Yy7YE6HkR3j6aNj0TY/3yUhvZhOKJB99ko0xyUefZGPMzPlIMeOj3rrETVEUrjk2D4CnFxXR6Djgw5h7NFzzDaSPhD2V7iM0ix+BAF6CZ8bL//xJ8tEn2RiTfPRJNsbMmo8UMyHknHHZ9EuKZmddE/N+2tp2g5RBcOVC94J6mgpf/g3mXQR7d/d+Z4UQQoheIsVMCIm0WbhukvvozFPtHZ0BiIiGaU/AGY+D1Q4bP4FnJ0HZmt7trBBCCNFLpJjxgcViITc3t1dnhp83IZvMxCh21Dbx9oG3OGhp3KXwu8/dl23v3gzPnwjr3++1fgYim1Ai+eiTbIxJPvokG2Nmzsd8I+plNputV9/PbrN6j848+U0RTU6DiVxZY+DqRZB/Mjgb4e3LYcOHvdNRej+bUCP56JNsjEk++iQbY2bNR4oZH6iqSkFBQa9PqDp/Qg7pCXbKahp5Z8U2441jUmD6GzDqAtBc8PZl8Ov/eryPgcomVEg++iQbY5KPPsnGmJnzkWImBEVFWLl235VNT35dRLOzgw+mxQrTnoKDzwHVCW9dChs/74WeCiGEED1PipkQNf3Q/vSNt1NavZf3VnZwdAbcBc1Zz8KIM0F1wJszoPDLnu+oEEII0cOkmAlRURFWrjlmEABPfFOIw9WJw4ZWG5zzPAw7HVxN7su2A7y4nhBCCOGroC5mnE4nf/nLX8jNzSU6OppBgwbxt7/9LWjO91ksFvLz8wM2M/ziiQNIjYukpGovC34u7dyTrBFw7oswZIp7UvDrF0LxYr/3LdDZBDvJR59kY0zy0SfZGDNzPkE9ogcffJCnn36auXPnsmHDBh566CEefvhhHn/88UB3zcvpdHa8UQ+JjrRy9b6jM3O/LsTZmaMzALZIOP9lGHwiOPfC6xfA5u/83r9AZhMKJB99ko0xyUefZGPMrPkEdTHzww8/cOaZZ3LaaacxcOBAzj33XE466SSWL18e6K4B7pnhxcXFAT1SNOOwAaTERrJl1x4+WN2FG0za7HDBqzBoMjga4NWzYcNHfutXMGQTzCQffZKNMclHn2RjzMz5BPUF50cddRRPP/00GzduZMiQIaxevZolS5YwZ84c3ec0NTXR1NTk/b62thZw32DLc3MtRVGwWCyoqoqmad5t9dotFguKorRp93x94E27PIfwDvzA6LVbrVY0TWvV7umLXrunL3arwpVHDeShzzYy96tCTj04gwirort9qzFFRKFe8BrKO1egFHyK9tYlMOVhlEOv9HlMnnxavk5nx9Tddr395GkP5H5qr11V1Q4/k6E2Jn/tp5bZmGVMnel7Z8bkyUdVVcOxhtKYDuxLd8d0YDZmGFNH7V0Zk8vlQtO0Nv82B/OYOiuoi5lZs2ZRU1PDsGHDsFqtuFwu7r//fqZPn677nNmzZ3Pvvfe2aS8qKiIuLg6AxMREMjMz2bFjBzU1Nd5tUlNTSU1NpbS0lIaGBm97RkYGSUlJbN68mebmZm97VlYWAMXFxa12RG5uLjabjYKCglZ9yM/Px+l0Ulxc7G2zWCwMGTKEhoYGtm3bf1VSZGQkgwYNoqamhvLycm97bGwsOTk5VFVVUVlZCcBhqSoJUVY2VTYw6t7Pye9jZ3haFMP62jlqaBbDBmbpj6l0B81j7yLDZSdp0/vwv9uhbjtF2Reg+jCmvLw8HA4HhYWF3h+SrozJn/spOzubuLg4ioqKWv1w9PZ+ajmmiooKqqqqvPmYYUz+2k/FxcWtsjHDmPy5n1RVpaqqiq1bt5KXl2eKMflrP3myqaqqIj093RRj8ud+8hR6qqqyadOmoB/Tli1b6CxFa/lbOMjMmzePP/7xjzz88MMcdNBBrFq1iltvvZVHH32UmTNntvuc9o7MeMJNSEgA/HtkZtOmTQwcOBCr1dpqe+jdav7TdTu4c8EvVO9xtMmkX1I0Y/snMfPw/ozrn9z+mDQNZfHDWBY94O7jqOlop89xTxjuxpgACgsLyc3N9WYTTP9D6c6Y/Pk/FIfDQVFREYMGDcJqtZpiTP7aT83NzWzatMmbjRnG5O8jM5s2bSIvL4+IiAhTjOnAvvhyZKZlNmYYU0ftXT0yU1xcTF5eHoqy/wh+sI6purqa5ORkampqvL+/9QR1MZOTk8Of//xnbrjhBm/bfffdx6uvvsqvv/7aqdeora0lMTGxU2GEOlXVKN7VwMotu/m5pJqVW3azcUcdaos9fNbYfvx5yjDSE6Laf5EVL8NHv3evFjz4BDjvZbDH9c4AhBBCiH268vs7qE8z7dmzx1stenjOgwYDTdNoaGggNja2TZUbCBaLQl7fOPL6xnHehBwA6pucrCmpZv7Ppbyzchvzfy7ls3Xl3HjcYH53VC52m7X1i4yfCXHp7tseFH4BL58OF70NcX271JdgyybYSD76JBtjko8+ycaYmfMJ6quZpk6dyv3338/HH3/M5s2bmT9/Po8++ihnnXVWoLsGuA/Hec5hB6s4u40jBqfy8Hmjef+GIxnXP4k9zS4e+vQ3TnrsW75Yv4M2B+eGngKXfQQxfWD7z/DyVGhuaP8NdIRCNoEk+eiTbIxJPvokG2Nmzieoi5nHH3+cc889l+uvv57hw4fzhz/8gWuuuYa///3vge5aSBqVncQ71x7Bo+ePJi3ezpZde7jyleXMfHEZ23bvab1x9gT43UL3UZqdG+Dj2yF4z0gKIYQIY0FdzMTHxzNnzhy2bNnC3r17KSoq4r777iMyMjLQXQtZFovC2eOy+eoPk7huUh6RVgvfbtzJNf9dgaoeUKz0yYNzXwDFAqvfgJ9fDUynhRBCCANBXcwEO0VRiIyMDMlzj3F2G7NOGcantx5NvN3Guu21fLimnUX3Bh4Fk+90f/2/P0D5L516/VDOpjdIPvokG2OSjz7JxpiZ8wnqq5n8IZyuZuquJ74u5OHPfiM7OZovbz+27aRgVYXXz3NPCO4zGK7+BuzxAemrEEKI8NCV399yZMYHmqZRXV3ddgJtiLniyFzSE+xs272X//7QziJFFguc9SzEZ8GuQvjw1g7nz5glm54i+eiTbIxJPvokG2NmzkeKGR+oqkp5eXnIzwyPjrTy+xOGAO4bVtbsbbvwHrF94LwXQbHCL+/AihcNX9Ms2fQUyUefZGNM8tEn2Rgzcz5SzAgAzh2fzeC0OKr3OHh6UVH7G/U/DE642/31J3+GstW910EhhBBChxQzAgCb1cKsU4YB8MKSYspq9ra/4eE3wZAp4GqCt2ZCY0372wkhhBC9JKhXAA52iqKYaiXFE4anccjAZJZt3s1jCzfy0Lmj225kscC0J+GZY2F3MTx3PEQng2MvOPaAsxEce7A49pIXm4ES9RQMPKL3BxPkzPbZ8SfJxpjko0+yMWbmfORqJtHKii27Oeep77Eo8OmtxzAkXeeqpW3L4YVTQG1nfk1LFhucdB9MvBZM+AMkhBCiZ3Tl97cUMz7w3G4+JSWlzT2kQtm1/13Bp+vKOX5YGs9fdoj+hmVrYOdvEBG9708MRERBRAyqYsXx2V+xF3zs3vbgc2Dqv+WmlfuY9bPjD5KNMclHn2RjLNTykUuze4mmaVRWVpruMrc/njIUq0Xhy18r+HHTLv0NM0fBqPNg+Okw+HgYcDhkjYW+Q9GScykedxfqSf9wH5355V34z/FQWdB7AwliZv3s+INkY0zy0SfZGDNzPlLMiDby+sZx4SHuu27P/uTX7n/wFQVt4rUw8yOIy4Cdv8Kzk2H9++1v72yG3Zvdqwyb8IdNCCFEz5AJwKJdt5yQz/yfS1lVUs0nv5Rz6sjM7r/YgMPhmm/hnStgyxJ461IYe4l7FeGabVBb6v67vgLYV8QcOwsm3+GXsQghhDA3OTLjA0VRSExMNOXM8LT4KK48ehAA//z8t7Y3oexAm2zi0+HS9+GIm9zf//xfWPokbPgASldA/Q5AA6vd/fi3D0PJMj+NJviY+bPjK8nGmOSjT7IxZuZ8ZAKw0FXf5OSI2V9S2+jkmUvGc/JBGf554d8+dRcxMX0gMRsS+kFiP0jMcbe9dxWsfRtS8uDaxRAZ65/3FUIIETJkAnAvUVWVsrIyUy4NDe47a884bAAAz367qUvPNcxm6CnutWpO+jtMvMY9gThrLMSmui/fPvVh932gqopg4d3+GErQMftnxxeSjTHJR59kY8zM+Ugx4wNN06ipqTHlzHCPy44cSKTVwootu1m+uarTz/Mpm+hkmPaE++tlz0Hhl11/jSAXDp+d7pJsjEk++iQbY2bOR4oZYSgtPopzxvcD4OlFXTs645O84+CQq9xfv38j7N3de+8thBAipEgxIzp05dGDUBT4YsMOCivqeu+NT7zXPW+mbjv870+9975CCCFCihQzPlAUhdTUVFPODG8pr28cJw5PB+C5b4s79Ry/ZBMZC2c/C4oF1r4F6+Z3/7WCTLh8drpDsjEm+eiTbIyZOR8pZnxgsVhITU0NiWWhfXXNsXkAzP+5lB21jR1u77dssifA0be7v/7o91BX7tvrBYlw+ux0lWRjTPLRJ9kYM3M+5htRL1JVlZKSElPODD/Q+AHJHDIwmWaXyovfbe5we79mc8yfIGOUe97MBzeZYnXgcPrsdJVkY0zy0SfZGDNzPlLM+EDTNBoaGkw5M7w91xzjPjrz2o9bqGs0vlv2ll0NrNi8yz/Z2CLdp5usdij4HJY8FvIFTbh9drpCsjEm+eiTbIyZOR8pZkSnHTcsjby+sdQ1Opn3U4nudm8vL+Hkfy3h1o9L+frXCv+8edpwOGHfmjNf3uu+wsnR8ekuIYQQ5ifFjOg0i0XxHp15fkkxzc7WhyqbnSp3LfiFP76zxvvYXR+sp77J6Z8OHHY9nHSfe0LwqlfhpVOhdrt/XlsIIUTIkmLGBxaLhYyMDFNOptJz5tgs0uLtlNc28uHq/YVERW0jFz23lP8u3QLAjZMH0y/RTllNIw99+qt/3lxR3Pd2mvEuRCW57+n0zLGwdal/Xr8XheNnp7MkG2OSjz7JxpiZ8zHfiHqRoigkJSWZ8jI3PXablcuPzAXgmW+L0DSNFVt2c/rjS1i+ZTfxdhvPz5zAH04eyoPnjgHgv0u3dGn14A7lHQdXfwNpB0FDBbx0Oix/wX+v3wvC8bPTWZKNMclHn2RjzMz5SDHjA1VV2bRpkylnhhu5aGJ/4uw2Nu6oZ9a7a7jw2R+oqGsiPy2OD246iuOHp6OqKlnWWs4d3w9Ng1nvrqHR4fJfJ1Jy4Xefw4gzQXW4L9v+8BZwNvvvPXpQuH52OkOyMSb56JNsjJk5HylmfKBpGs3NzaacGW4kMTqCiyb2B+Ct5dtwuDROHZnBghuOJDfVfYdrTzZ3TBlGapydop0NPPF1oX87Yo+D816G4+8GFFjxErx2bkhMDA7Xz05nSDbGJB99ko0xM+cjxYzolsuPHEh0hBWLArNOGcYTF40j1m5rs11idAR/O/MgAJ76pogNZbX+7YiiwNG3wcVvQ2QcFC+Ct2eCy/jScSGEEOYhxYzolszEaD686Sg+//2xXDcpz/Ac7JSDMzhpRDpOVePP767BpXbvfwXbq/dyw2srWbppV9sH80+Ei94EWxRs/BTeuxpUP57WEkIIEbSkmPGBxWIhOzvblDPDO2NwWhyD0+LafaxlNoqi8PdpBxMfZWP1thpe/K5z93c60OxPfuXjtWXc8d5a1PYKooFHwQWvgSUC1r0HH94MQXpuONw/O0YkG2OSjz7JxpiZ8zHfiHqRoijExcWZcma4rw7MJj0hijtOHQ7APz//ja279nTp9Yp21vPRGvel4JsqG1i0cWf7G+afAOc+716L5udX4bM7gnK1YPns6JNsjEk++iQbY2bOR4oZH7hcLjZu3IjLJaczDtReNhceksNhg1JodKjcMX9tlyahPfF1IZoGEVb3D+ELRkd3RpwJZz7h/vrHp+Dr+7s1hp4knx19ko0xyUefZGPMzPlIMeMjM17i5i8HZqMoCg+cPQq7zcKSwkreXrGtU6+zddce3l/lPirz2AVjsCiwuKCSjTvq9J805iI49Z/ur799GJbM6c4QepR8dvRJNsYkH32SjTGz5iPFjOhVA1Nj+f2JQwC4/+MN7Kxr6vA5Ty0qxKVqHJ2fyumjsjhpRAZAx3NvDr1q32XbwBd3w6f/B2vegs3fwe4tcsWTEEKYRNtraYXoYVcelcuHq7ezbnstf/toPY9PH6u7bWn1Xt7ZdwTn5uPzAbjiqFw+XVfOeytL+ePJw0iJjdR/s6Nvg+Z6WPwILH3ygAcViM+AhH4w6FiYdAdY5UdCCCFCjRyZ8YHFYiE3N9eUM8N9ZZSNzWrhgbNHYVHgw9Xb+erXHbqv8+yiIhwujcMGpXDIwBQADhmYzMH9Emhyqrzx09aOO3PcXe45NKOnw8CjITkXrJGABnVlULocFj+C+vXs7g63y+Szo0+yMSb56JNsjJk5H/ONqJfZbPI/eT1G2YzMTuR3R7nv8fSX+b+0e2ftitpG3lhWAsDNx+V72xVF4Yp994d65YfNbe7e3YaiwNgZcNbTcNlHcMsqqm8r4Ynx/+MC9X5mO6a7N1vyCBR91aUx+kI+O/okG2OSjz7JxphZ85FixgeqqlJQUGDaCVW+6Ew2vz9xCDkp0WyvaeSfn/3W5vFnv91Es1NlXP8kDs/r0+qx00Zl0jfezo7aJj75pazT/arZ4+DRz3/jqIcW8fB31fzYnMvnyRfyunMyChqOt66EuvLOD7Sb5LOjT7IxJvnok2yMmTkfKWZEwMRE2vjHWSMBePmHzazcutv72K76Jl770X0K6abj89usi2C3WbnksAEAvLCkuMPLvGsbHcz5YiNHPfQV//6qkPomJ8MzE3jmkvF8dfuxrB15BxvUHCKaduF463edXz04CNewEUKIcCPFjAioo/P7cvY49521/+/dtd5TRs8vKWavw8Wo7EQmDenb7nMvmtifSJuF1dtqWLm1Wvc9fijaxXH//IY5XxRQ1+hkSHocT108jo9vOoqTD8pAURTuOmscD8T/mQbNTkTJErRvHjTueGMNLLgBHhrkvkJKCCFEwEgxIwLuL6eNICU2kt921PHMoiKq9zTzyg9bALhx8mDd1SpT4+xMG5MFtL+InqZp/GfxJmY8/yOV9c0M6hvL3IvG8uktxzBlZCYWy/7XjYm08ecZZ/BX9Sp3w7cPwaZF7Xd40zfw5BGw6lXYWwXzr4G173Q/ACGEED5RNDPeC7yF2tpaEhMTqampISEhwa+vrWkaqqp67z8k9utqNu+vKuWWeauItFo4bVQm838uZVhGPP+7+ehWRceBNpTVMuVfi7FaFL7902T6JUUDsLfZxZ/fW+NdbO+ssf34x1kjiY60Gvbj1aVbsH10MxfavsER3ZeI67+D+HT3g8173OvV/PSs+/vkXMgcDesXuG+fcM7zcPDZHYeDfHaMSDbGJB99ko2xUMunK7+/5ciMj5zOtlfhCLeuZHPG6CyOHdKXZpfK/J9LAbjxuMGGhQzA8MwEjsjrg0vVeOWHzYB7xeCznvyO91dtx2pRuGfqCB49f3SHhQzAxRP78/2QP/GrmkPE3p0437nSPX+mZBk8fdT+QmbC7+DaJXDui+4rpTQV3r0S1r/f6THLZ0efZGNM8tEn2Rgzaz5SzPhAVVWKi4tNOTPcV13NRlEU7pt2MNER7oIjr28sUw7O7NRzPZdpv/HjVj79pYypc5fwa3kdqXGRvH7lRC47MrfT/wtRFIW/n3cof4/+I3s0O7Yt36K9PBVeOAmqiiA+C2a8B6c/CvY4sFhg6uMw+iLQXPDOFbDhww7fRz47+iQbY5KPPsnGmJnzkWJGBI2clBjuPfMgEqMj+MtpI7B2cFTG47hhaQzoE0Nto5NrX11JzV4HY3KS+PCmo5g4qE/HL3CAxOgIbr/4DP7qugIAZct37iMvoy6E67+Hwce3foLFAmfOhZHng+qEty+DX//X5fcVQgjRPVLMiKBy/oQcVt99EpOHpXX6ORaLwuVHDPR+P/3QHN685jAyE6O73Y9x/ZMZfOJVPOWcSpGWxdYTn4Gzn4HoZJ1OWGHaU3DwOe6C5q1LYeNn3X5/IYQQnRf0xUxpaSkzZsygT58+xMTEMGbMGFasWBHobnmZcVlof+nNbKZP7M9Nxw3m39PHMvvsUdhtHc+P6cjVRw9i6aCbOb7pn1z4bd+Ob4pptcFZz8KIaaA64M0ZUPCF7uby2dEn2RiTfPRJNsbMmk9QX820e/duxo4dy+TJk7nuuutIS0ujqKiIgQMHkpeX16nX6MmrmYT51exxcNaT37GpsoFx/ZN4/arDiIrooFByOfbNnfkArHaY/kbbU1NCCCEMmeZqpgcffJCcnBxefPFFDj30UAYOHMjxxx/f6UKmp2maRn19fYerz4Yjs2STGBPBf2ZOICHKxsqt1dzx3tqOx2SNgHNfgGGng6sJ5l3U5p5PZsmnJ0g2xiQffZKNMTPnE9RHZkaMGMHJJ5/Mtm3bWLRoEf369eP666/nqquu0n1OU1MTTU37TwfU1taSk5NDVVWVt7JTFAWLxYKqqq12ql6755r8A9s1TaOwsJBBgwZhtVpbbQ+0mTGu1261Wr3X/x/YF732zva9q2PytLtcrZfz7+qYADZu3EheXp43m1Ae03eFlVz+8gpcqsafTh7CNccM6nhMqhPtrUtRNn6CZotCvfANlEGTsFgsOBwOCgoKGDx4MFarNWD7KRg/e83NzRQWFnqzMcOY/LmfXC4XhYWF5OfnExERYYoxHdiX7o7pwGzMMKaO2rsyJpfLRVFREfn5bW8RE4xjqq6uJjk5uVNHZoL69pmbNm3iqaee4rbbbuOOO+7gp59+4uabb8Zut3PppZe2+5zZs2dz7733tmkvKioiLi4OgMTERDIzM9mxYwc1NTXebVJTU0lNTaW0tJSGhgZve0ZGBklJSWzevJnm5mZve1aWe/XZ4uLW9wbKzc3FZrNRUFDQqg/5+fk4nU6Ki/evVmuxWBgyZAgNDQ1s27bN2x4ZGcmgQYOoqamhvHz/jQ9jY2O9xVllZaW33V9jys7OJi4ujqKiolYf0q6OKS8vD4fDQWFhofeHJJTHlGGxcM/UEdz1/joe/mwj0Y5aDu8f2+GYdh3/KPb6euK3L0Z540J2T3mGPhPOoqKigqqqKm8+gdpPwfjZKy4ubpWNGcbkz/2kqipVVVVs3bqVvLw8U4zJX/vJk01VVRXp6emmGJM/95Oqqt4/mzZtCvoxbdmyhc4K6iMzkZGRTJgwge+//97bdvPNN7Ns2TJ++OGHdp8jR2aC439dYK4jM+DeT39ZsJZXl24lJtLK29ccxvDMhI7H5GjE8s5MlILP0WzRKBe/jSP7MDkyI0dm5MiMHJmRIzPhcGQmMzOTESNGtGobPnw47777ru5z7HY7dru9TbvVam1VcID+rO7OtquqSmRkJDabrd3nHPh+Ru2KonSp3de+d9Telb63166qKna7vd1sQnVMAHdPPYjiyga+K9zF1f9dyfs3HklqnN14TPYYuOBVmHcxSuFCeP18rBe9RVRUvzb5BGJMwfbZs9lsREVFtckmlMfkz/2kKApRUVGt/pMQ6mPqbHtHfTwwGzOMydf2lu+pKAp2ux2LxdKlDIJtTO0J6iMzF110ESUlJSxevNjb9vvf/54ff/yx1dEaI3I1k/C3mj0Opj35HcX7rnC6eOIAbFYFm8WCzaoQYVWwWizE2a2Mzk7CZt33A+lo3DcZ+EuIiIXzX4b8EwM7GCGECFJd+f0d1MXMsmXLOOKII7j33ns5//zz+emnn7jqqqt49tlnufjiizv1Gj19o8mamhoSExM7vVx+uDB7NkU76znrie+obTS+z8m4/kk8fcl40uKj3A2ORpg33Xt1k5Z3PMqJ90LGyJ7ucsgw+2fHV5KPPsnGWKjlY5pLsw855BDmz5/PG2+8wcEHH8zf//535syZ0+lCpqepqkp5eXm780XCndmzyesbxyu/m8ipIzM4Zkhfjsjrw6EDUxjXP4lR2YkMz0wgNtLKyq3VnPH4d6wuqXY/MSIKLnwd9dCr0Sw2lKIv4emj4b2rYXcnJrvV74S91T05tIAz+2fHV5KPPsnGmJnzCeo5MwCnn346p59+eqC7IUQbY3KSePLi8bqPb9pZz1WvLKdoZwPnPfMDD5w9krPHZUNENNrJD1Dc92RyN7+GZd17sOZNWDcfDrkSjv4DxPaBpjrYvgq2r4TSFVC6EmpKIKYPXPUVJA/stbEKIUQwC/piRohQNahvHPNvOJLfz1vFl79WcNtbq9lQVsusU4ahAI74bLSz/wNH3gxf3AObvoGlT8LPr0JCP9j5K9DOWeA9u9wrDF/+Kdgie3dQQggRhIL6NFOwUxSF2NjYkDj32NskG7eEqAieu3QCN04eDMBzi4u5/KVl1DY69+eTNRYufR8umQ8Zo6CpFnZuADRIzIHhZ8AJ98LMj+D6pRCV5D5S82Xb9ZTMQD47xiQffZKNMTPnE9QTgP1BrmYSweLjNWX84e3V7HW4GNAnhmlj+lGz10HtXge1jQ7313uaGLZ3JeOyorn03LNR4jPavtCv/3NPIgaYPg+GTundgQghRC8wzdVM/tCTxYxntcmUlJQuXQ8fDiSb9q3fXstVryyntHpvh9s+dsFozhqb3f6Dn94BS59wH6W5dgkk5fi3owEknx1jko8+ycZYqOXTld/fMmfGB5qmUVlZSXJycqC7EnQkm/aNyErggxuP5OlFRWyv2EX/jFSSYiJJiI4gcd+fbwt28syiTdz74XqOGtyXvvFtF4HkhHugZKn7dNM7l8Pln7hvcGkC8tkxJvnok2yMmTkfKWaE6GV94uz8+ZShFBQUkJ+f32YFzUNzU1i8sZL1ZbXc8+E6nrhoXNsXsUW678z99DGwbRl8+Tc46e+9NAIhhAguwX+cSYgwE2G18NC5o7BaFD5eU8Zn68rb3zB5IEx7wv319/+GjZ/1Wh+FECKYSDHjA0VRQmYlxd4m2RjrKJ+D+yVy9TGDALhrwS/U7HW0/0LDp8Kh17i/nn8t1JT2RHd7lXx2jEk++iQbY2bORyYACxGkGh0uTv3XYjZVNnDBhBwePHdU+xs6m+D5k6BsFeRMhHNfhMR+vdpXIYTwN9PcziDYqapKWVmZKZeG9pVkY6wz+URFWL0FzJvLS/iusLL9DW12OO9FsCdAyY/wr9Gw4AbY+VtPdL3HyWfHmOSjT7IxZuZ8pJjxgeemXSY/uNUtko2xzuZzyMAULj18AAB/fm8Ne5p1bmyZMgguWQADjwbVAatehScOhXkXQ8kyP/e+Z8lnx5jko0+yMWbmfKSYESLI/emUYfRLiqakai///Gxjq8c0TaOwop6Xvivmyi9VLtf+St2MT2DYvvuZ/foRPH8CvHgaFHwBJvxHTAgh5NJsIYJcnN3G/WcdzGUvLuPF74s5Oj+VuiYnSwp2sqSgku01ja22vys6izkXvuY+zfTdv903sdyyxP1n1IVw+mMQGROg0QghhP/JkRkfKIpCamqqKWeG+0qyMdbVfCYNTePscf3QNLj8pWXc/MbPvLV8G9trGom0Wjgirw/XTcrDosCCVdvdl3P3Heq+dPuW1XDY9aBYYM08+M/xUFnYwyPsPvnsGJN89Ek2xsycj1zNJESIqN7TzKn/Wsz2mkaGZcRz1OBUjh7Sl0MHphAd6V5478FPf+Wpb4pIjYvk898fS0psi7tqFy923227oQIi42HakzDijACNRgghjMm9mVro6XszlZaW0q9fv5C4z0VvkmyMdTef+iYnTQ4XfeLaucUB0OR0MfXxJWzcUc/U0Vk8Pn1s6w3qyuHty2Hr9+7vD7/RfWuEILoVgnx2jEk++iQbY6GWj1ya3Us0TaOhocGUM8N9JdkY624+cXabbiEDYLdZ+ed5o7FaFD5cvZ1P1pa13iA+A2Z+AEfc5P7+h7nw8lSoLWv7YgEinx1jko8+ycaYmfORCcBCmMyo7CSuOzaPuV8X8pcFv3BobkrrAsgaASfd515gb8H1sPUHeOZoGHIyJA2E5AGQNMD9d1w6tDy/3rwH9lTCnl37/uyGnEPct1YQQogAkWJGCBO66fjBLFy/g9921PHXD3RuVjl8KqSNgLcuhR2/wM+vtt3GFgWJ2e5Vhhsqwbm37TaxfeGabyEhy/8DEUKITpDTTD6wWCxkZGSExLnH3ibZGOvpfFqebvp4TRkfr9E5jdQnD678wn0H7sl3wpgZMOAoSMxxX/3kbIRdhVBTsr+QsUZCfBakj4T4TGjYCW9fBs5mv/RdPjvGJB99ko0xM+cjE4CFMLFHP/+Nf39VSEpsJJ///hhS25lvo2kaTU6VqAhr6wdcDncRU1MKETEQkwKxqRAZt//UU9UmeGYSNNXAxGthyoM9PyghRFiQq5la6OmrmTZv3szAgQNNWen6QrIx1lv5NDtVzpi7hF/L6zhxRDrnjc9m2+69lOze4/67ag+lu/dS1+QkNS6SwWlxDEmPJz89nvx9X7e6vLs9v30Cb1zo/vqc52HkuT71WT47xiQffZKNsVDLpyu/v2XOjA80TaO5udmUM8N9JdkY6618Im0W/nneaKY98R0L1+9g4foduttW1jdTWV/F0k1VrdpT4+zcckI+lxw2oP0nDp0CR98Oix+BD26C9IMgbXi3+yyfHWOSjz7JxpiZ85FiRgiTO7hfIneeNpynFxWRFh9FTko0OckxZCdHk50SQ05yNH1i7ZTs3sPGHfUU7KijoKKejTvq2LZ7L5X1Tdy14BcsClw8UaegmXwnlK6ATd/AmzPgqq8hSk7rCiF6hxQzQoSBy4/M5fIjcw23SY6NZFR2Uqu2Pc1O/v1lIU8vKuIvC34hJtLKWWOz2z7ZYoVzXoBnjnFPGH7/ejj/v60v697nt/I6Pl5bhtOlcssJ+dht1rav1x7VBT88Ab+8614nx8fTWUII85A5Mz7wLEAUGxtryntd+EKyMRZK+Wiaxj0frOPlH7ZgtSg8efE4Tj4oo/2Nt62AF08BVzOc+Dc48hYAinbW89HqMj5as52Cinrv5scNS+OpGeNaFTTtZrNzo7tA2rZs/3uNuxROeTDsbpoZSp+d3ibZGAu1fGQCcAtyNZMQvlNVjT++s4Z3V24j0mrh+csmcHR+3/Y3XvY8fHwbmmLhfyMe4cVtmSzf4QLc/3hGWi0cObgPP2zaRaNDbbeg2f/G+47GfHUfuJpossbyhXoIU7RFWNDc6+Sc95L7pppCCFORYqaFnixmXC4XRUVF5OXlYbV28lB5mJBsjIViPk6Xyk1v/Mwnv5QTHWHlv787lAkDU1pt41I1vtqwg+j/3chRDQu97Q7NSoMtESWmD7HJ6djiUinV+vDIuliWOXLJH3IwT10yHrvNuj+bRBXrhzfBtp8AWG0fz7U1l1FGH46w/MK/Ip6gr1KDyxqN5fRHUMZe3H7HVRds/xm2fA8Dj4J+7SwgGEJC8bPTWyQbY6GWj1zN1ItUVQ10F4KWZGMs1PKxWS3MuXAMe15ZwaKNO7n8xWW8cfVhHNwvkYq6Rt78qYQ3ftrK9ppGoriIORHVTLKtJUprJEJxkeSqgroqqCsAoB/wqAWww67N8fz2yHBGHDIZpd84kjZ8h+WXZ8HZiCsijgfUS3iu5iiiI2z83wn5LN+SzqkbcnjU9gRH8wu8fz2bV3xG5kVzscckuNfGKfoSCr90T0purHYPIiIGZrwHAw4PUIr+EWqfnd4k2Rgzaz5SzAghOs1us/L0jPHMfPEnfiqu4tIXfuKwQSl8vm4HTtV9kDc5JoLzJwxi+MT3ieoTC469sKeqxf2cdrlvjVBVBKUrUMvW0keto8/en+Bb91GYtH3vV5J8GNN3XMw2tQ+D0+J46uJx5KfHcw1QXDmcl5YcxIqVT3CT8hYDt73P1od+xG6PIr1p8wEdT4S4NNhVAK+dBzPfh37jOxyvS9W4/a1VlNc28tgFY8hMjPZfmMFk3XxY8TJM/Zf7nlxChBgpZoQQXRIdaeX5mRO4+D8/smZbDf9bWw7A+AHJzDisP1MOzmy9mnBENCT2c/9ph8XZxOrlS3j/4w84iEKOsG8hJaKZeTEXcXfpBEBh2pgs7j9rJLH2/f9k5abGcu+00VSfNJcPPz+BI1bNoj/l0AQuTWG1lsdy6zh2Zx1N6pDDmNA/nlHfXImyZQn892y47CPIGGk41qcXFbFg1XZA4/xnfuD1Kw8jJ8VkE46b98DHt7uLzM/ugAtfC3SPhOgymTPjA88CRJGRkSExM7w3STbGzJDP7oZm/vrBOpKiI7hoYn+GZ/r28/VdYSVXvLSMJqdKhFXB4dKItFq4+4wRXHRo/w5zaq6poPibl1lXF8OCmsEs3a7S7Gp9SP3EvBieUe7Hsm0ZxKTC5f/TnTy8pmQ3jz/9BLdZ5xFraebmphuoSDiY1686jIGpsT6N1cjPW3dzz4frmZibwu0nDWkzMVprrKFZtRIZ7acrUn56Dv73h/3fX/EZ9D/M99cNADP8XPWkUMtHJgC30NPFjKqqWCyWkPhg9CbJxpjk076WBU1OcjRPXjyekdmJ3XqtRoeLtaU1LN+8m+Wbq/iuqJJGh8rFoxO5r/YOlLLVEJfhLmj65LV+7pblbHjlFsa6fvG2ObBxr+MSPo8+jdevPpzBaXEd9qHZqRJp6/yy8QvX7+CmN1bS6HAXYYekK8w5Bvrt/Q22r3JPZK7egpZ+MFz6AUpsn06/drtcDvj3OKjZiis+G2vdNsg+FH73ebtrBHWJqsJvH0NSf8gc7dtrdZL8XBkLtXykmGmhp69mKigoID8/PyRmhvcmycaY5KNv5eZdvP/jRm45bSwpcVF+e91FG3dyxUvLcKkafzkunSsLb4KK9e47hF/+CSTlwO4t8NXfYe3bADQRgXbotUTVbYENHwCwwHUE/4y8nv9cdSzDMtr+m9LsVPnf2jJe/H4zv5TWcMlhA/jDyUOJ85wiczmhfDWUroSmOvedyR17+XVbBauLy7HTTHaMi4ymYrKp0B2PljkGZeYHENW9Yg+ANW/Be1dRa0nitL1380XU/2HXGuGCV2H41A6fXlazl/dWlnLKwRnk9W1R3FVvhfnXwZYl7u9HTIPj/9qmaPQ3+bkyFmr5yNVMQoiQNToniZjGZBKjI/z6uscO6cs9ZxzEXQt+4b6vdtD/rKc56acr3CsWv3IGDJkCy55zL/gHvOc6iuyz7+fQsWNA0+CHJ9AW/pVp1u8Z7tjKn575A/dfeTYH93MXExW1jbz641Ze/3ErlfVN3vd99ftCStYsYtbwSobsXQ1bf4Tmujb9GwYM8/x+2f90tqhprNVycaaN5oQTTiE6Ng7ttfOxla2C186HS96DyG6c9tI0WDIHgKebTqJES+dpxxRusc2n/uO/EDfkFLC2vw/2Nrt45tsinl5URKND5bWlW/jk1mNIjLLBqtfhk1nuMdqiwNkE6xfAhg9h/Ew4dhbE6yy62Bk1pbD1B8g/SW6ZIbykmBFChI1LDhvA5soGnl9SzI0fbuedi/7LqM8vhKpNsPQJAH5SRnJv44UcesRkzh57kPuJigJH3IjSbxzqWzMZ2rCN19Q/89fntnLk1N+xaONO/re2jAh1L3nKdk6P3cHUrFoGNBcRs2M5Mc5GWNuiI1GJkDMRV3Qffti6h/WVThqJ4JC8LA4b2g8lIhr65KGmj+Z/y6p45PPfcJZq9Htf4bHz8+g76XEGLroRpWQpvDEdLnoLIrp4FKtgIVSso16L4lXXCfzhpCF8/vMFXFz7Jan1m3nj6b9z4sw7SY2ze5+iqhofrN7Og5/+SllNIwARVoXtNY3MfvtbZkf8B+W3/7k3zpkIZz3tnmD85d+g4DNY/gKsngeHXQ9H3tz1o0pNdfDiFKjeApHxMPZiOPTqHj/iI4KfnGbyQagdsutNko0xyUdfT2fjUjWu+e8Kvtiwgz6xkXw4I4esjy9Bs9l5nOk8unkA+WnxfHjTUa2vyvKo24HzrZnYSn4A4CPXRKJpZoiyjRzLznbfc681gcWOISx1DWeN7WCmnngCZ47rz3WvruSHTbuwWRQeOGcU545v575XwKqSam6Z9zNbdu3BalE4IS+O6QNqOebHq7E4GmDIKe57YdkiO51D3VMnEr/jJ55xnkbDMXdz20lDaXK6WPzabE4ofpidWgLTLHP505kTOGN0Fiu3VvP3j9azqqQagH5J0dxx6nCykqJ4+pnHud/2HKlKLVgi4Lg74Yib3ffs8tj8HXxxt/eWFHtsiSwZcAPHX/wnrJZOzt9YcAOsehUUC2ieyd0KDDkZJl6La8DRFBQWtv7sNOyCinWwY537KJw9HuKzICFz/9+xaWD1///tHS6VPc0uvx9l7K5Q+3dH5sy0IBOAA0OyMSb56OuNbBqanJz/zA+s217L4LQ43r3uCD79pYxZ764lwqqw4IYjOSjL4KiBy4lj4T1ELH287WMxqZA23H2VVN9h0P9wSBvBrxX13PHeWlZurQbAbrPQ5FSJs9t4asY4/dtD7FPX6OCv769j/s+l3raJygZesT+InWY2Z5xE4xnPMiQjCUsHxcGu9d/S562pNGk2/jLgNR687OT9z3E5aPrXIdhri/mX82wec57L0PR4ftvhPjUWG2nlhuMGc8WRuUS56uHT/4NV7su5N2r9iZ/+ApnDDmn/jTUNx/qP2LngDrIcWwGYn/N/TLtiVsf7ev378NalaCg83n8OozOiOHznW0QWf7n/5fsOQxt1IcqeXSgV690FTH258euCuziKy4CxM2DS/4Gl85O221Pf5GTeT1v5z+JidtY3cdkRA7n1hHziowJb1PTEz1Z5TSN/ff8X/nH2yFZH8fxBipkW5NLswJBsjEk++norm/KaRqY98R3ltY0cMjCZddtr2dPs4s9ThnHtsZ07baEVfola9DXWlFx34dJ3KMSm6m6vqhqv/bSVhz75lbomJ2nxdl68/BDjwukA326s4KsNO1hZUsO67bUcxSqei/gnkYqLd1zH8Fj0zVx73BDOn5Dd7v2uGh0u1jx8Coc2/8QnESdx9B/e2D852WNf4dBsiWZS0yNsdyWhKHD++BxuP3kIafFRUPgFfHAz1JaiofB+7Dn8addUDu7fl7euORybtW1B4HCpXP/aSr5av51ZkW9xteVDHJqV9w/+N+eeN0N/0LVl8NThsHc3TzjP4GHnhQBYFDgzp4Gr7F8yrPwDLI497T8/ORfSD4LUIdDcAHXb3a9ZVwZ15aC59m978Lkw7Umwdf0X8676Jl7+fjMv/7CFmr2OVo/1jbdzx6nDmDamX8B+5v39s1VavZeLnlvKll17OG5YGi9cplPEdpMUMy3IaabAkGyMST76ejObX0prOP+ZH9jT7P5lNjE3hdevOqzzpz26aUdtI5+sLeOUgzPJSOzaXJeW+TQ6NVaXVLNr+buc+uufsaLyuvM4/uq8jL6JcVw/eXCrokbTNB585T3+XHwFKgrlMxaTNbidhQM1DZ4/EbYtY/eIGTybcBOnjcx0T3ZurIHP7oSf/+veNjkXpj3JtoQxTJmzmLomJ7eekM+tJwxp9ZIOl8rN++7tFWmz8Pyl48n+6mZyyz+hVovhqyNfZdpJx7fti6qivXo2yqavWasO5OzmvzFpRD+27d7LhrJa72bx7OHmlKUcrawhIm0wSvpBROeMInngaKJiDYpF1eVekXrjp/DxbaA6IfcYuOC1Tk8wLqnaw38Wb+LN5SXey+oHpcZyzbGD6Btv5+8fbaC4sgGAQwYmc+8ZBzMiq/cnL/vzZ6ukag/Tn1vKtt17yUmJ7pEFJaWYaUGKmcCQbIxJPvp6O5sv1u/g6v8uJ85u43+3HE12cnCv8Kubz5q30d67CgWNdcpgbmi8js1aJpmJUd6i5qXvNpP6xS2cY11MZf8ppF4xT/+NtnzvnmyrWOH6H9xHnVocjQFg4rXuS673XU31/qpSbpm3CosCb197OOMHuG9E6nSp3PLmKj5eU0ak1cKzl45n0tA0cDRS+vhJ9KtdTYnWl3VT3uWUw1qvSaMtfQrl0z+zV4vk9Ob7Of24Sdx6Qj6KolBStYfP1pXz+fodLN9charz2ywxOoKMhCjSE6NIjI4gIcpGfFQE8VE279eJ0REcqq0idv7l0FwP6SNhxjuGV17tbXbx94/X8+ayElz73nxUdiLXT8rjxBEZ3qK4yeniP4uLmftVIXZHNSdYf+bK1HXk23ZgnfqY+waofvDz1t389f11ZCZG8ZfTRtC/T+vPsr9+toorG7jouaWU1TSSmxrLa1dOJCvJ/7f6kGKmBSlmAkOyMSb56AtENkU764mz20hP8N+6Nj3FMJ8NH8L7N0JjNQ5rFA9xBc81HAkopCfYiajbxteRtxGhuOCqrzu+g/gbF7kXvht8ovuXesujMWc+AQOPbPOU37+5ivk/l5KTEs3/bj6amEgbv39zFR+s3k6EVeGZS8Zz3LB07/ZaQyVV/zqGPs2lrFIHU3fhAo4ekeMea/l61GeOJUJr5i+Oyxl4ys1cefSgdrtaWd/EF+vK+Xz1Zhq0SCrqmiivaWSvw9Xu9u3pG2/n6eOtjF98FTTshMT+7kvfU/PbbLtlVwPX/HcFv5a75xIdnZ/KdcfmcXhen7ancKpL4Lf/0bT2fWzbfsDK/pWp9yoxvDbsSWJzxzM4LY68vnGkxHZ+Ije4J7U/vaiIeQt/4FTlO2y4KLFkcfghEzn7xGOwR7vXAGrz2WlucE+Krixw/4mMhUN+Z3ipf2FFPRc9t5SKuiby+sbyxlWHkdZDPzdSzLTQ08VMKN1OvTdJNsYkH32SjbEO86kphfnXwObFAGxJO56rdl/KxroI7ra9zOW2z9Byj3UvuNeRnb/Bk4e1vnLIezSm/SNYtY0OTv3XYrbt3stZY/uhAO/9XIrNovDUjPGcOCK9zXPUnQXsfWoysWodn2oTSbviDUZmxLDj0SPJbiria9doKqb+lwsONb4J5oHZaJpGbaOTHbWNlNc0sqO2kdpGJ3WNDuoandTudf9d1+SgeGcD2/ddbv77CRHcVDoLy+5NEJ3svvQ951Dv+3y5YQe3vrmKukYnqXGR/PvCsRwx+IC5Uru3wC/vuucfla1q9VBD0jDe2TOGoY2rOcyygUotgXOb72azlglASmwkg9PimHJwBueMzybBYOJw+e4GXvnvfxi7cwHHWX7GqrT9ld4Um4U9fQhqymBqa2pIdFag7Crcf4StpeRc9yX17dzS4rfyOi7+z1Iq65sZmh7Pq1dOpG+8fyf9tiTFTAs9WcwIIURQUl3w/ePw1X2gOtDiMlky+DYOW/MXItQmuGQB5E3u3Gt9dBssf947N4YBR3T4lOWbqzj/mR+8p32sFoUnLhrHKQfrn7JxFC1G+e80bDh5nmlkxEdwWt3bVGnxrDjtY048tGdvidDocDH7fxt4+YctABzaV+Xl6H8SXbEKbNFw4t9w5Uxk7lorj33t3mZc/ySevHj8/nlP9RWwbgH88g6U/Nji1RX3VW3DTnP/ScnF4VJZU7iV3I8vIKX2V3ZaM7jSdj+ra1qfromJtDJtbD8uPXxA6xWna8so/OwpYte9RiaV3mZt4FGQmM3urRuw7i4ikXrjgcf0gT750GcwbPoGare5+3vETTD5Tu/6Reu21zDjPz+ye4+DEZkJvHrlxC4fQeoqKWZa6OmrmRoaGoiN9dMN30xEsjEm+eiTbIx1KZ/tP8O7V8Gugv1tmWPg6m86f+8llxO2fAfZh+gejWnPows38u8vC7BaFP594VhOG5XZ4XOaVryG/cPrW7WtPeopRp5wUafe0x+fna9/reCP76ymsr6ZRFszH6X/h5xdS7yPOzQrRVoWTX2GcdDYI7FljoT6He4CZtM3rY9iDTwKDj4Hhp0OcTqX3tdXwAsnuxduTBvBnos/ZFN9BMs3V/Haj1spqNhfjBw6MIUbh9Vy+PaXsWz8FCvuU2i1SjzqqOkkHX11q1NiNQ3NPPHJUlauXE6uUsawiB0MS4/DkTyY5qTBuPoMxh7fhzh7BLF2K5GOepIX/5XUwnfcz4/L4+sRf2dzZD4vfreZmr0ORmcn8soVE0mMifCE7j7CY4syvJKvO6SYaUHmzASGZGNM8tEn2Rjrcj7NDe6rj1a86P7+vJfgoLN6tI/gnvT7+k9bGZIez2GDOn9DzL2f/Z3oH/4JwI78C0m/+JlOP9dfn53K+ib+9M4avvq1AisuZqd9SX79MnJdm0lSGoyfnDUORp7nzjih4wIOcJ+Sev4k95o4ORPhkvkQGYumaSzdVMV/l26mfP133GR5h8nW1d6n/agOo2zwhUw5/2rsUfrzXFaVVHPn/LWs216ru01LJ1hWMDviP/RVanBoVuY6p/GE60zG9k/hxTP7EFe1HspWQ/kaKFsDe6vgxL/Bkbd0brydJMVMC1LMBIZkY0zy0SfZGOt2PpsWuf8HPXq673fE7kmahvb1bNSqTVinzgF7x3cn9/DnZ0fTNF79cSv3fbSeJqf7aEv/5Gj+c1Y/hrAZdvwC5b+4F+azRsKIM+Hgs7t/a4Ud6+HFU9yXvg8+Eaa/4b43VslP8M0DUOReHNCJhQWuo3gr8ixuuGAqxw4xXmzRw+lSeXt5Cd+t34otOo76JhcNTU7qm5w0NDmpa3LiUjXi7Dbi7DYyIxq4tv4JDtnzLQB19nTi1HoURzvFnGKFw2+Ak/7evbHrMO2NJmfPns0dd9zBLbfcwpw5cwLdHSGECB2Djg10DzpHUVCOu4NAl7GKonDJYQM4fFAKd3+wjtQ4O3874+B9p1eGum+h4E/pI+Cit+GVM6FwIbx9GTj2QNFX+zpkhdHTUY/4PRk1STyTlUByF+as2KwWzp+QzdjEvZ0v9rST3ZOYP76d+MYd7raIGPcChBmjIHOU+++0EV2/N5ifhUwxs2zZMp599llGjRoV6K54KYoiK7jqkGyMST76JBtjko++nshmcFo8r13Z9sqeHtF/Ipz/CsybDr9+5G5TrDBmOhx9O6QMIhI4Kq17L9/lfBQFRp4LgybBtuWQkuueKGwJdKnZVkicZqqvr2fcuHE8+eST3HfffYwZM6bTR2bkaiYhhBAh5Zd34fO/uq84O/p2dxERhkx3mumGG27gtNNO44QTTuC+++4z3LapqYmmpibv97W17glPLpcLl8s981tRFCwWC6qq0rKW02v33JSrve1ra2uJi4trVela9t2kTFX3L4xk1O5ZD6Flu6cveu2d7XtXx+Rp92TV3TFZLBaqq6uJj4/3ZhPqY/LnfnK5XN4fUEVRTDEmf+0np9NJbW2tNxszjMmf+0nTNO8/8kZjDaUxHdiX7o7pwGxCdkwHn4M64qz97S6XX/aTpmnU1dWRmJjYatteGVM3P3udFfTFzLx581i5ciXLli3r1PazZ8/m3nvvbdNeVFREXJx7IlliYiKZmZns2LGDmpoa7zapqamkpqZSWlpKQ8P+SU4ZGRkkJSWxefNmmpubve1ZWVmUl5ejKEqrHZGbm4vNZqOgoMXlkEB+fj5Op5Pi4mJvm8ViYciQITQ0NLBt2zZve2RkJIMGDaKmpoby8v13fY2NjSUnJ4eqqioqK/evLeCvMWVnZxMXF0dRUVGrD1JXx5SXl0dJSQk2m837QxLqY/LnfiovL6e4uJiUlBQsFospxuTP/bRz505vNmYZk7/2k6qqVFVVkZmZSV5eninG5K/95MkmPz+f9PR0U4zJn/tJVVVUVSU2NpZNmzYF/Zi2bNlCZwX1aaaSkhImTJjA559/zujR7gWTJk2aZHiaqb0jM55wPYep/PU/FE3TKCwsZNCgQa0mUwXD/1C6OyZ//Q8FYOPGja1WKQ31MflzPzkcDgoKChg8eDBWq9UUY/LXfmpubqawsNCbjRnG5M/95HK5KCwsJD8/n4iICFOM6cC+dHdMB2ZjhjF11N6VMXlWSM7Pz28zbyYYx1RdXU1ycnLon2ZasWIFFRUVjB8/3tvmcrn49ttvmTt3Lk1NTW1mZNvtduz2tssrW63WNtt6dt6BOtvu+ZC099qe9va0164oSpfafe17R+1d6Xt77a59h0XbyyZUxwT+3U8Wi6VNPqE+Jn+0ewqYA7MJ9TH52t6y757Pz4HtvvQx0GPqTHtn+tgyG7OMyZf29v791etLe9t7nhNMY2pPUBczxx9/PGvXrm3VdvnllzNs2DBmzZoV8DUoFEWRVUp1SDbGJB99ko0xyUefZGPMzPkE9Wmm9nR0mulAcjWTEEIIEXq68vu788dwRBuqqlJZWdmlGdfhQrIxJvnok2yMST76JBtjZs4n5IqZb775JmhW/9U0jcrKyjaXuAnJpiOSjz7Jxpjko0+yMWbmfEKumBFCCCGEaEmKGSGEEEKENClmfKAoComJiaacGe4rycaY5KNPsjEm+eiTbIyZOZ+Qu5qpq+RqJiGEECL0yNVMvURVVcrKykw5M9xXko0xyUefZGNM8tEn2Rgzcz5SzPhA0zRqampMOTPcV5KNMclHn2RjTPLRJ9kYM3M+UswIIYQQIqQF9e0M/MFTgdbW1vr9tV0uF/X19dTW1gb81grBRrIxJvnok2yMST76JBtjoZaP5/d2Z44kmb6YqaurAyAnJyfAPRFCCCFEV9XV1ZGYmGi4jemvZlJVle3btxMfH+/3y9Fqa2vJycmhpKRErpQ6gGRjTPLRJ9kYk3z0STbGQi0fTdOoq6sjKyurwztom/7IjMViITs7u0ffIyEhISQ+GIEg2RiTfPRJNsYkH32SjbFQyqejIzIeMgFYCCGEECFNihkhhBBChDQpZnxgt9u5++67sdvtge5K0JFsjEk++iQbY5KPPsnGmJnzMf0EYCGEEEKYmxyZEUIIIURIk2JGCCGEECFNihkhhBBChDQpZoQQQggR0qSY6aYnn3yS3NxcoqKiGD9+PIsXLw50lwLi22+/ZerUqWRlZaEoCgsWLGj1uKZp3HPPPWRlZREdHc2kSZNYt25dYDrby2bPns0hhxxCfHw8aWlpTJs2jd9++63VNuGaz1NPPcWoUaO8i3cdfvjhfPLJJ97HwzUXPbNnz0ZRFG699VZvWzhndM8996AoSqs/GRkZ3sfDORuA0tJSZsyYQZ8+fYiJiWHMmDGsWLHC+7gZ85FiphvefPNNbr31Vu68805+/vlnjj76aKZMmcLWrVsD3bVe19DQwOjRo5k7d267jz/00EM8+uijzJ07l2XLlpGRkcGJJ57ovWeWmS1atIgbbriBpUuXsnDhQpxOJyeddBINDQ3ebcI1n+zsbB544AGWL1/O8uXLOe644zjzzDO9/6CGay7tWbZsGc8++yyjRo1q1R7uGR100EGUlZV5/6xdu9b7WDhns3v3bo488kgiIiL45JNPWL9+PY888ghJSUnebUyZjya67NBDD9WuvfbaVm3Dhg3T/vznPweoR8EB0ObPn+/9XlVVLSMjQ3vggQe8bY2NjVpiYqL29NNPB6CHgVVRUaEB2qJFizRNk3wOlJycrP3nP/+RXFqoq6vT8vPztYULF2rHHnusdsstt2iaJp+du+++Wxs9enS7j4V7NrNmzdKOOuoo3cfNmo8cmemi5uZmVqxYwUknndSq/aSTTuL7778PUK+CU3FxMeXl5a2ystvtHHvssWGZVU1NDQApKSmA5OPhcrmYN28eDQ0NHH744ZJLCzfccAOnnXYaJ5xwQqt2yQgKCgrIysoiNzeXCy+8kE2bNgGSzQcffMCECRM477zzSEtLY+zYsTz33HPex82ajxQzXVRZWYnL5SI9Pb1Ve3p6OuXl5QHqVXDy5CFZuc9R33bbbRx11FEcfPDBgOSzdu1a4uLisNvtXHvttcyfP58RI0aEfS4e8+bNY+XKlcyePbvNY+Ge0cSJE3nllVf47LPPeO655ygvL+eII45g165dYZ/Npk2beOqpp8jPz+ezzz7j2muv5eabb+aVV14BzPvZMf1ds3uKoiitvtc0rU2bcJOs4MYbb2TNmjUsWbKkzWPhms/QoUNZtWoV1dXVvPvuu8ycOZNFixZ5Hw/XXABKSkq45ZZb+Pzzz4mKitLdLlwzmjJlivfrkSNHcvjhh5OXl8fLL7/MYYcdBoRvNqqqMmHCBP7xj38AMHbsWNatW8dTTz3FpZde6t3ObPnIkZkuSk1NxWq1tqlgKyoq2lS64c5zdUG4Z3XTTTfxwQcf8PXXX5Odne1tD/d8IiMjGTx4MBMmTGD27NmMHj2af/3rX2GfC8CKFSuoqKhg/Pjx2Gw2bDYbixYt4t///jc2m82bQzhn1FJsbCwjR46koKAg7D8/mZmZjBgxolXb8OHDvReomDUfKWa6KDIykvHjx7Nw4cJW7QsXLuSII44IUK+CU25uLhkZGa2yam5uZtGiRWGRlaZp3Hjjjbz33nt89dVX5Obmtno83PM5kKZpNDU1SS7A8ccfz9q1a1m1apX3z4QJE7j44otZtWoVgwYNCvuMWmpqamLDhg1kZmaG/efnyCOPbLMExMaNGxkwYABg4n93AjXzOJTNmzdPi4iI0J5//nlt/fr12q233qrFxsZqmzdvDnTXel1dXZ32888/az///LMGaI8++qj2888/a1u2bNE0TdMeeOABLTExUXvvvfe0tWvXatOnT9cyMzO12traAPe851133XVaYmKi9s0332hlZWXeP3v27PFuE675/N///Z/27bffasXFxdqaNWu0O+64Q7NYLNrnn3+uaVr45mKk5dVMmhbeGd1+++3aN998o23atElbunSpdvrpp2vx8fHef4PDOZuffvpJs9ls2v33368VFBRor732mhYTE6O9+uqr3m3MmI8UM930xBNPaAMGDNAiIyO1cePGeS+3DTdff/21BrT5M3PmTE3T3JcB3n333VpGRoZmt9u1Y445Rlu7dm1gO91L2ssF0F588UXvNuGazxVXXOH9+enbt692/PHHewsZTQvfXIwcWMyEc0YXXHCBlpmZqUVERGhZWVna2Wefra1bt877eDhno2ma9uGHH2oHH3ywZrfbtWHDhmnPPvtsq8fNmI+iaZoWmGNCQgghhBC+kzkzQgghhAhpUswIIYQQIqRJMSOEEEKIkCbFjBBCCCFCmhQzQgghhAhpUswIIYQQIqRJMSOEEEKIkCbFjBBCCCFCmhQzQoiwoygKCxYsCHQ3hBB+IsWMEKJXXXbZZSiK0ubPKaecEuiuCSFClC3QHRBChJ9TTjmFF198sVWb3W4PUG+EEKFOjswIIXqd3W4nIyOj1Z/k5GTAfQroqaeeYsqUKURHR5Obm8vbb7/d6vlr167luOOOIzo6mj59+nD11VdTX1/fapsXXniBgw46CLvdTmZmJjfeeGOrxysrKznrrLOIiYkhPz+fDz74oGcHLYToMVLMCCGCzl133cU555zD6tWrmTFjBtOnT2fDhg0A7Nmzh1NOOYXk5GSWLVvG22+/zRdffNGqWHnqqae44YYbuPrqq1m7di0ffPABgwcPbvUe9957L+effz5r1qzh1FNP5eKLL6aqqqpXxymE8JNA37ZbCBFeZs6cqVmtVi02NrbVn7/97W+apmkaoF177bWtnjNx4kTtuuuu0zRN05599lktOTlZq6+v9z7+8ccfaxaLRSsvL9c0TdOysrK0O++8U7cPgPaXv/zF+319fb2mKIr2ySef+G2cQojeI3NmhBC9bvLkyTz11FOt2lJSUrxfH3744a0eO/zww1m1ahUAGzZsYPTo0cTGxnofP/LII1FVld9++w1FUdi+fTvHH3+8YR9GjRrl/To2Npb4+HgqKiq6OyQhRABJMSOE6HWxsbFtTvt0RFEUADRN837d3jbR0dGder2IiIg2z1VVtUt9EkIEB5kzI4QIOkuXLm3z/bBhwwAYMWIEq1atoqGhwfv4d999h8ViYciQIcTHxzNw4EC+/PLLXu2zECJw5MiMEKLXNTU1UV5e3qrNZrORmpoKwNtvv82ECRM46qijeO211/jpp594/vnnAbj44ou5++67mTlzJvfccw87d+7kpptu4pJLLiE9PR2Ae+65h2uvvZa0tDSmTJlCXV0d3333HTfddFPvDlQI0SukmBFC9LpPP/2UzMzMVm1Dhw7l119/BdxXGs2bN4/rr7+ejIwMXnvtNUaMGAFATEwMn332GbfccguHHHIIMTExnHPOOTz66KPe15o5cyaNjY089thj/OEPfyA1NZVzzz239wYohOhViqZpWqA7IYQQHoqiMH/+fKZNmxborgghQoTMmRFCCCFESJNiRgghhBAhTebMCCGCipz5FkJ0lRyZEUIIIURIk2JGCCGEECFNihkhhBBChDQpZoQQQggR0qSYEUIIIURIk2JGCCGEECFNihkhhBBChDQpZoQQQggR0v4fEcgXl94dC94AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = mlp.history.history[\"loss\"]\n",
    "loss_history_val = mlp.history.history[\"val_loss\"]\n",
    "plt.plot(loss_history, label=\"Train\")\n",
    "plt.plot(loss_history_val, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
