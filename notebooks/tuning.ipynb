{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\marco\\miniconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import keras\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.alpha\": 0.5,\n",
    "        \"grid.linestyle\": \"--\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/FS_features_ABIDE_males.csv\", sep=\";\")\n",
    "df = df.set_index(\"FILE_ID\")\n",
    "\n",
    "# drop target\n",
    "y = df[\"AGE_AT_SCAN\"]\n",
    "df = df.drop([\"AGE_AT_SCAN\", \"SEX\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# L1 regularization\n",
    "\n",
    "linear_regressor = Pipeline(\n",
    "    [\n",
    "        ('scaler', PowerTransformer()),\n",
    "        (\"principal_components\", PCA(n_components=20, svd_solver = 'full')),\n",
    "        (\"model\",  Lasso())\n",
    "    ]\n",
    ")\n",
    "\n",
    "param_distr = {\"model__alpha\": np.arange(1, 10)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "search = GridSearchCV(\n",
    "    linear_regressor, \n",
    "    param_distr, \n",
    "    return_train_score=True, \n",
    "    scoring=\"r2\", \n",
    "    cv=KFold(shuffle=True) # 5 split\n",
    ")\n",
    "\n",
    "search.fit(df.values, y.values)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_regressor = pd.DataFrame(search.cv_results_)\n",
    "no_overfit = results_regressor[\"mean_train_score\"] - results_regressor[\"mean_test_score\"] < 6\n",
    "\n",
    "(\n",
    "    results_regressor[no_overfit]\n",
    "    .sort_values(\"mean_test_score\", ascending=False)\n",
    "    .to_csv(\"best_model_regressor.csv\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_model__alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.082170</td>\n",
       "      <td>0.031829</td>\n",
       "      <td>0.018959</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>1</td>\n",
       "      <td>{'model__alpha': 1}</td>\n",
       "      <td>0.627481</td>\n",
       "      <td>0.663396</td>\n",
       "      <td>0.680098</td>\n",
       "      <td>0.636030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.656434</td>\n",
       "      <td>0.021042</td>\n",
       "      <td>1</td>\n",
       "      <td>0.681996</td>\n",
       "      <td>0.671485</td>\n",
       "      <td>0.662360</td>\n",
       "      <td>0.670433</td>\n",
       "      <td>0.664104</td>\n",
       "      <td>0.670076</td>\n",
       "      <td>0.006919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.063576</td>\n",
       "      <td>0.016696</td>\n",
       "      <td>0.019566</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>2</td>\n",
       "      <td>{'model__alpha': 2}</td>\n",
       "      <td>0.592633</td>\n",
       "      <td>0.629701</td>\n",
       "      <td>0.661870</td>\n",
       "      <td>0.599219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625730</td>\n",
       "      <td>0.026458</td>\n",
       "      <td>2</td>\n",
       "      <td>0.647118</td>\n",
       "      <td>0.636664</td>\n",
       "      <td>0.627052</td>\n",
       "      <td>0.641162</td>\n",
       "      <td>0.633921</td>\n",
       "      <td>0.637183</td>\n",
       "      <td>0.006754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.048280</td>\n",
       "      <td>0.011892</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>3</td>\n",
       "      <td>{'model__alpha': 3}</td>\n",
       "      <td>0.565414</td>\n",
       "      <td>0.615736</td>\n",
       "      <td>0.643024</td>\n",
       "      <td>0.570821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602155</td>\n",
       "      <td>0.029571</td>\n",
       "      <td>3</td>\n",
       "      <td>0.620352</td>\n",
       "      <td>0.614544</td>\n",
       "      <td>0.597967</td>\n",
       "      <td>0.615303</td>\n",
       "      <td>0.614108</td>\n",
       "      <td>0.612455</td>\n",
       "      <td>0.007582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.049556</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>0.018789</td>\n",
       "      <td>0.000344</td>\n",
       "      <td>4</td>\n",
       "      <td>{'model__alpha': 4}</td>\n",
       "      <td>0.560284</td>\n",
       "      <td>0.599865</td>\n",
       "      <td>0.629426</td>\n",
       "      <td>0.562328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.589740</td>\n",
       "      <td>0.025870</td>\n",
       "      <td>4</td>\n",
       "      <td>0.610516</td>\n",
       "      <td>0.592324</td>\n",
       "      <td>0.582471</td>\n",
       "      <td>0.604355</td>\n",
       "      <td>0.601269</td>\n",
       "      <td>0.598187</td>\n",
       "      <td>0.009807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.048604</td>\n",
       "      <td>0.015534</td>\n",
       "      <td>0.018924</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>5</td>\n",
       "      <td>{'model__alpha': 5}</td>\n",
       "      <td>0.558501</td>\n",
       "      <td>0.591416</td>\n",
       "      <td>0.619986</td>\n",
       "      <td>0.555186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582313</td>\n",
       "      <td>0.023758</td>\n",
       "      <td>5</td>\n",
       "      <td>0.603441</td>\n",
       "      <td>0.584619</td>\n",
       "      <td>0.574655</td>\n",
       "      <td>0.596781</td>\n",
       "      <td>0.594389</td>\n",
       "      <td>0.590777</td>\n",
       "      <td>0.010073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.049956</td>\n",
       "      <td>0.012525</td>\n",
       "      <td>0.019407</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>6</td>\n",
       "      <td>{'model__alpha': 6}</td>\n",
       "      <td>0.554380</td>\n",
       "      <td>0.581638</td>\n",
       "      <td>0.609292</td>\n",
       "      <td>0.546440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.574712</td>\n",
       "      <td>0.022391</td>\n",
       "      <td>6</td>\n",
       "      <td>0.594793</td>\n",
       "      <td>0.575201</td>\n",
       "      <td>0.565101</td>\n",
       "      <td>0.587524</td>\n",
       "      <td>0.588586</td>\n",
       "      <td>0.582241</td>\n",
       "      <td>0.010668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.044247</td>\n",
       "      <td>0.016093</td>\n",
       "      <td>0.018972</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>7</td>\n",
       "      <td>{'model__alpha': 7}</td>\n",
       "      <td>0.547921</td>\n",
       "      <td>0.570531</td>\n",
       "      <td>0.597345</td>\n",
       "      <td>0.536090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565581</td>\n",
       "      <td>0.021559</td>\n",
       "      <td>7</td>\n",
       "      <td>0.584573</td>\n",
       "      <td>0.564070</td>\n",
       "      <td>0.553811</td>\n",
       "      <td>0.576583</td>\n",
       "      <td>0.581728</td>\n",
       "      <td>0.572153</td>\n",
       "      <td>0.011553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.064272</td>\n",
       "      <td>0.021544</td>\n",
       "      <td>0.019125</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>8</td>\n",
       "      <td>{'model__alpha': 8}</td>\n",
       "      <td>0.539124</td>\n",
       "      <td>0.558095</td>\n",
       "      <td>0.584145</td>\n",
       "      <td>0.524135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554918</td>\n",
       "      <td>0.021271</td>\n",
       "      <td>8</td>\n",
       "      <td>0.572781</td>\n",
       "      <td>0.551227</td>\n",
       "      <td>0.540784</td>\n",
       "      <td>0.563960</td>\n",
       "      <td>0.573814</td>\n",
       "      <td>0.560513</td>\n",
       "      <td>0.012765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.097383</td>\n",
       "      <td>0.063088</td>\n",
       "      <td>0.019175</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>9</td>\n",
       "      <td>{'model__alpha': 9}</td>\n",
       "      <td>0.527989</td>\n",
       "      <td>0.544330</td>\n",
       "      <td>0.569691</td>\n",
       "      <td>0.510576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.542725</td>\n",
       "      <td>0.021521</td>\n",
       "      <td>9</td>\n",
       "      <td>0.559417</td>\n",
       "      <td>0.536672</td>\n",
       "      <td>0.526020</td>\n",
       "      <td>0.549654</td>\n",
       "      <td>0.564845</td>\n",
       "      <td>0.547322</td>\n",
       "      <td>0.014325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       1.082170      0.031829         0.018959        0.000963   \n",
       "1       1.063576      0.016696         0.019566        0.000557   \n",
       "2       1.048280      0.011892         0.018084        0.000540   \n",
       "3       1.049556      0.010791         0.018789        0.000344   \n",
       "4       1.048604      0.015534         0.018924        0.001187   \n",
       "5       1.049956      0.012525         0.019407        0.001010   \n",
       "6       1.044247      0.016093         0.018972        0.000541   \n",
       "7       1.064272      0.021544         0.019125        0.000738   \n",
       "8       1.097383      0.063088         0.019175        0.001426   \n",
       "\n",
       "  param_model__alpha               params  split0_test_score  \\\n",
       "0                  1  {'model__alpha': 1}           0.627481   \n",
       "1                  2  {'model__alpha': 2}           0.592633   \n",
       "2                  3  {'model__alpha': 3}           0.565414   \n",
       "3                  4  {'model__alpha': 4}           0.560284   \n",
       "4                  5  {'model__alpha': 5}           0.558501   \n",
       "5                  6  {'model__alpha': 6}           0.554380   \n",
       "6                  7  {'model__alpha': 7}           0.547921   \n",
       "7                  8  {'model__alpha': 8}           0.539124   \n",
       "8                  9  {'model__alpha': 9}           0.527989   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  ...  \\\n",
       "0           0.663396           0.680098           0.636030  ...   \n",
       "1           0.629701           0.661870           0.599219  ...   \n",
       "2           0.615736           0.643024           0.570821  ...   \n",
       "3           0.599865           0.629426           0.562328  ...   \n",
       "4           0.591416           0.619986           0.555186  ...   \n",
       "5           0.581638           0.609292           0.546440  ...   \n",
       "6           0.570531           0.597345           0.536090  ...   \n",
       "7           0.558095           0.584145           0.524135  ...   \n",
       "8           0.544330           0.569691           0.510576  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.656434        0.021042                1            0.681996   \n",
       "1         0.625730        0.026458                2            0.647118   \n",
       "2         0.602155        0.029571                3            0.620352   \n",
       "3         0.589740        0.025870                4            0.610516   \n",
       "4         0.582313        0.023758                5            0.603441   \n",
       "5         0.574712        0.022391                6            0.594793   \n",
       "6         0.565581        0.021559                7            0.584573   \n",
       "7         0.554918        0.021271                8            0.572781   \n",
       "8         0.542725        0.021521                9            0.559417   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            0.671485            0.662360            0.670433   \n",
       "1            0.636664            0.627052            0.641162   \n",
       "2            0.614544            0.597967            0.615303   \n",
       "3            0.592324            0.582471            0.604355   \n",
       "4            0.584619            0.574655            0.596781   \n",
       "5            0.575201            0.565101            0.587524   \n",
       "6            0.564070            0.553811            0.576583   \n",
       "7            0.551227            0.540784            0.563960   \n",
       "8            0.536672            0.526020            0.549654   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0            0.664104          0.670076         0.006919  \n",
       "1            0.633921          0.637183         0.006754  \n",
       "2            0.614108          0.612455         0.007582  \n",
       "3            0.601269          0.598187         0.009807  \n",
       "4            0.594389          0.590777         0.010073  \n",
       "5            0.588586          0.582241         0.010668  \n",
       "6            0.581728          0.572153         0.011553  \n",
       "7            0.573814          0.560513         0.012765  \n",
       "8            0.564845          0.547322         0.014325  \n",
       "\n",
       "[9 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>1.08217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.031829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.018959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.000963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_model__alpha</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'model__alpha': 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.627481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.663396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>0.680098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>0.63603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>0.675166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.656434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.021042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>0.681996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>0.671485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>0.66236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>0.670433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>0.664104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>0.670076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.006919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0\n",
       "mean_fit_time                   1.08217\n",
       "std_fit_time                   0.031829\n",
       "mean_score_time                0.018959\n",
       "std_score_time                 0.000963\n",
       "param_model__alpha                    1\n",
       "params              {'model__alpha': 1}\n",
       "split0_test_score              0.627481\n",
       "split1_test_score              0.663396\n",
       "split2_test_score              0.680098\n",
       "split3_test_score               0.63603\n",
       "split4_test_score              0.675166\n",
       "mean_test_score                0.656434\n",
       "std_test_score                 0.021042\n",
       "rank_test_score                       1\n",
       "split0_train_score             0.681996\n",
       "split1_train_score             0.671485\n",
       "split2_train_score              0.66236\n",
       "split3_train_score             0.670433\n",
       "split4_train_score             0.664104\n",
       "mean_train_score               0.670076\n",
       "std_train_score                0.006919"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"best_model_regressor.csv\").head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from itertools import product\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoLayerFeedForward(nlayers, hiddens, dropouts, meta):\n",
    "    clf = Sequential()\n",
    "    X_shape_ = (meta[\"X_shape_\"][1],)\n",
    "\n",
    "    clf.add(Dense(hiddens[0], activation='relu', input_shape=X_shape_))\n",
    "    if dropouts[0] > 0:\n",
    "        clf.add(Dropout(dropouts[0]))\n",
    "    for i in range(1, nlayers):\n",
    "        clf.add(Dense(hiddens[i], activation='relu'))\n",
    "        if dropouts[i] > 0:\n",
    "            clf.add(Dropout(dropouts[i]))\n",
    "    clf.add(Dense(1))\n",
    "    return clf\n",
    "\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "\n",
    "mlp = KerasRegressor(\n",
    "    twoLayerFeedForward, \n",
    "    epochs=200, \n",
    "    loss=\"mse\",\n",
    "    callbacks=[callback],\n",
    "    validation_split=0.2,\n",
    "    nlayers=3,\n",
    "    hiddens=[2,2,2],\n",
    "    dropouts=[0.2, 0, 0],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_entries = [4, 8, 16, 32, 64]\n",
    "hidden_layers = list(product(valid_entries, repeat=6))\n",
    "dropouts = list(np.random.choice([0.0, 0.1, 0.2, 0.3], p=[0.4, 0.2, 0.2, 0.2], size=(2000, 6)))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(\n",
    "    [\n",
    "        ('scaler', PowerTransformer()),\n",
    "        (\"principal_components\", PCA(n_components=20, svd_solver = 'full')),\n",
    "        (\"mlp\", mlp)\n",
    "    ]\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"mlp__nlayers\": np.random.randint(1, 7, size=22),\n",
    "    \"mlp__hiddens\": hidden_layers,\n",
    "    \"mlp__dropouts\": dropouts,\n",
    "    \"mlp__optimizer__learning_rate\": [0.0001, 0.001, 0.005],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = RandomizedSearchCV(\n",
    "    model, \n",
    "    params, \n",
    "    refit=False, \n",
    "    cv=KFold(shuffle=True), \n",
    "    return_train_score=True,\n",
    "    scoring=\"neg_mean_absolute_error\", \n",
    "    n_iter=100, \n",
    "    verbose=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.  0.1], mlp__hiddens=(32, 32, 32, 4, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.895, test=-4.097) total time=  11.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.  0.1], mlp__hiddens=(32, 32, 32, 4, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.290, test=-5.258) total time=   8.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.  0.1], mlp__hiddens=(32, 32, 32, 4, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.647, test=-4.415) total time=   7.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.  0.1], mlp__hiddens=(32, 32, 32, 4, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.431, test=-4.496) total time=   8.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.  0.1], mlp__hiddens=(32, 32, 32, 4, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.756, test=-4.617) total time=   7.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 64, 16, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.106, test=-4.328) total time=   8.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 64, 16, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.901, test=-4.799) total time=   9.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 64, 16, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.251, test=-4.434) total time=   6.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 64, 16, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.818, test=-5.120) total time=   6.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 64, 16, 32, 4, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.674, test=-3.670) total time=   8.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.2 0.3], mlp__hiddens=(16, 8, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.887, test=-3.677) total time=   4.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.2 0.3], mlp__hiddens=(16, 8, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.297, test=-4.189) total time=   5.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.2 0.3], mlp__hiddens=(16, 8, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.230, test=-4.080) total time=   3.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.2 0.3], mlp__hiddens=(16, 8, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.545, test=-4.590) total time=   3.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.2 0.3], mlp__hiddens=(16, 8, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.671, test=-4.695) total time=   3.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 8, 32, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.166, test=-4.235) total time=   3.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 8, 32, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.546, test=-5.559) total time=   2.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 8, 32, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.159, test=-3.184) total time=   3.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 8, 32, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.007, test=-3.285) total time=   3.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 8, 32, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.171, test=-3.989) total time=   2.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.3 0.  0.3], mlp__hiddens=(8, 64, 4, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-17.054, test=-17.031) total time=  15.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.3 0.  0.3], mlp__hiddens=(8, 64, 4, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.346, test=-5.906) total time=  10.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.3 0.  0.3], mlp__hiddens=(8, 64, 4, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.017, test=-5.173) total time=  14.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.3 0.  0.3], mlp__hiddens=(8, 64, 4, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.475, test=-5.716) total time=  10.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.3 0.  0.3], mlp__hiddens=(8, 64, 4, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.876, test=-5.609) total time=  10.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.  0.2 0.2 0. ], mlp__hiddens=(8, 4, 16, 64, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.248, test=-3.238) total time=   3.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.  0.2 0.2 0. ], mlp__hiddens=(8, 4, 16, 64, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.003, test=-3.568) total time=   3.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.  0.2 0.2 0. ], mlp__hiddens=(8, 4, 16, 64, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.511, test=-3.554) total time=   2.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.  0.2 0.2 0. ], mlp__hiddens=(8, 4, 16, 64, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.715, test=-3.760) total time=   2.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.  0.2 0.2 0. ], mlp__hiddens=(8, 4, 16, 64, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.078, test=-3.160) total time=   3.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.  0.3 0.  0. ], mlp__hiddens=(64, 16, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.830, test=-3.958) total time=   9.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.  0.3 0.  0. ], mlp__hiddens=(64, 16, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.128, test=-4.891) total time=   6.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.  0.3 0.  0. ], mlp__hiddens=(64, 16, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.773, test=-3.945) total time=   9.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.  0.3 0.  0. ], mlp__hiddens=(64, 16, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.158, test=-4.538) total time=   5.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.  0.3 0.  0. ], mlp__hiddens=(64, 16, 16, 32, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.009, test=-3.870) total time=   7.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.2 0.1], mlp__hiddens=(4, 8, 64, 32, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.829, test=-3.457) total time=   2.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.2 0.1], mlp__hiddens=(4, 8, 64, 32, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.139, test=-3.681) total time=   3.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.2 0.1], mlp__hiddens=(4, 8, 64, 32, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.223, test=-3.506) total time=   3.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.2 0.1], mlp__hiddens=(4, 8, 64, 32, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.332, test=-3.545) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.  0.1 0.2 0.1], mlp__hiddens=(4, 8, 64, 32, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.675, test=-3.372) total time=   3.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 32, 16, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.865, test=-3.210) total time=   5.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 32, 16, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.929, test=-3.981) total time=   5.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 32, 16, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.900, test=-3.303) total time=   5.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 32, 16, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.126, test=-3.656) total time=   4.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.3 0.1], mlp__hiddens=(32, 32, 16, 4, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.848, test=-3.370) total time=   5.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 64, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.813, test=-4.870) total time=   6.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 64, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.219, test=-6.397) total time=   7.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 64, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.747, test=-4.765) total time=   6.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 64, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.915, test=-5.291) total time=   5.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 64, 64, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.176, test=-4.633) total time=   4.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(16, 32, 4, 8, 8, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.206, test=-3.902) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(16, 32, 4, 8, 8, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-5.732, test=-6.546) total time=  15.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(16, 32, 4, 8, 8, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.528, test=-3.508) total time=   2.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(16, 32, 4, 8, 8, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.010, test=-3.258) total time=   4.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.  0.2], mlp__hiddens=(16, 32, 4, 8, 8, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.538, test=-3.595) total time=   3.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.2], mlp__hiddens=(32, 32, 64, 64, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.898, test=-3.257) total time=   3.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.2], mlp__hiddens=(32, 32, 64, 64, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.651, test=-3.685) total time=   3.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.2], mlp__hiddens=(32, 32, 64, 64, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.694, test=-3.668) total time=   2.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.2], mlp__hiddens=(32, 32, 64, 64, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.038, test=-3.688) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.  0.1 0.2], mlp__hiddens=(32, 32, 64, 64, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.714, test=-3.914) total time=   2.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.  0.2 0.  0.3], mlp__hiddens=(4, 32, 4, 32, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.618, test=-3.653) total time=  11.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.  0.2 0.  0.3], mlp__hiddens=(4, 32, 4, 32, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.867, test=-4.711) total time=  13.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.  0.2 0.  0.3], mlp__hiddens=(4, 32, 4, 32, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-5.133, test=-5.260) total time=  13.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.  0.2 0.  0.3], mlp__hiddens=(4, 32, 4, 32, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-4.169, test=-4.511) total time=  12.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.  0.2 0.  0.3], mlp__hiddens=(4, 32, 4, 32, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.775, test=-4.013) total time=  13.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 32, 4, 64, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.276, test=-3.360) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 32, 4, 64, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.058, test=-3.574) total time=   3.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 32, 4, 64, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.008, test=-3.195) total time=   2.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 32, 4, 64, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.996, test=-3.500) total time=   3.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 32, 4, 64, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.942, test=-3.068) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.2 0.2 0.  0.1], mlp__hiddens=(8, 32, 4, 4, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.702, test=-3.809) total time=   4.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.2 0.2 0.  0.1], mlp__hiddens=(8, 32, 4, 4, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.101, test=-6.175) total time=   4.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.2 0.2 0.  0.1], mlp__hiddens=(8, 32, 4, 4, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.410, test=-4.427) total time=   4.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.2 0.2 0.  0.1], mlp__hiddens=(8, 32, 4, 4, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.342, test=-4.601) total time=   4.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.2 0.2 0.  0.1], mlp__hiddens=(8, 32, 4, 4, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.577, test=-4.535) total time=   3.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.2 0.2], mlp__hiddens=(8, 8, 32, 4, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.994, test=-7.107) total time=  13.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.2 0.2], mlp__hiddens=(8, 8, 32, 4, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.821, test=-9.865) total time=  14.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.2 0.2], mlp__hiddens=(8, 8, 32, 4, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.220, test=-10.535) total time=  13.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.2 0.2], mlp__hiddens=(8, 8, 32, 4, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-12.070, test=-11.303) total time=  14.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.2 0.2], mlp__hiddens=(8, 8, 32, 4, 64, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.478, test=-7.634) total time=  14.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 32, 4, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.287, test=-3.661) total time=   3.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 32, 4, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.425, test=-4.291) total time=   2.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 32, 4, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.341, test=-3.476) total time=   3.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 32, 4, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.954, test=-3.536) total time=   3.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 32, 4, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.873, test=-3.406) total time=   3.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.1 0.1 0.2 0. ], mlp__hiddens=(16, 32, 32, 64, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.931, test=-3.254) total time=  11.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.1 0.1 0.2 0. ], mlp__hiddens=(16, 32, 32, 64, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.978, test=-3.862) total time=   6.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.1 0.1 0.2 0. ], mlp__hiddens=(16, 32, 32, 64, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.033, test=-3.484) total time=   6.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.1 0.1 0.2 0. ], mlp__hiddens=(16, 32, 32, 64, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.975, test=-3.488) total time=   8.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.1 0.1 0.2 0. ], mlp__hiddens=(16, 32, 32, 64, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.174, test=-3.730) total time=   7.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.  0.  0.3 0. ], mlp__hiddens=(8, 64, 64, 64, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.618, test=-3.500) total time=   3.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.  0.  0.3 0. ], mlp__hiddens=(8, 64, 64, 64, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.411, test=-4.085) total time=   3.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.  0.  0.3 0. ], mlp__hiddens=(8, 64, 64, 64, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.331, test=-3.366) total time=   3.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.  0.  0.3 0. ], mlp__hiddens=(8, 64, 64, 64, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.960, test=-3.460) total time=   4.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.  0.  0.3 0. ], mlp__hiddens=(8, 64, 64, 64, 8, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.993, test=-4.457) total time=   3.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 32, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.366, test=-3.618) total time=  13.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 32, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.331, test=-4.177) total time=  14.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 32, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.560, test=-3.701) total time=  13.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 32, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.436, test=-3.981) total time=  14.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.  0.  0. ], mlp__hiddens=(64, 16, 32, 32, 16, 16), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.761, test=-3.801) total time=  16.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.3 0. ], mlp__hiddens=(8, 8, 8, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.053, test=-3.973) total time=   4.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.3 0. ], mlp__hiddens=(8, 8, 8, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.263, test=-4.021) total time=   4.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.3 0. ], mlp__hiddens=(8, 8, 8, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.995, test=-3.952) total time=   3.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.3 0. ], mlp__hiddens=(8, 8, 8, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-5.678, test=-5.605) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.3 0. ], mlp__hiddens=(8, 8, 8, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.290, test=-3.323) total time=   4.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.  0.2], mlp__hiddens=(8, 64, 4, 16, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.870, test=-11.605) total time=  13.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.  0.2], mlp__hiddens=(8, 64, 4, 16, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.238, test=-9.878) total time=  13.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.  0.2], mlp__hiddens=(8, 64, 4, 16, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.195, test=-10.756) total time=  13.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.  0.2], mlp__hiddens=(8, 64, 4, 16, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.749, test=-7.117) total time=  13.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.  0.  0.  0.2], mlp__hiddens=(8, 64, 4, 16, 32, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.933, test=-7.686) total time=  13.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.1 0. ], mlp__hiddens=(64, 4, 4, 16, 32, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.556, test=-4.507) total time=   5.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.1 0. ], mlp__hiddens=(64, 4, 4, 16, 32, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.400, test=-5.323) total time=   3.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.1 0. ], mlp__hiddens=(64, 4, 4, 16, 32, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.058, test=-3.931) total time=   4.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.1 0. ], mlp__hiddens=(64, 4, 4, 16, 32, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.961, test=-6.182) total time=   3.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.1 0. ], mlp__hiddens=(64, 4, 4, 16, 32, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.658, test=-5.452) total time=   3.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.3 0.  0.1 0.3], mlp__hiddens=(4, 8, 8, 4, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.517, test=-4.442) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.3 0.  0.1 0.3], mlp__hiddens=(4, 8, 8, 4, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.795, test=-4.441) total time=   3.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.3 0.  0.1 0.3], mlp__hiddens=(4, 8, 8, 4, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.723, test=-3.622) total time=   3.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.3 0.  0.1 0.3], mlp__hiddens=(4, 8, 8, 4, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.455, test=-3.835) total time=   3.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.3 0.  0.1 0.3], mlp__hiddens=(4, 8, 8, 4, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.601, test=-3.315) total time=   3.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.1 0.1 0.3 0.2], mlp__hiddens=(64, 4, 4, 64, 32, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.015, test=-3.027) total time=   4.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.1 0.1 0.3 0.2], mlp__hiddens=(64, 4, 4, 64, 32, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.359, test=-4.080) total time=   3.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.1 0.1 0.3 0.2], mlp__hiddens=(64, 4, 4, 64, 32, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.816, test=-3.691) total time=   3.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.1 0.1 0.3 0.2], mlp__hiddens=(64, 4, 4, 64, 32, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.146, test=-4.555) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.1 0.1 0.3 0.2], mlp__hiddens=(64, 4, 4, 64, 32, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.375, test=-3.470) total time=   4.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.1 0.3 0.3 0.1], mlp__hiddens=(32, 64, 8, 16, 8, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.358, test=-3.462) total time=  12.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.1 0.3 0.3 0.1], mlp__hiddens=(32, 64, 8, 16, 8, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.699, test=-4.243) total time=   7.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.1 0.3 0.3 0.1], mlp__hiddens=(32, 64, 8, 16, 8, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.577, test=-3.677) total time=  10.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.1 0.3 0.3 0.1], mlp__hiddens=(32, 64, 8, 16, 8, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.403, test=-3.647) total time=  12.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.1 0.3 0.3 0.1], mlp__hiddens=(32, 64, 8, 16, 8, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.438, test=-3.891) total time=  12.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.1 0.2 0.  0. ], mlp__hiddens=(64, 64, 4, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.782, test=-3.879) total time=   3.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.1 0.2 0.  0. ], mlp__hiddens=(64, 64, 4, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.154, test=-3.989) total time=   3.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.1 0.2 0.  0. ], mlp__hiddens=(64, 64, 4, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.387, test=-3.747) total time=   3.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.1 0.2 0.  0. ], mlp__hiddens=(64, 64, 4, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.296, test=-4.810) total time=   2.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.1 0.2 0.  0. ], mlp__hiddens=(64, 64, 4, 32, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.732, test=-3.726) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 8, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.640, test=-4.034) total time=   2.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 8, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.050, test=-3.703) total time=   2.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 8, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.817, test=-2.963) total time=   3.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 8, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.424, test=-3.967) total time=   2.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.1 0.  0.  0.2], mlp__hiddens=(64, 4, 16, 8, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.018, test=-3.026) total time=   3.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 16, 64, 16, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-10.086, test=-10.090) total time=  13.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 16, 64, 16, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.251, test=-11.927) total time=  14.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 16, 64, 16, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-12.850, test=-12.809) total time=  13.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 16, 64, 16, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-11.670, test=-11.464) total time=  13.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 16, 64, 16, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.319, test=-8.556) total time=  13.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 16, 8, 32, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.836, test=-4.823) total time=  16.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 16, 8, 32, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.896, test=-5.611) total time=  13.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 16, 8, 32, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.848, test=-4.882) total time=  16.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 16, 8, 32, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.879, test=-5.901) total time=  12.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 16, 8, 32, 32, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.829, test=-4.435) total time=  16.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(32, 64, 16, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.258, test=-4.142) total time=   3.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(32, 64, 16, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.186, test=-5.306) total time=   3.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(32, 64, 16, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.816, test=-3.906) total time=   3.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(32, 64, 16, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-5.644, test=-5.648) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(32, 64, 16, 32, 4, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-5.861, test=-5.955) total time=   3.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 4, 32, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.191, test=-3.004) total time=   7.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 4, 32, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.204, test=-3.795) total time=   5.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 4, 32, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.387, test=-3.614) total time=   4.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 4, 32, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.343, test=-3.305) total time=   5.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.  0.1 0.1], mlp__hiddens=(4, 16, 4, 32, 4, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-2.948, test=-3.224) total time=   6.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.3 0.1], mlp__hiddens=(64, 64, 16, 8, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.379, test=-5.241) total time=   6.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.3 0.1], mlp__hiddens=(64, 64, 16, 8, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.076, test=-6.129) total time=   6.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.3 0.1], mlp__hiddens=(64, 64, 16, 8, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.928, test=-4.977) total time=   7.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.3 0.1], mlp__hiddens=(64, 64, 16, 8, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.643, test=-6.043) total time=   6.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.3 0.1], mlp__hiddens=(64, 64, 16, 8, 16, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.740, test=-5.541) total time=   6.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 64, 64, 16, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.756, test=-4.650) total time=   5.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 64, 64, 16, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.142, test=-5.190) total time=   7.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 64, 64, 16, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.396, test=-4.463) total time=   7.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 64, 64, 16, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.652, test=-4.764) total time=   5.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 64, 64, 16, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.879, test=-3.757) total time=   9.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(64, 64, 8, 16, 64, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.373, test=-3.624) total time=   2.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(64, 64, 8, 16, 64, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-2.977, test=-3.911) total time=   3.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(64, 64, 8, 16, 64, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.462, test=-3.452) total time=   2.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(64, 64, 8, 16, 64, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.288, test=-3.707) total time=   3.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(64, 64, 8, 16, 64, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.040, test=-3.491) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.3 0.  0.1 0.3], mlp__hiddens=(8, 4, 64, 16, 4, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.050, test=-3.180) total time=   5.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.3 0.  0.1 0.3], mlp__hiddens=(8, 4, 64, 16, 4, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.369, test=-4.176) total time=   3.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.3 0.  0.1 0.3], mlp__hiddens=(8, 4, 64, 16, 4, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-4.005, test=-4.140) total time=   3.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.3 0.  0.1 0.3], mlp__hiddens=(8, 4, 64, 16, 4, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.411, test=-3.816) total time=   3.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.3 0.  0.1 0.3], mlp__hiddens=(8, 4, 64, 16, 4, 64), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.361, test=-3.742) total time=   4.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.3 0.3 0.  0.1], mlp__hiddens=(32, 16, 32, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.663, test=-3.781) total time=   3.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.3 0.3 0.  0.1], mlp__hiddens=(32, 16, 32, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.604, test=-4.611) total time=   3.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.3 0.3 0.  0.1], mlp__hiddens=(32, 16, 32, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.809, test=-3.753) total time=   3.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.3 0.3 0.  0.1], mlp__hiddens=(32, 16, 32, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.238, test=-4.455) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.3 0.3 0.  0.1], mlp__hiddens=(32, 16, 32, 64, 4, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.906, test=-4.042) total time=   4.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.1 0.2 0.  0.3 0. ], mlp__hiddens=(4, 64, 32, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.113, test=-3.078) total time=   5.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.1 0.2 0.  0.3 0. ], mlp__hiddens=(4, 64, 32, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.445, test=-4.308) total time=   3.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.1 0.2 0.  0.3 0. ], mlp__hiddens=(4, 64, 32, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.464, test=-3.672) total time=   4.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.1 0.2 0.  0.3 0. ], mlp__hiddens=(4, 64, 32, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.251, test=-3.525) total time=   5.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.1 0.2 0.  0.3 0. ], mlp__hiddens=(4, 64, 32, 4, 4, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.178, test=-3.139) total time=   4.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.2 0.  0.3 0.2], mlp__hiddens=(64, 32, 16, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.058, test=-3.287) total time=   3.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.2 0.  0.3 0.2], mlp__hiddens=(64, 32, 16, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.174, test=-4.065) total time=   3.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.2 0.  0.3 0.2], mlp__hiddens=(64, 32, 16, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.750, test=-3.803) total time=   2.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.2 0.  0.3 0.2], mlp__hiddens=(64, 32, 16, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.284, test=-3.827) total time=   2.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.2 0.  0.3 0.2], mlp__hiddens=(64, 32, 16, 32, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.020, test=-2.979) total time=   3.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.3 0.2], mlp__hiddens=(4, 8, 64, 16, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.974, test=-13.486) total time=  23.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.3 0.2], mlp__hiddens=(4, 8, 64, 16, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-13.039, test=-14.773) total time=  17.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.3 0.2], mlp__hiddens=(4, 8, 64, 16, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-10.985, test=-10.691) total time=  13.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.3 0.2], mlp__hiddens=(4, 8, 64, 16, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-10.275, test=-10.221) total time=  13.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.  0.  0.3 0.2], mlp__hiddens=(4, 8, 64, 16, 16, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-15.973, test=-15.269) total time=  13.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(16, 64, 4, 4, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.939, test=-3.908) total time=  11.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(16, 64, 4, 4, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.111, test=-4.971) total time=   9.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(16, 64, 4, 4, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.650, test=-4.497) total time=   9.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(16, 64, 4, 4, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.021, test=-4.187) total time=  11.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.  0.  0.3 0.1], mlp__hiddens=(16, 64, 4, 4, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.862, test=-3.643) total time=  11.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.2 0.2 0.3 0.1 0.2], mlp__hiddens=(16, 4, 64, 16, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-8.036, test=-7.879) total time=  13.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.2 0.2 0.3 0.1 0.2], mlp__hiddens=(16, 4, 64, 16, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.782, test=-9.166) total time=  13.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.2 0.2 0.3 0.1 0.2], mlp__hiddens=(16, 4, 64, 16, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.552, test=-5.719) total time=  13.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.2 0.2 0.3 0.1 0.2], mlp__hiddens=(16, 4, 64, 16, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.934, test=-5.192) total time=  13.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.2 0.2 0.3 0.1 0.2], mlp__hiddens=(16, 4, 64, 16, 16, 4), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.952, test=-5.577) total time=  13.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 64, 4, 16, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.900, test=-3.108) total time=   3.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 64, 4, 16, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.110, test=-3.514) total time=   3.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 64, 4, 16, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.890, test=-3.247) total time=   5.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 64, 4, 16, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.144, test=-3.413) total time=   3.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 64, 4, 16, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-6.278, test=-6.177) total time=   3.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0. ], mlp__hiddens=(32, 4, 32, 8, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.625, test=-3.580) total time=   2.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0. ], mlp__hiddens=(32, 4, 32, 8, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.823, test=-4.400) total time=   2.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0. ], mlp__hiddens=(32, 4, 32, 8, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.361, test=-3.274) total time=   2.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0. ], mlp__hiddens=(32, 4, 32, 8, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.679, test=-3.006) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.  0.  0.  0. ], mlp__hiddens=(32, 4, 32, 8, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.958, test=-3.030) total time=   3.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 64, 32, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.272, test=-3.185) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 64, 32, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.955, test=-3.542) total time=   3.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 64, 32, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.285, test=-3.476) total time=   2.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 64, 32, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.516, test=-3.637) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.  0.3 0.  0.3], mlp__hiddens=(4, 64, 32, 64, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.227, test=-3.406) total time=   2.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 64, 64, 8, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.024, test=-3.444) total time=   3.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 64, 64, 8, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.927, test=-3.853) total time=   3.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 64, 64, 8, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.956, test=-3.582) total time=   3.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 64, 64, 8, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.209, test=-3.670) total time=   4.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.  0.3 0.2], mlp__hiddens=(16, 64, 64, 8, 4, 8), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.907, test=-3.490) total time=   3.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 64, 16, 4, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.434, test=-3.417) total time=   3.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 64, 16, 4, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.768, test=-4.432) total time=   2.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 64, 16, 4, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.445, test=-3.405) total time=   3.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 64, 16, 4, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.331, test=-3.266) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.3 0.  0.1 0.2], mlp__hiddens=(4, 64, 16, 4, 64, 4), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.654, test=-4.202) total time=   2.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.  0.3 0.3 0. ], mlp__hiddens=(8, 16, 16, 64, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.050, test=-2.980) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.  0.3 0.3 0. ], mlp__hiddens=(8, 16, 16, 64, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.067, test=-3.809) total time=   3.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.  0.3 0.3 0. ], mlp__hiddens=(8, 16, 16, 64, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.244, test=-3.380) total time=   2.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.  0.3 0.3 0. ], mlp__hiddens=(8, 16, 16, 64, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.062, test=-3.399) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.  0.3 0.3 0. ], mlp__hiddens=(8, 16, 16, 64, 8, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.361, test=-3.377) total time=   2.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0. ], mlp__hiddens=(64, 16, 32, 32, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.701, test=-3.701) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0. ], mlp__hiddens=(64, 16, 32, 32, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.604, test=-4.480) total time=   3.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0. ], mlp__hiddens=(64, 16, 32, 32, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.363, test=-3.380) total time=   6.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0. ], mlp__hiddens=(64, 16, 32, 32, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.933, test=-4.272) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.2 0.  0. ], mlp__hiddens=(64, 16, 32, 32, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.763, test=-3.929) total time=   3.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.  0.2], mlp__hiddens=(4, 8, 32, 64, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.331, test=-3.675) total time=   4.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.  0.2], mlp__hiddens=(4, 8, 32, 64, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.343, test=-3.740) total time=   7.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.  0.2], mlp__hiddens=(4, 8, 32, 64, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.505, test=-3.846) total time=   8.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.  0.2], mlp__hiddens=(4, 8, 32, 64, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.012, test=-3.305) total time=   6.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.2 0.1 0.  0.2], mlp__hiddens=(4, 8, 32, 64, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.219, test=-3.869) total time=   4.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.1 0.1], mlp__hiddens=(64, 64, 8, 4, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.152, test=-4.231) total time=   8.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.1 0.1], mlp__hiddens=(64, 64, 8, 4, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-16.740, test=-18.290) total time=  21.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.1 0.1], mlp__hiddens=(64, 64, 8, 4, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.198, test=-4.106) total time=   7.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.1 0.1], mlp__hiddens=(64, 64, 8, 4, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.230, test=-4.682) total time=   7.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.  0.1 0.1 0.1], mlp__hiddens=(64, 64, 8, 4, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.980, test=-3.884) total time=  12.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0.1], mlp__hiddens=(16, 8, 16, 64, 4, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.297, test=-3.382) total time=   8.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0.1], mlp__hiddens=(16, 8, 16, 64, 4, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.167, test=-4.067) total time=   6.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0.1], mlp__hiddens=(16, 8, 16, 64, 4, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.027, test=-3.269) total time=   7.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0.1], mlp__hiddens=(16, 8, 16, 64, 4, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.969, test=-3.463) total time=   8.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0.1], mlp__hiddens=(16, 8, 16, 64, 4, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.500, test=-3.669) total time=   5.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.  0.3 0.3 0. ], mlp__hiddens=(8, 8, 16, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.578, test=-3.319) total time=   5.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.  0.3 0.3 0. ], mlp__hiddens=(8, 8, 16, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.723, test=-5.592) total time=   5.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.  0.3 0.3 0. ], mlp__hiddens=(8, 8, 16, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-5.099, test=-5.299) total time=   3.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.  0.3 0.3 0. ], mlp__hiddens=(8, 8, 16, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.904, test=-3.854) total time=   5.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.  0.3 0.3 0. ], mlp__hiddens=(8, 8, 16, 8, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.295, test=-4.366) total time=   4.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.  0.1 0. ], mlp__hiddens=(32, 64, 8, 4, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.222, test=-4.380) total time=   7.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.  0.1 0. ], mlp__hiddens=(32, 64, 8, 4, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.381, test=-5.273) total time=   6.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.  0.1 0. ], mlp__hiddens=(32, 64, 8, 4, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.262, test=-4.205) total time=   7.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.  0.1 0. ], mlp__hiddens=(32, 64, 8, 4, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.854, test=-5.265) total time=   5.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.  0.1 0. ], mlp__hiddens=(32, 64, 8, 4, 32, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.898, test=-4.097) total time=   9.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0. ], mlp__hiddens=(4, 64, 64, 64, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.112, test=-3.358) total time=  13.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0. ], mlp__hiddens=(4, 64, 64, 64, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.551, test=-3.798) total time=  12.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0. ], mlp__hiddens=(4, 64, 64, 64, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.292, test=-3.401) total time=  14.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0. ], mlp__hiddens=(4, 64, 64, 64, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.910, test=-3.870) total time=  12.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.  0.  0. ], mlp__hiddens=(4, 64, 64, 64, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.375, test=-3.282) total time=  15.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0.2], mlp__hiddens=(4, 16, 32, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.401, test=-4.258) total time=   7.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0.2], mlp__hiddens=(4, 16, 32, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.095, test=-4.889) total time=   5.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0.2], mlp__hiddens=(4, 16, 32, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.808, test=-4.979) total time=   4.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0.2], mlp__hiddens=(4, 16, 32, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.377, test=-5.626) total time=   4.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0.2], mlp__hiddens=(4, 16, 32, 4, 4, 8), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.277, test=-4.023) total time=   4.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.2 0. ], mlp__hiddens=(16, 16, 16, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.822, test=-3.992) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.2 0. ], mlp__hiddens=(16, 16, 16, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.604, test=-4.085) total time=   2.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.2 0. ], mlp__hiddens=(16, 16, 16, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.541, test=-3.730) total time=   2.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.2 0. ], mlp__hiddens=(16, 16, 16, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.152, test=-3.317) total time=   3.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.2 0.3 0.2 0. ], mlp__hiddens=(16, 16, 16, 8, 8, 4), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.832, test=-4.024) total time=   2.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 16, 32, 16, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.734, test=-3.807) total time=   3.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 16, 32, 16, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.169, test=-4.165) total time=   3.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 16, 32, 16, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.355, test=-3.362) total time=   3.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 16, 32, 16, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.826, test=-4.112) total time=   2.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.3 0.  0. ], mlp__hiddens=(32, 16, 32, 16, 8, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.474, test=-3.624) total time=   3.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.3 0.1 0.2 0. ], mlp__hiddens=(4, 16, 16, 8, 8, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.759, test=-3.740) total time=   3.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.3 0.1 0.2 0. ], mlp__hiddens=(4, 16, 16, 8, 8, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.098, test=-4.890) total time=   3.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.3 0.1 0.2 0. ], mlp__hiddens=(4, 16, 16, 8, 8, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.616, test=-4.577) total time=   3.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.3 0.1 0.2 0. ], mlp__hiddens=(4, 16, 16, 8, 8, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.692, test=-3.701) total time=   3.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.3 0.1 0.2 0. ], mlp__hiddens=(4, 16, 16, 8, 8, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.785, test=-3.784) total time=   3.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.  0.  0.  0.2], mlp__hiddens=(16, 16, 64, 32, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.662, test=-3.808) total time=  11.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.  0.  0.  0.2], mlp__hiddens=(16, 16, 64, 32, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.469, test=-4.351) total time=  14.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.  0.  0.  0.2], mlp__hiddens=(16, 16, 64, 32, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.861, test=-3.623) total time=  10.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.  0.  0.  0.2], mlp__hiddens=(16, 16, 64, 32, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.870, test=-4.456) total time=  11.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.  0.  0.  0.2], mlp__hiddens=(16, 16, 64, 32, 8, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.560, test=-3.758) total time=  12.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.3 0.  0.2 0.3], mlp__hiddens=(64, 4, 16, 64, 32, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.337, test=-3.726) total time=   2.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.3 0.  0.2 0.3], mlp__hiddens=(64, 4, 16, 64, 32, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.995, test=-4.067) total time=   2.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.3 0.  0.2 0.3], mlp__hiddens=(64, 4, 16, 64, 32, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.335, test=-3.455) total time=   2.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.3 0.  0.2 0.3], mlp__hiddens=(64, 4, 16, 64, 32, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.429, test=-3.988) total time=   3.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.3 0.  0.2 0.3], mlp__hiddens=(64, 4, 16, 64, 32, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.686, test=-3.359) total time=   2.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.2 0. ], mlp__hiddens=(64, 32, 32, 16, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.871, test=-3.890) total time=   3.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.2 0. ], mlp__hiddens=(64, 32, 32, 16, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.871, test=-5.915) total time=   3.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.2 0. ], mlp__hiddens=(64, 32, 32, 16, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.533, test=-4.765) total time=   2.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.2 0. ], mlp__hiddens=(64, 32, 32, 16, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.585, test=-5.065) total time=   2.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.3 0.2 0.2 0. ], mlp__hiddens=(64, 32, 32, 16, 32, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.604, test=-4.710) total time=   3.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.2 0.3], mlp__hiddens=(32, 8, 4, 64, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.346, test=-5.405) total time=   8.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.2 0.3], mlp__hiddens=(32, 8, 4, 64, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.381, test=-6.030) total time=   7.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.2 0.3], mlp__hiddens=(32, 8, 4, 64, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.276, test=-5.121) total time=   8.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.2 0.3], mlp__hiddens=(32, 8, 4, 64, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.070, test=-5.419) total time=   9.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.1 0.3 0.2 0.3], mlp__hiddens=(32, 8, 4, 64, 4, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.890, test=-4.904) total time=  12.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.1 0.2 0.  0.  0. ], mlp__hiddens=(16, 16, 4, 16, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.457, test=-4.535) total time=   3.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.1 0.2 0.  0.  0. ], mlp__hiddens=(16, 16, 4, 16, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.286, test=-5.191) total time=   4.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.1 0.2 0.  0.  0. ], mlp__hiddens=(16, 16, 4, 16, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.661, test=-5.887) total time=   3.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.1 0.2 0.  0.  0. ], mlp__hiddens=(16, 16, 4, 16, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.709, test=-4.877) total time=   3.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.1 0.2 0.  0.  0. ], mlp__hiddens=(16, 16, 4, 16, 4, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.696, test=-4.805) total time=   3.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.1 0.  0.  0. ], mlp__hiddens=(64, 4, 32, 8, 64, 4), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.909, test=-4.101) total time=   3.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.1 0.  0.  0. ], mlp__hiddens=(64, 4, 32, 8, 64, 4), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.401, test=-4.598) total time=   3.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.1 0.  0.  0. ], mlp__hiddens=(64, 4, 32, 8, 64, 4), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-4.098, test=-4.520) total time=   4.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.1 0.  0.  0. ], mlp__hiddens=(64, 4, 32, 8, 64, 4), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.768, test=-4.165) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.1 0.  0.  0. ], mlp__hiddens=(64, 4, 32, 8, 64, 4), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.590, test=-3.774) total time=   3.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0. ], mlp__hiddens=(16, 4, 32, 4, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.937, test=-5.041) total time=   3.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0. ], mlp__hiddens=(16, 4, 32, 4, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-2.969, test=-3.991) total time=   4.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0. ], mlp__hiddens=(16, 4, 32, 4, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.005, test=-4.214) total time=   4.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0. ], mlp__hiddens=(16, 4, 32, 4, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.044, test=-4.232) total time=   3.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.  0.  0.2 0. ], mlp__hiddens=(16, 4, 32, 4, 4, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-4.402, test=-4.488) total time=   4.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.2 0.  0.  0.2], mlp__hiddens=(32, 32, 64, 16, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.472, test=-3.609) total time=   3.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.2 0.  0.  0.2], mlp__hiddens=(32, 32, 64, 16, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.481, test=-3.958) total time=   3.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.2 0.  0.  0.2], mlp__hiddens=(32, 32, 64, 16, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.351, test=-3.407) total time=   3.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.2 0.  0.  0.2], mlp__hiddens=(32, 32, 64, 16, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-3.749, test=-4.234) total time=   2.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.2 0.  0.  0.2], mlp__hiddens=(32, 32, 64, 16, 64, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-2.756, test=-3.262) total time=   4.2s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 8, 4, 64, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.238, test=-4.388) total time=   7.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 8, 4, 64, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.482, test=-5.440) total time=   7.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 8, 4, 64, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.406, test=-4.360) total time=   8.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 8, 4, 64, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.352, test=-4.764) total time=   8.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 8, 4, 64, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.224, test=-4.926) total time=   5.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.  0.  0.  0. ], mlp__hiddens=(8, 16, 64, 8, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.161, test=-3.615) total time=   2.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.  0.  0.  0. ], mlp__hiddens=(8, 16, 64, 8, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.108, test=-3.923) total time=   2.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.  0.  0.  0. ], mlp__hiddens=(8, 16, 64, 8, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.856, test=-3.245) total time=   3.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.  0.  0.  0. ], mlp__hiddens=(8, 16, 64, 8, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.507, test=-3.548) total time=   2.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.  0.  0.  0. ], mlp__hiddens=(8, 16, 64, 8, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.911, test=-3.133) total time=   3.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.  0.2], mlp__hiddens=(64, 4, 4, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.533, test=-3.886) total time=  13.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.  0.2], mlp__hiddens=(64, 4, 4, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.362, test=-4.181) total time=  13.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.  0.2], mlp__hiddens=(64, 4, 4, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.623, test=-3.547) total time=  13.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.  0.2], mlp__hiddens=(64, 4, 4, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.499, test=-3.934) total time=  13.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.  0.3 0.  0.2], mlp__hiddens=(64, 4, 4, 64, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.604, test=-3.790) total time=  13.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.3 0.3], mlp__hiddens=(16, 4, 64, 32, 64, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.202, test=-3.533) total time=   2.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.3 0.3], mlp__hiddens=(16, 4, 64, 32, 64, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.918, test=-3.778) total time=   3.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.3 0.3], mlp__hiddens=(16, 4, 64, 32, 64, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.892, test=-3.255) total time=   3.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.3 0.3], mlp__hiddens=(16, 4, 64, 32, 64, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.319, test=-3.698) total time=   2.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.3 0.3], mlp__hiddens=(16, 4, 64, 32, 64, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.866, test=-3.454) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-7.001, test=-6.989) total time=   6.8s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.779, test=-7.093) total time=  15.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.328, test=-6.046) total time=  17.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.333, test=-6.490) total time=   9.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.  0.1], mlp__hiddens=(8, 32, 4, 32, 8, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.660, test=-5.637) total time=  10.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.2 0. ], mlp__hiddens=(8, 4, 64, 4, 4, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.947, test=-3.328) total time=   4.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.2 0. ], mlp__hiddens=(8, 4, 64, 4, 4, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.976, test=-3.710) total time=   3.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.2 0. ], mlp__hiddens=(8, 4, 64, 4, 4, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.354, test=-3.565) total time=   3.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.2 0. ], mlp__hiddens=(8, 4, 64, 4, 4, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-3.380, test=-3.871) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.  0.3 0.2 0. ], mlp__hiddens=(8, 4, 64, 4, 4, 32), mlp__nlayers=2, mlp__optimizer__learning_rate=0.005;, score=(train=-2.916, test=-3.454) total time=   3.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0. ], mlp__hiddens=(8, 32, 16, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.147, test=-3.417) total time=   3.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0. ], mlp__hiddens=(8, 32, 16, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.916, test=-3.901) total time=   4.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0. ], mlp__hiddens=(8, 32, 16, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.185, test=-3.585) total time=   3.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0. ], mlp__hiddens=(8, 32, 16, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.966, test=-3.692) total time=   3.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.1 0.2 0.  0. ], mlp__hiddens=(8, 32, 16, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.358, test=-3.782) total time=   3.1s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.1 0. ], mlp__hiddens=(8, 4, 4, 8, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.924, test=-9.602) total time=  13.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.1 0. ], mlp__hiddens=(8, 4, 4, 8, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.321, test=-7.022) total time=  13.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.1 0. ], mlp__hiddens=(8, 4, 4, 8, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.522, test=-8.680) total time=  13.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.1 0. ], mlp__hiddens=(8, 4, 4, 8, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-12.844, test=-12.677) total time=  13.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.2 0.  0.  0.1 0. ], mlp__hiddens=(8, 4, 4, 8, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-9.546, test=-9.544) total time=  13.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 8, 32, 16, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.701, test=-5.163) total time=  15.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 8, 32, 16, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.791, test=-6.699) total time=  15.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 8, 32, 16, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.725, test=-4.963) total time=  15.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 8, 32, 16, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.300, test=-4.246) total time=  14.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.1 0.  0.1 0.  0.1], mlp__hiddens=(4, 8, 32, 16, 8, 64), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.247, test=-3.848) total time=  14.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.1 0. ], mlp__hiddens=(32, 16, 32, 4, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.103, test=-3.215) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.1 0. ], mlp__hiddens=(32, 16, 32, 4, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.099, test=-4.002) total time=   2.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.1 0. ], mlp__hiddens=(32, 16, 32, 4, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.184, test=-3.239) total time=   2.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.1 0. ], mlp__hiddens=(32, 16, 32, 4, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-3.596, test=-4.060) total time=   2.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.  0.1 0.1 0. ], mlp__hiddens=(32, 16, 32, 4, 32, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.005;, score=(train=-2.960, test=-3.284) total time=   2.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.2], mlp__hiddens=(4, 8, 32, 16, 8, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.220, test=-3.120) total time=   5.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.2], mlp__hiddens=(4, 8, 32, 16, 8, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.050, test=-3.749) total time=   4.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.2], mlp__hiddens=(4, 8, 32, 16, 8, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.270, test=-3.336) total time=   4.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.2], mlp__hiddens=(4, 8, 32, 16, 8, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.171, test=-3.258) total time=   4.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.3 0.  0.  0.2], mlp__hiddens=(4, 8, 32, 16, 8, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.001;, score=(train=-3.475, test=-3.585) total time=   4.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.2 0.  0.2 0.1], mlp__hiddens=(64, 8, 8, 32, 64, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.675, test=-3.714) total time=  13.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.2 0.  0.2 0.1], mlp__hiddens=(64, 8, 8, 32, 64, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.644, test=-4.419) total time=  13.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.2 0.  0.2 0.1], mlp__hiddens=(64, 8, 8, 32, 64, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.839, test=-3.836) total time=  13.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.2 0.  0.2 0.1], mlp__hiddens=(64, 8, 8, 32, 64, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.635, test=-4.172) total time=  13.5s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.2 0.  0.2 0.1], mlp__hiddens=(64, 8, 8, 32, 64, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.718, test=-3.726) total time=  13.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.2 0.1], mlp__hiddens=(32, 64, 32, 4, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-5.529, test=-5.329) total time=   3.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.2 0.1], mlp__hiddens=(32, 64, 32, 4, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.378, test=-5.318) total time=   2.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.2 0.1], mlp__hiddens=(32, 64, 32, 4, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-3.913, test=-4.180) total time=   3.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.2 0.1], mlp__hiddens=(32, 64, 32, 4, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-13.711, test=-13.379) total time=  14.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.2 0.1], mlp__hiddens=(32, 64, 32, 4, 16, 16), mlp__nlayers=4, mlp__optimizer__learning_rate=0.001;, score=(train=-4.358, test=-4.299) total time=   2.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 32, 16, 16, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.959, test=-4.905) total time=  15.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 32, 16, 16, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.398, test=-4.966) total time=  15.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 32, 16, 16, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.015, test=-4.156) total time=  15.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 32, 16, 16, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.783, test=-5.034) total time=  15.6s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.  0.1 0.2 0. ], mlp__hiddens=(4, 32, 16, 16, 16, 8), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.988, test=-4.590) total time=  15.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.3 0.  0.2 0.3], mlp__hiddens=(16, 64, 64, 16, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.535, test=-4.488) total time=   7.6s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.3 0.  0.2 0.3], mlp__hiddens=(16, 64, 64, 16, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.475, test=-5.361) total time=   8.2s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.3 0.  0.2 0.3], mlp__hiddens=(16, 64, 64, 16, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.136, test=-4.052) total time=   8.8s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.3 0.  0.2 0.3], mlp__hiddens=(16, 64, 64, 16, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.725, test=-4.931) total time=   7.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.3 0.  0.2 0.3], mlp__hiddens=(16, 64, 64, 16, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.575, test=-4.517) total time=  10.9s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.  0. ], mlp__hiddens=(32, 32, 32, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.024, test=-3.261) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.  0. ], mlp__hiddens=(32, 32, 32, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.100, test=-4.031) total time=   2.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.  0. ], mlp__hiddens=(32, 32, 32, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.349, test=-3.461) total time=   2.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.  0. ], mlp__hiddens=(32, 32, 32, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.861, test=-3.381) total time=   3.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.1 0.  0.  0.  0. ], mlp__hiddens=(32, 32, 32, 32, 4, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-3.095, test=-3.421) total time=   2.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.2 0.  0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 16, 64, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.879, test=-4.019) total time=  12.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.2 0.  0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 16, 64, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.835, test=-4.707) total time=   7.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.2 0.  0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 16, 64, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.571, test=-3.654) total time=   9.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.2 0.  0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 16, 64, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.837, test=-4.077) total time=   6.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.2 0.  0.  0.2 0. ], mlp__hiddens=(64, 16, 8, 16, 64, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-3.789, test=-3.999) total time=   7.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 4, 16, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.446, test=-5.201) total time=   9.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 4, 16, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.597, test=-6.752) total time=  19.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 4, 16, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.001, test=-5.193) total time=  22.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 4, 16, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.495, test=-5.374) total time=  14.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.  0. ], mlp__hiddens=(16, 16, 4, 16, 16, 32), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.187, test=-5.065) total time=  14.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 4, 16, 64, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.380, test=-6.164) total time=   7.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 4, 16, 64, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.862, test=-5.922) total time=  11.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 4, 16, 64, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.837, test=-5.653) total time=   8.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 4, 16, 64, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.433, test=-6.549) total time=   7.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.2 0.3 0.  0.  0. ], mlp__hiddens=(16, 4, 16, 64, 32, 4), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-6.167, test=-5.947) total time=   7.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(16, 32, 8, 32, 8, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.196, test=-3.517) total time=   4.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(16, 32, 8, 32, 8, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.334, test=-4.205) total time=   4.1s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(16, 32, 8, 32, 8, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.429, test=-3.658) total time=   4.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(16, 32, 8, 32, 8, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.446, test=-3.769) total time=   4.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.1 0.1 0.2 0.1 0.2], mlp__hiddens=(16, 32, 8, 32, 8, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.098, test=-3.128) total time=   4.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0. ], mlp__hiddens=(4, 64, 32, 16, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.021, test=-2.957) total time=   5.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0. ], mlp__hiddens=(4, 64, 32, 16, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.034, test=-3.614) total time=   4.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0. ], mlp__hiddens=(4, 64, 32, 16, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.147, test=-3.174) total time=   6.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0. ], mlp__hiddens=(4, 64, 32, 16, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.116, test=-3.380) total time=   5.4s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0. ], mlp__hiddens=(4, 64, 32, 16, 16, 16), mlp__nlayers=2, mlp__optimizer__learning_rate=0.001;, score=(train=-3.033, test=-2.824) total time=   5.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.2 0.2 0.  0. ], mlp__hiddens=(8, 8, 32, 32, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.916, test=-3.260) total time=   4.2s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.2 0.2 0.  0. ], mlp__hiddens=(8, 8, 32, 32, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.105, test=-3.535) total time=   3.6s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.2 0.2 0.  0. ], mlp__hiddens=(8, 8, 32, 32, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.207, test=-3.389) total time=   3.9s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.2 0.2 0.  0. ], mlp__hiddens=(8, 8, 32, 32, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-2.858, test=-3.293) total time=   4.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.2 0.2 0.  0. ], mlp__hiddens=(8, 8, 32, 32, 8, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.190, test=-3.432) total time=   3.6s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.3 0.2 0.1 0.  0.3], mlp__hiddens=(64, 16, 4, 8, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.519, test=-4.548) total time=   4.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.3 0.2 0.1 0.  0.3], mlp__hiddens=(64, 16, 4, 8, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-6.088, test=-7.066) total time=   3.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.3 0.2 0.1 0.  0.3], mlp__hiddens=(64, 16, 4, 8, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.100, test=-5.009) total time=   3.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.3 0.2 0.1 0.  0.3], mlp__hiddens=(64, 16, 4, 8, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.217, test=-5.469) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.3 0.2 0.1 0.  0.3], mlp__hiddens=(64, 16, 4, 8, 32, 32), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.972, test=-4.869) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.  0.1 0.  0.  0.3], mlp__hiddens=(64, 64, 32, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.168, test=-3.454) total time=   4.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.  0.1 0.  0.  0.3], mlp__hiddens=(64, 64, 32, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.042, test=-3.869) total time=   4.3s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.  0.1 0.  0.  0.3], mlp__hiddens=(64, 64, 32, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.200, test=-3.406) total time=   4.7s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.  0.1 0.  0.  0.3], mlp__hiddens=(64, 64, 32, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-3.130, test=-3.780) total time=   4.1s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.  0.1 0.  0.  0.3], mlp__hiddens=(64, 64, 32, 8, 32, 32), mlp__nlayers=1, mlp__optimizer__learning_rate=0.001;, score=(train=-2.952, test=-3.368) total time=   5.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.2 0.3 0.  0.1], mlp__hiddens=(16, 16, 32, 32, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.411, test=-3.516) total time=   4.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.2 0.3 0.  0.1], mlp__hiddens=(16, 16, 32, 32, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.341, test=-3.949) total time=   4.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.2 0.3 0.  0.1], mlp__hiddens=(16, 16, 32, 32, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.344, test=-3.299) total time=   5.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.2 0.3 0.  0.1], mlp__hiddens=(16, 16, 32, 32, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.678, test=-4.104) total time=   3.8s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.2 0.3 0.  0.1], mlp__hiddens=(16, 16, 32, 32, 4, 16), mlp__nlayers=3, mlp__optimizer__learning_rate=0.001;, score=(train=-3.548, test=-3.662) total time=   4.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0.1], mlp__hiddens=(8, 4, 32, 16, 16, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.168, test=-4.196) total time=  16.4s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0.1], mlp__hiddens=(8, 4, 32, 16, 16, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.462, test=-6.282) total time=  11.0s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0.1], mlp__hiddens=(8, 4, 32, 16, 16, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.127, test=-4.806) total time=  12.3s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0.1], mlp__hiddens=(8, 4, 32, 16, 16, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.101, test=-4.108) total time=  15.0s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.  0.  0.3 0.  0.1], mlp__hiddens=(8, 4, 32, 16, 16, 64), mlp__nlayers=5, mlp__optimizer__learning_rate=0.0001;, score=(train=-5.682, test=-5.016) total time=  11.0s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 8, 32, 64, 8, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.977, test=-3.403) total time=   2.9s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 8, 32, 64, 8, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.999, test=-3.915) total time=   2.9s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 8, 32, 64, 8, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.905, test=-3.472) total time=   3.1s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 8, 32, 64, 8, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.951, test=-3.426) total time=   2.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.  0.  0.  0.1 0. ], mlp__hiddens=(32, 8, 32, 64, 8, 64), mlp__nlayers=1, mlp__optimizer__learning_rate=0.005;, score=(train=-2.650, test=-3.206) total time=   3.4s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.3 0.1 0.1 0.  0.2], mlp__hiddens=(4, 4, 64, 4, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.937, test=-4.760) total time=   3.5s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.3 0.1 0.1 0.  0.2], mlp__hiddens=(4, 4, 64, 4, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.290, test=-6.466) total time=   3.8s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.3 0.1 0.1 0.  0.2], mlp__hiddens=(4, 4, 64, 4, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.153, test=-3.870) total time=   5.2s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.3 0.1 0.1 0.  0.2], mlp__hiddens=(4, 4, 64, 4, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-5.304, test=-5.294) total time=   3.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.3 0.1 0.1 0.  0.2], mlp__hiddens=(4, 4, 64, 4, 64, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.001;, score=(train=-4.135, test=-3.595) total time=   6.3s\n",
      "[CV 1/5] END mlp__dropouts=[0.1 0.1 0.1 0.  0.1 0.2], mlp__hiddens=(16, 16, 32, 8, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.158, test=-3.388) total time=   3.0s\n",
      "[CV 2/5] END mlp__dropouts=[0.1 0.1 0.1 0.  0.1 0.2], mlp__hiddens=(16, 16, 32, 8, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.773, test=-3.640) total time=   3.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.1 0.1 0.1 0.  0.1 0.2], mlp__hiddens=(16, 16, 32, 8, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.048, test=-3.264) total time=   3.5s\n",
      "[CV 4/5] END mlp__dropouts=[0.1 0.1 0.1 0.  0.1 0.2], mlp__hiddens=(16, 16, 32, 8, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.702, test=-3.223) total time=   3.9s\n",
      "[CV 5/5] END mlp__dropouts=[0.1 0.1 0.1 0.  0.1 0.2], mlp__hiddens=(16, 16, 32, 8, 16, 16), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.867, test=-3.237) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.3 0.2], mlp__hiddens=(64, 4, 32, 4, 16, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.111, test=-4.184) total time=  12.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.3 0.2], mlp__hiddens=(64, 4, 32, 4, 16, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.895, test=-5.781) total time=  10.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.3 0.2], mlp__hiddens=(64, 4, 32, 4, 16, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.620, test=-4.802) total time=   9.4s\n",
      "[CV 4/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.3 0.2], mlp__hiddens=(64, 4, 32, 4, 16, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.178, test=-4.426) total time=  11.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.2 0.2 0.3 0.  0.3 0.2], mlp__hiddens=(64, 4, 32, 4, 16, 8), mlp__nlayers=4, mlp__optimizer__learning_rate=0.0001;, score=(train=-4.423, test=-4.391) total time=  11.8s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.3 0.1 0.  0.3 0.2], mlp__hiddens=(8, 8, 4, 32, 4, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.512, test=-3.362) total time=   3.3s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.3 0.1 0.  0.3 0.2], mlp__hiddens=(8, 8, 4, 32, 4, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-6.887, test=-8.101) total time=   2.7s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.3 0.1 0.  0.3 0.2], mlp__hiddens=(8, 8, 4, 32, 4, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-4.371, test=-4.329) total time=   5.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.3 0.1 0.  0.3 0.2], mlp__hiddens=(8, 8, 4, 32, 4, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.867, test=-3.884) total time=   3.3s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.3 0.1 0.  0.3 0.2], mlp__hiddens=(8, 8, 4, 32, 4, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-5.821, test=-5.566) total time=   3.5s\n",
      "[CV 1/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 16, 16, 32, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.038, test=-3.299) total time=   3.1s\n",
      "[CV 2/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 16, 16, 32, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-2.613, test=-3.820) total time=   3.4s\n",
      "[CV 3/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 16, 16, 32, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.908, test=-3.894) total time=   2.6s\n",
      "[CV 4/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 16, 16, 32, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.525, test=-3.751) total time=   2.7s\n",
      "[CV 5/5] END mlp__dropouts=[0.  0.  0.2 0.  0.1 0. ], mlp__hiddens=(64, 16, 16, 32, 32, 32), mlp__nlayers=5, mlp__optimizer__learning_rate=0.005;, score=(train=-3.555, test=-3.864) total time=   2.7s\n",
      "[CV 1/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.2 0.3], mlp__hiddens=(8, 4, 32, 16, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.680, test=-3.602) total time=   3.7s\n",
      "[CV 2/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.2 0.3], mlp__hiddens=(8, 4, 32, 16, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.349, test=-3.652) total time=   3.5s\n",
      "[CV 3/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.2 0.3], mlp__hiddens=(8, 4, 32, 16, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.842, test=-4.833) total time=   3.0s\n",
      "[CV 4/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.2 0.3], mlp__hiddens=(8, 4, 32, 16, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-4.920, test=-4.972) total time=   3.2s\n",
      "[CV 5/5] END mlp__dropouts=[0.3 0.2 0.  0.3 0.2 0.3], mlp__hiddens=(8, 4, 32, 16, 8, 16), mlp__nlayers=6, mlp__optimizer__learning_rate=0.005;, score=(train=-3.723, test=-3.373) total time=   3.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[(&#x27;scaler&#x27;, PowerTransformer()),\n",
       "                                             (&#x27;principal_components&#x27;,\n",
       "                                              PCA(n_components=20,\n",
       "                                                  svd_solver=&#x27;full&#x27;)),\n",
       "                                             (&#x27;mlp&#x27;,\n",
       "                                              KerasRegressor(callbacks=[&lt;keras.src.callbacks.EarlyStopping object at 0x0000016BF024C850&gt;], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss=&#x27;mse&#x27;, model=...\n",
       "                                                         (4, 4, 4, 4, 64, 4),\n",
       "                                                         (4, 4, 4, 4, 64, 8),\n",
       "                                                         (4, 4, 4, 4, 64, 16),\n",
       "                                                         (4, 4, 4, 4, 64, 32),\n",
       "                                                         (4, 4, 4, 4, 64, 64),\n",
       "                                                         (4, 4, 4, 8, 4, 4),\n",
       "                                                         (4, 4, 4, 8, 4, 8),\n",
       "                                                         (4, 4, 4, 8, 4, 16),\n",
       "                                                         (4, 4, 4, 8, 4, 32),\n",
       "                                                         (4, 4, 4, 8, 4, 64), ...],\n",
       "                                        &#x27;mlp__nlayers&#x27;: array([4, 5, 4, 4, 5, 1, 1, 1, 5, 2, 4, 1, 2, 5, 6, 2, 5, 3, 5, 6, 1, 6]),\n",
       "                                        &#x27;mlp__optimizer__learning_rate&#x27;: [0.0001,\n",
       "                                                                          0.001,\n",
       "                                                                          0.005]},\n",
       "                   refit=False, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[(&#x27;scaler&#x27;, PowerTransformer()),\n",
       "                                             (&#x27;principal_components&#x27;,\n",
       "                                              PCA(n_components=20,\n",
       "                                                  svd_solver=&#x27;full&#x27;)),\n",
       "                                             (&#x27;mlp&#x27;,\n",
       "                                              KerasRegressor(callbacks=[&lt;keras.src.callbacks.EarlyStopping object at 0x0000016BF024C850&gt;], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss=&#x27;mse&#x27;, model=...\n",
       "                                                         (4, 4, 4, 4, 64, 4),\n",
       "                                                         (4, 4, 4, 4, 64, 8),\n",
       "                                                         (4, 4, 4, 4, 64, 16),\n",
       "                                                         (4, 4, 4, 4, 64, 32),\n",
       "                                                         (4, 4, 4, 4, 64, 64),\n",
       "                                                         (4, 4, 4, 8, 4, 4),\n",
       "                                                         (4, 4, 4, 8, 4, 8),\n",
       "                                                         (4, 4, 4, 8, 4, 16),\n",
       "                                                         (4, 4, 4, 8, 4, 32),\n",
       "                                                         (4, 4, 4, 8, 4, 64), ...],\n",
       "                                        &#x27;mlp__nlayers&#x27;: array([4, 5, 4, 4, 5, 1, 1, 1, 5, 2, 4, 1, 2, 5, 6, 2, 5, 3, 5, 6, 1, 6]),\n",
       "                                        &#x27;mlp__optimizer__learning_rate&#x27;: [0.0001,\n",
       "                                                                          0.001,\n",
       "                                                                          0.005]},\n",
       "                   refit=False, return_train_score=True,\n",
       "                   scoring=&#x27;neg_mean_absolute_error&#x27;, verbose=3)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, PowerTransformer()),\n",
       "                (&#x27;principal_components&#x27;,\n",
       "                 PCA(n_components=20, svd_solver=&#x27;full&#x27;)),\n",
       "                (&#x27;mlp&#x27;,\n",
       "                 KerasRegressor(callbacks=[&lt;keras.src.callbacks.EarlyStopping object at 0x0000016BF024C850&gt;], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss=&#x27;mse&#x27;, model=&lt;function twoLayerFeedForward at 0x0000016BF024B430&gt;, nlayers=3, validation_split=0.2, verbose=0))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PowerTransformer</label><div class=\"sk-toggleable__content\"><pre>PowerTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=20, svd_solver=&#x27;full&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>KerasRegressor(\n",
       "\tmodel=&lt;function twoLayerFeedForward at 0x0000016BF024B430&gt;\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=mse\n",
       "\tmetrics=None\n",
       "\tbatch_size=None\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=0\n",
       "\tcallbacks=[&lt;keras.src.callbacks.EarlyStopping object at 0x0000016BF024C850&gt;]\n",
       "\tvalidation_split=0.2\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=200\n",
       "\tnlayers=3\n",
       "\thiddens=[2, 2, 2]\n",
       "\tdropouts=[0.2, 0, 0]\n",
       ")</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=KFold(n_splits=5, random_state=None, shuffle=True),\n",
       "                   estimator=Pipeline(steps=[('scaler', PowerTransformer()),\n",
       "                                             ('principal_components',\n",
       "                                              PCA(n_components=20,\n",
       "                                                  svd_solver='full')),\n",
       "                                             ('mlp',\n",
       "                                              KerasRegressor(callbacks=[<keras.src.callbacks.EarlyStopping object at 0x0000016BF024C850>], dropouts=[0.2, 0, 0], epochs=200, hiddens=[2, 2, 2], loss='mse', model=...\n",
       "                                                         (4, 4, 4, 4, 64, 4),\n",
       "                                                         (4, 4, 4, 4, 64, 8),\n",
       "                                                         (4, 4, 4, 4, 64, 16),\n",
       "                                                         (4, 4, 4, 4, 64, 32),\n",
       "                                                         (4, 4, 4, 4, 64, 64),\n",
       "                                                         (4, 4, 4, 8, 4, 4),\n",
       "                                                         (4, 4, 4, 8, 4, 8),\n",
       "                                                         (4, 4, 4, 8, 4, 16),\n",
       "                                                         (4, 4, 4, 8, 4, 32),\n",
       "                                                         (4, 4, 4, 8, 4, 64), ...],\n",
       "                                        'mlp__nlayers': array([4, 5, 4, 4, 5, 1, 1, 1, 5, 2, 4, 1, 2, 5, 6, 2, 5, 3, 5, 6, 1, 6]),\n",
       "                                        'mlp__optimizer__learning_rate': [0.0001,\n",
       "                                                                          0.001,\n",
       "                                                                          0.005]},\n",
       "                   refit=False, return_train_score=True,\n",
       "                   scoring='neg_mean_absolute_error', verbose=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(gs.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_overfit = df_results[\"mean_train_score\"] - df_results[\"mean_test_score\"] < 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_mlp__optimizer__learning_rate</th>\n",
       "      <th>param_mlp__nlayers</th>\n",
       "      <th>param_mlp__hiddens</th>\n",
       "      <th>param_mlp__dropouts</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5.470936</td>\n",
       "      <td>0.476462</td>\n",
       "      <td>0.149602</td>\n",
       "      <td>0.009291</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2</td>\n",
       "      <td>(4, 64, 32, 16, 16, 16)</td>\n",
       "      <td>[0.2, 0.0, 0.0, 0.3, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-2.957158</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.189668</td>\n",
       "      <td>0.284323</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.021284</td>\n",
       "      <td>-3.033675</td>\n",
       "      <td>-3.146795</td>\n",
       "      <td>-3.115551</td>\n",
       "      <td>-3.032817</td>\n",
       "      <td>-3.070024</td>\n",
       "      <td>0.051084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3.254239</td>\n",
       "      <td>0.393104</td>\n",
       "      <td>0.159371</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 16, 32, 4, 64, 64)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.1, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.359709</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.339414</td>\n",
       "      <td>0.187513</td>\n",
       "      <td>2</td>\n",
       "      <td>-3.276286</td>\n",
       "      <td>-3.058097</td>\n",
       "      <td>-3.007638</td>\n",
       "      <td>-2.995851</td>\n",
       "      <td>-2.941716</td>\n",
       "      <td>-3.055918</td>\n",
       "      <td>0.116231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.769879</td>\n",
       "      <td>0.999212</td>\n",
       "      <td>0.158720</td>\n",
       "      <td>0.003884</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 16, 4, 32, 4, 8)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.1, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.004115</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.388544</td>\n",
       "      <td>0.282147</td>\n",
       "      <td>5</td>\n",
       "      <td>-3.191301</td>\n",
       "      <td>-3.203838</td>\n",
       "      <td>-3.386880</td>\n",
       "      <td>-3.343279</td>\n",
       "      <td>-2.948415</td>\n",
       "      <td>-3.214743</td>\n",
       "      <td>0.153461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2.913223</td>\n",
       "      <td>0.096621</td>\n",
       "      <td>0.147412</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(8, 16, 16, 64, 8, 32)</td>\n",
       "      <td>[0.0, 0.3, 0.0, 0.3, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-2.979923</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.389105</td>\n",
       "      <td>0.262381</td>\n",
       "      <td>6</td>\n",
       "      <td>-3.050441</td>\n",
       "      <td>-3.067475</td>\n",
       "      <td>-3.243703</td>\n",
       "      <td>-3.062296</td>\n",
       "      <td>-3.360610</td>\n",
       "      <td>-3.156905</td>\n",
       "      <td>0.124348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>4.804874</td>\n",
       "      <td>0.198884</td>\n",
       "      <td>0.167738</td>\n",
       "      <td>0.013939</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 8, 32, 16, 8, 4)</td>\n",
       "      <td>[0.1, 0.0, 0.3, 0.0, 0.0, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.120479</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.409711</td>\n",
       "      <td>0.226976</td>\n",
       "      <td>7</td>\n",
       "      <td>-3.220394</td>\n",
       "      <td>-3.050101</td>\n",
       "      <td>-3.269757</td>\n",
       "      <td>-3.170700</td>\n",
       "      <td>-3.474918</td>\n",
       "      <td>-3.237174</td>\n",
       "      <td>0.139485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.922058</td>\n",
       "      <td>0.248130</td>\n",
       "      <td>0.153787</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 64, 32, 64, 32, 8)</td>\n",
       "      <td>[0.1, 0.0, 0.0, 0.3, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.184837</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.449240</td>\n",
       "      <td>0.152593</td>\n",
       "      <td>8</td>\n",
       "      <td>-3.271727</td>\n",
       "      <td>-2.954547</td>\n",
       "      <td>-3.284860</td>\n",
       "      <td>-3.516394</td>\n",
       "      <td>-3.227274</td>\n",
       "      <td>-3.250960</td>\n",
       "      <td>0.179141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.082494</td>\n",
       "      <td>0.262968</td>\n",
       "      <td>0.157247</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 4, 16, 64, 16, 16)</td>\n",
       "      <td>[0.2, 0.0, 0.0, 0.2, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.238365</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.456015</td>\n",
       "      <td>0.223538</td>\n",
       "      <td>9</td>\n",
       "      <td>-3.247670</td>\n",
       "      <td>-3.003067</td>\n",
       "      <td>-3.510962</td>\n",
       "      <td>-3.714861</td>\n",
       "      <td>-3.077744</td>\n",
       "      <td>-3.310861</td>\n",
       "      <td>0.266957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.814192</td>\n",
       "      <td>0.231426</td>\n",
       "      <td>0.166566</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(32, 4, 32, 8, 4, 8)</td>\n",
       "      <td>[0.0, 0.2, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.580219</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.458093</td>\n",
       "      <td>0.514689</td>\n",
       "      <td>10</td>\n",
       "      <td>-3.624743</td>\n",
       "      <td>-3.823013</td>\n",
       "      <td>-3.361067</td>\n",
       "      <td>-2.679449</td>\n",
       "      <td>-2.958318</td>\n",
       "      <td>-3.289318</td>\n",
       "      <td>0.420473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.129875</td>\n",
       "      <td>0.208705</td>\n",
       "      <td>0.159137</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 8, 64, 32, 16, 4)</td>\n",
       "      <td>[0.1, 0.2, 0.0, 0.1, 0.2, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.456528</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.512017</td>\n",
       "      <td>0.102197</td>\n",
       "      <td>15</td>\n",
       "      <td>-3.829248</td>\n",
       "      <td>-3.139033</td>\n",
       "      <td>-3.222706</td>\n",
       "      <td>-3.332092</td>\n",
       "      <td>-3.674975</td>\n",
       "      <td>-3.439611</td>\n",
       "      <td>0.266897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>13.533426</td>\n",
       "      <td>1.276400</td>\n",
       "      <td>0.147617</td>\n",
       "      <td>0.003273</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 64, 64, 64, 8, 8)</td>\n",
       "      <td>[0.0, 0.1, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-3.357574</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.541656</td>\n",
       "      <td>0.242615</td>\n",
       "      <td>17</td>\n",
       "      <td>-3.112184</td>\n",
       "      <td>-3.550833</td>\n",
       "      <td>-3.291955</td>\n",
       "      <td>-3.909624</td>\n",
       "      <td>-3.374830</td>\n",
       "      <td>-3.447885</td>\n",
       "      <td>0.270610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.707712</td>\n",
       "      <td>0.612028</td>\n",
       "      <td>0.148188</td>\n",
       "      <td>0.007616</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 64, 32, 4, 4, 64)</td>\n",
       "      <td>[0.1, 0.1, 0.2, 0.0, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.077817</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.544212</td>\n",
       "      <td>0.443199</td>\n",
       "      <td>19</td>\n",
       "      <td>-3.112963</td>\n",
       "      <td>-3.445468</td>\n",
       "      <td>-3.464016</td>\n",
       "      <td>-3.251278</td>\n",
       "      <td>-3.177948</td>\n",
       "      <td>-3.290334</td>\n",
       "      <td>0.141314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>4.405320</td>\n",
       "      <td>0.428264</td>\n",
       "      <td>0.161241</td>\n",
       "      <td>0.016881</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3</td>\n",
       "      <td>(16, 16, 32, 32, 4, 16)</td>\n",
       "      <td>[0.0, 0.3, 0.2, 0.3, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.516341</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.706200</td>\n",
       "      <td>0.290178</td>\n",
       "      <td>34</td>\n",
       "      <td>-3.411198</td>\n",
       "      <td>-3.340559</td>\n",
       "      <td>-3.343767</td>\n",
       "      <td>-3.677989</td>\n",
       "      <td>-3.548275</td>\n",
       "      <td>-3.464358</td>\n",
       "      <td>0.130671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3.113723</td>\n",
       "      <td>0.356087</td>\n",
       "      <td>0.166270</td>\n",
       "      <td>0.005083</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(4, 64, 16, 4, 64, 4)</td>\n",
       "      <td>[0.0, 0.1, 0.3, 0.0, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.416646</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.744554</td>\n",
       "      <td>0.476248</td>\n",
       "      <td>37</td>\n",
       "      <td>-3.433923</td>\n",
       "      <td>-3.767617</td>\n",
       "      <td>-3.445217</td>\n",
       "      <td>-3.330944</td>\n",
       "      <td>-4.653721</td>\n",
       "      <td>-3.726284</td>\n",
       "      <td>0.486336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.599186</td>\n",
       "      <td>0.430841</td>\n",
       "      <td>0.157698</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(64, 4, 4, 64, 32, 16)</td>\n",
       "      <td>[0.3, 0.1, 0.1, 0.1, 0.3, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.027332</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.764683</td>\n",
       "      <td>0.521580</td>\n",
       "      <td>38</td>\n",
       "      <td>-3.014995</td>\n",
       "      <td>-3.359020</td>\n",
       "      <td>-3.815708</td>\n",
       "      <td>-4.145503</td>\n",
       "      <td>-3.374597</td>\n",
       "      <td>-3.541964</td>\n",
       "      <td>0.394556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.985743</td>\n",
       "      <td>1.947874</td>\n",
       "      <td>0.134508</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>2</td>\n",
       "      <td>(32, 64, 8, 16, 8, 64)</td>\n",
       "      <td>[0.0, 0.2, 0.1, 0.3, 0.3, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-3.461880</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.784165</td>\n",
       "      <td>0.266862</td>\n",
       "      <td>40</td>\n",
       "      <td>-3.358058</td>\n",
       "      <td>-3.698754</td>\n",
       "      <td>-3.577373</td>\n",
       "      <td>-3.402665</td>\n",
       "      <td>-3.438046</td>\n",
       "      <td>-3.494979</td>\n",
       "      <td>0.125558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2.773342</td>\n",
       "      <td>0.177885</td>\n",
       "      <td>0.164475</td>\n",
       "      <td>0.016423</td>\n",
       "      <td>0.005</td>\n",
       "      <td>4</td>\n",
       "      <td>(16, 16, 16, 8, 8, 4)</td>\n",
       "      <td>[0.0, 0.2, 0.2, 0.3, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.992399</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.829739</td>\n",
       "      <td>0.283589</td>\n",
       "      <td>43</td>\n",
       "      <td>-3.822174</td>\n",
       "      <td>-3.603794</td>\n",
       "      <td>-3.540664</td>\n",
       "      <td>-3.151512</td>\n",
       "      <td>-3.832396</td>\n",
       "      <td>-3.590108</td>\n",
       "      <td>0.248015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3.658636</td>\n",
       "      <td>0.934027</td>\n",
       "      <td>0.158290</td>\n",
       "      <td>0.007577</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(4, 64, 4, 16, 64, 16)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.1, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.108080</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.891749</td>\n",
       "      <td>1.150862</td>\n",
       "      <td>46</td>\n",
       "      <td>-2.900171</td>\n",
       "      <td>-3.110227</td>\n",
       "      <td>-2.890419</td>\n",
       "      <td>-3.143502</td>\n",
       "      <td>-6.277767</td>\n",
       "      <td>-3.664417</td>\n",
       "      <td>1.310818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.420327</td>\n",
       "      <td>0.248797</td>\n",
       "      <td>0.178662</td>\n",
       "      <td>0.024193</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 8, 8, 4, 32, 32)</td>\n",
       "      <td>[0.3, 0.0, 0.3, 0.0, 0.1, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.441650</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.930755</td>\n",
       "      <td>0.448353</td>\n",
       "      <td>47</td>\n",
       "      <td>-4.517016</td>\n",
       "      <td>-3.795329</td>\n",
       "      <td>-3.722823</td>\n",
       "      <td>-3.454863</td>\n",
       "      <td>-3.601107</td>\n",
       "      <td>-3.818227</td>\n",
       "      <td>0.367996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3.866739</td>\n",
       "      <td>1.498146</td>\n",
       "      <td>0.161018</td>\n",
       "      <td>0.009644</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(64, 16, 32, 32, 4, 64)</td>\n",
       "      <td>[0.0, 0.3, 0.2, 0.2, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.700792</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.952290</td>\n",
       "      <td>0.392927</td>\n",
       "      <td>48</td>\n",
       "      <td>-3.701288</td>\n",
       "      <td>-3.604465</td>\n",
       "      <td>-3.363142</td>\n",
       "      <td>-3.933469</td>\n",
       "      <td>-3.762802</td>\n",
       "      <td>-3.673033</td>\n",
       "      <td>0.188371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>13.500372</td>\n",
       "      <td>0.039636</td>\n",
       "      <td>0.122368</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(64, 8, 8, 32, 64, 32)</td>\n",
       "      <td>[0.3, 0.0, 0.2, 0.0, 0.2, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-3.714196</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.973507</td>\n",
       "      <td>0.277631</td>\n",
       "      <td>49</td>\n",
       "      <td>-3.674985</td>\n",
       "      <td>-3.643582</td>\n",
       "      <td>-3.838981</td>\n",
       "      <td>-3.634803</td>\n",
       "      <td>-3.717554</td>\n",
       "      <td>-3.701981</td>\n",
       "      <td>0.074382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.833933</td>\n",
       "      <td>0.340188</td>\n",
       "      <td>0.258638</td>\n",
       "      <td>0.155654</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 8, 32, 32, 32, 64)</td>\n",
       "      <td>[0.0, 0.3, 0.2, 0.2, 0.3, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-4.235472</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.050233</td>\n",
       "      <td>0.854436</td>\n",
       "      <td>52</td>\n",
       "      <td>-4.166159</td>\n",
       "      <td>-4.546339</td>\n",
       "      <td>-3.158995</td>\n",
       "      <td>-3.006879</td>\n",
       "      <td>-4.170782</td>\n",
       "      <td>-3.809831</td>\n",
       "      <td>0.611232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3.251839</td>\n",
       "      <td>0.234404</td>\n",
       "      <td>0.193962</td>\n",
       "      <td>0.015705</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 4, 32, 16, 8, 16)</td>\n",
       "      <td>[0.3, 0.2, 0.0, 0.3, 0.2, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.601571</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.086420</td>\n",
       "      <td>0.674452</td>\n",
       "      <td>53</td>\n",
       "      <td>-3.679801</td>\n",
       "      <td>-3.348609</td>\n",
       "      <td>-4.842064</td>\n",
       "      <td>-4.919756</td>\n",
       "      <td>-3.722860</td>\n",
       "      <td>-4.102618</td>\n",
       "      <td>0.649002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3.685257</td>\n",
       "      <td>0.226788</td>\n",
       "      <td>0.173976</td>\n",
       "      <td>0.005980</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(32, 16, 32, 64, 4, 16)</td>\n",
       "      <td>[0.0, 0.2, 0.3, 0.3, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.780503</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.128354</td>\n",
       "      <td>0.349025</td>\n",
       "      <td>55</td>\n",
       "      <td>-3.663205</td>\n",
       "      <td>-3.603885</td>\n",
       "      <td>-3.809171</td>\n",
       "      <td>-4.237593</td>\n",
       "      <td>-3.905857</td>\n",
       "      <td>-3.843942</td>\n",
       "      <td>0.223739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3.051652</td>\n",
       "      <td>0.083580</td>\n",
       "      <td>0.168676</td>\n",
       "      <td>0.017655</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 16, 16, 8, 8, 32)</td>\n",
       "      <td>[0.0, 0.3, 0.3, 0.1, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.739882</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.138396</td>\n",
       "      <td>0.496801</td>\n",
       "      <td>56</td>\n",
       "      <td>-3.758855</td>\n",
       "      <td>-4.097590</td>\n",
       "      <td>-4.616371</td>\n",
       "      <td>-3.692351</td>\n",
       "      <td>-3.784866</td>\n",
       "      <td>-3.990007</td>\n",
       "      <td>0.342934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.827376</td>\n",
       "      <td>0.611622</td>\n",
       "      <td>0.406129</td>\n",
       "      <td>0.419923</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6</td>\n",
       "      <td>(8, 8, 8, 4, 4, 8)</td>\n",
       "      <td>[0.0, 0.0, 0.2, 0.1, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.973052</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.175025</td>\n",
       "      <td>0.759491</td>\n",
       "      <td>58</td>\n",
       "      <td>-4.053489</td>\n",
       "      <td>-3.262997</td>\n",
       "      <td>-3.994926</td>\n",
       "      <td>-5.677628</td>\n",
       "      <td>-3.290253</td>\n",
       "      <td>-4.055858</td>\n",
       "      <td>0.877341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.303824</td>\n",
       "      <td>1.669410</td>\n",
       "      <td>0.268030</td>\n",
       "      <td>0.212775</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(64, 16, 16, 32, 32, 64)</td>\n",
       "      <td>[0.2, 0.1, 0.0, 0.3, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-3.957772</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.240322</td>\n",
       "      <td>0.404230</td>\n",
       "      <td>60</td>\n",
       "      <td>-3.829952</td>\n",
       "      <td>-4.127654</td>\n",
       "      <td>-3.772763</td>\n",
       "      <td>-4.157570</td>\n",
       "      <td>-4.009025</td>\n",
       "      <td>-3.979393</td>\n",
       "      <td>0.154684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10.415561</td>\n",
       "      <td>1.032343</td>\n",
       "      <td>0.174150</td>\n",
       "      <td>0.016677</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(16, 64, 4, 4, 32, 16)</td>\n",
       "      <td>[0.1, 0.2, 0.0, 0.0, 0.3, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-3.908104</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.241201</td>\n",
       "      <td>0.462397</td>\n",
       "      <td>61</td>\n",
       "      <td>-3.938528</td>\n",
       "      <td>-4.110858</td>\n",
       "      <td>-4.649694</td>\n",
       "      <td>-4.021063</td>\n",
       "      <td>-3.861953</td>\n",
       "      <td>-4.116419</td>\n",
       "      <td>0.279250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.867887</td>\n",
       "      <td>0.799057</td>\n",
       "      <td>0.160561</td>\n",
       "      <td>0.005973</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(16, 8, 16, 32, 32, 64)</td>\n",
       "      <td>[0.2, 0.2, 0.0, 0.1, 0.2, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.676929</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.246026</td>\n",
       "      <td>0.367301</td>\n",
       "      <td>62</td>\n",
       "      <td>-3.886688</td>\n",
       "      <td>-3.296845</td>\n",
       "      <td>-4.230406</td>\n",
       "      <td>-4.545318</td>\n",
       "      <td>-4.671376</td>\n",
       "      <td>-4.126127</td>\n",
       "      <td>0.495768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>4.744860</td>\n",
       "      <td>0.703259</td>\n",
       "      <td>0.168538</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 8, 16, 8, 32, 32)</td>\n",
       "      <td>[0.0, 0.2, 0.0, 0.3, 0.3, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.318925</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.485782</td>\n",
       "      <td>0.855480</td>\n",
       "      <td>66</td>\n",
       "      <td>-3.578431</td>\n",
       "      <td>-4.722735</td>\n",
       "      <td>-5.098557</td>\n",
       "      <td>-3.904302</td>\n",
       "      <td>-4.295389</td>\n",
       "      <td>-4.319883</td>\n",
       "      <td>0.546202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>6.800638</td>\n",
       "      <td>1.451079</td>\n",
       "      <td>0.147935</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(16, 64, 64, 16, 4, 64)</td>\n",
       "      <td>[0.1, 0.2, 0.3, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.649604</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.564738</td>\n",
       "      <td>0.469138</td>\n",
       "      <td>67</td>\n",
       "      <td>-4.756161</td>\n",
       "      <td>-4.141934</td>\n",
       "      <td>-4.396064</td>\n",
       "      <td>-4.651727</td>\n",
       "      <td>-3.879096</td>\n",
       "      <td>-4.364996</td>\n",
       "      <td>0.323124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.659430</td>\n",
       "      <td>1.323079</td>\n",
       "      <td>0.154089</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(32, 32, 32, 4, 8, 32)</td>\n",
       "      <td>[0.3, 0.2, 0.0, 0.0, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.097206</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.576668</td>\n",
       "      <td>0.381651</td>\n",
       "      <td>68</td>\n",
       "      <td>-3.894645</td>\n",
       "      <td>-4.290361</td>\n",
       "      <td>-4.646753</td>\n",
       "      <td>-4.430793</td>\n",
       "      <td>-4.756420</td>\n",
       "      <td>-4.403794</td>\n",
       "      <td>0.302060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>8.522782</td>\n",
       "      <td>1.261459</td>\n",
       "      <td>0.153680</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(16, 64, 64, 16, 16, 16)</td>\n",
       "      <td>[0.0, 0.3, 0.3, 0.0, 0.2, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.487602</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.669700</td>\n",
       "      <td>0.443496</td>\n",
       "      <td>70</td>\n",
       "      <td>-4.535055</td>\n",
       "      <td>-4.475437</td>\n",
       "      <td>-4.136284</td>\n",
       "      <td>-4.725159</td>\n",
       "      <td>-4.575393</td>\n",
       "      <td>-4.489466</td>\n",
       "      <td>0.194909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.159350</td>\n",
       "      <td>0.276013</td>\n",
       "      <td>0.147613</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(8, 32, 4, 4, 4, 8)</td>\n",
       "      <td>[0.0, 0.0, 0.2, 0.2, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-3.809388</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.709663</td>\n",
       "      <td>0.785014</td>\n",
       "      <td>71</td>\n",
       "      <td>-3.702351</td>\n",
       "      <td>-5.100660</td>\n",
       "      <td>-4.410270</td>\n",
       "      <td>-4.342451</td>\n",
       "      <td>-4.576970</td>\n",
       "      <td>-4.426540</td>\n",
       "      <td>0.449188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>11.113196</td>\n",
       "      <td>1.149716</td>\n",
       "      <td>0.185216</td>\n",
       "      <td>0.017740</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(64, 4, 32, 4, 16, 8)</td>\n",
       "      <td>[0.2, 0.2, 0.3, 0.0, 0.3, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.183973</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.716829</td>\n",
       "      <td>0.568313</td>\n",
       "      <td>72</td>\n",
       "      <td>-4.110705</td>\n",
       "      <td>-4.895442</td>\n",
       "      <td>-4.619998</td>\n",
       "      <td>-4.177744</td>\n",
       "      <td>-4.422670</td>\n",
       "      <td>-4.445312</td>\n",
       "      <td>0.288860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>15.316132</td>\n",
       "      <td>0.109703</td>\n",
       "      <td>0.157230</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 32, 16, 16, 16, 8)</td>\n",
       "      <td>[0.1, 0.0, 0.0, 0.1, 0.2, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.904852</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.730211</td>\n",
       "      <td>0.324791</td>\n",
       "      <td>73</td>\n",
       "      <td>-4.959069</td>\n",
       "      <td>-4.398470</td>\n",
       "      <td>-4.014894</td>\n",
       "      <td>-4.783172</td>\n",
       "      <td>-4.988226</td>\n",
       "      <td>-4.628766</td>\n",
       "      <td>0.372029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>5.072642</td>\n",
       "      <td>1.164321</td>\n",
       "      <td>0.175479</td>\n",
       "      <td>0.027345</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(4, 16, 32, 4, 4, 8)</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.257807</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.754752</td>\n",
       "      <td>0.567503</td>\n",
       "      <td>74</td>\n",
       "      <td>-4.401380</td>\n",
       "      <td>-4.095224</td>\n",
       "      <td>-4.807626</td>\n",
       "      <td>-5.376511</td>\n",
       "      <td>-4.276505</td>\n",
       "      <td>-4.591449</td>\n",
       "      <td>0.457080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>7.233608</td>\n",
       "      <td>1.203604</td>\n",
       "      <td>0.156587</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(64, 8, 4, 64, 16, 8)</td>\n",
       "      <td>[0.1, 0.0, 0.2, 0.0, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.388000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.775378</td>\n",
       "      <td>0.396797</td>\n",
       "      <td>75</td>\n",
       "      <td>-4.238092</td>\n",
       "      <td>-4.482292</td>\n",
       "      <td>-4.405696</td>\n",
       "      <td>-4.352433</td>\n",
       "      <td>-5.224022</td>\n",
       "      <td>-4.540507</td>\n",
       "      <td>0.350879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>4.387280</td>\n",
       "      <td>1.067347</td>\n",
       "      <td>0.197560</td>\n",
       "      <td>0.019012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(4, 4, 64, 4, 64, 16)</td>\n",
       "      <td>[0.2, 0.3, 0.1, 0.1, 0.0, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.760008</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.796932</td>\n",
       "      <td>1.033392</td>\n",
       "      <td>76</td>\n",
       "      <td>-4.937289</td>\n",
       "      <td>-5.290316</td>\n",
       "      <td>-4.152518</td>\n",
       "      <td>-5.304166</td>\n",
       "      <td>-4.135320</td>\n",
       "      <td>-4.763922</td>\n",
       "      <td>0.523061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>13.057288</td>\n",
       "      <td>2.172402</td>\n",
       "      <td>0.182621</td>\n",
       "      <td>0.017662</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 4, 32, 16, 16, 64)</td>\n",
       "      <td>[0.2, 0.0, 0.0, 0.3, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.196199</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.881512</td>\n",
       "      <td>0.781419</td>\n",
       "      <td>78</td>\n",
       "      <td>-4.168462</td>\n",
       "      <td>-5.461524</td>\n",
       "      <td>-5.127280</td>\n",
       "      <td>-4.101031</td>\n",
       "      <td>-5.682073</td>\n",
       "      <td>-4.908074</td>\n",
       "      <td>0.656015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>15.038958</td>\n",
       "      <td>0.208476</td>\n",
       "      <td>0.151496</td>\n",
       "      <td>0.009477</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(4, 8, 32, 16, 8, 64)</td>\n",
       "      <td>[0.1, 0.1, 0.0, 0.1, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.163329</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.983803</td>\n",
       "      <td>0.980938</td>\n",
       "      <td>79</td>\n",
       "      <td>-4.700527</td>\n",
       "      <td>-5.791149</td>\n",
       "      <td>-4.725082</td>\n",
       "      <td>-4.299535</td>\n",
       "      <td>-4.247497</td>\n",
       "      <td>-4.752758</td>\n",
       "      <td>0.555419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.170824</td>\n",
       "      <td>0.091711</td>\n",
       "      <td>0.168823</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5</td>\n",
       "      <td>(32, 64, 16, 32, 4, 16)</td>\n",
       "      <td>[0.1, 0.2, 0.0, 0.0, 0.3, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.142184</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.991305</td>\n",
       "      <td>0.819548</td>\n",
       "      <td>80</td>\n",
       "      <td>-4.258037</td>\n",
       "      <td>-4.185727</td>\n",
       "      <td>-3.815762</td>\n",
       "      <td>-5.644473</td>\n",
       "      <td>-5.861397</td>\n",
       "      <td>-4.753079</td>\n",
       "      <td>0.832883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3.588123</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.181228</td>\n",
       "      <td>0.020010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 8, 4, 32, 4, 32)</td>\n",
       "      <td>[0.0, 0.3, 0.1, 0.0, 0.3, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.005, 'mlp_...</td>\n",
       "      <td>-3.362262</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.048584</td>\n",
       "      <td>1.691472</td>\n",
       "      <td>81</td>\n",
       "      <td>-3.512012</td>\n",
       "      <td>-6.887151</td>\n",
       "      <td>-4.371084</td>\n",
       "      <td>-3.867183</td>\n",
       "      <td>-5.821289</td>\n",
       "      <td>-4.891744</td>\n",
       "      <td>1.270447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>3.832562</td>\n",
       "      <td>0.352106</td>\n",
       "      <td>0.158609</td>\n",
       "      <td>0.014964</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(16, 16, 4, 16, 4, 8)</td>\n",
       "      <td>[0.3, 0.1, 0.2, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.535151</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.058758</td>\n",
       "      <td>0.463635</td>\n",
       "      <td>82</td>\n",
       "      <td>-4.457248</td>\n",
       "      <td>-4.286443</td>\n",
       "      <td>-5.660571</td>\n",
       "      <td>-4.708597</td>\n",
       "      <td>-4.695567</td>\n",
       "      <td>-4.761685</td>\n",
       "      <td>0.476177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.988067</td>\n",
       "      <td>0.731589</td>\n",
       "      <td>0.180840</td>\n",
       "      <td>0.021492</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(64, 4, 4, 16, 32, 8)</td>\n",
       "      <td>[0.0, 0.3, 0.2, 0.2, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.507253</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.078855</td>\n",
       "      <td>0.782286</td>\n",
       "      <td>83</td>\n",
       "      <td>-4.556275</td>\n",
       "      <td>-4.399721</td>\n",
       "      <td>-4.058169</td>\n",
       "      <td>-5.960668</td>\n",
       "      <td>-5.657922</td>\n",
       "      <td>-4.926551</td>\n",
       "      <td>0.744720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15.071348</td>\n",
       "      <td>1.730528</td>\n",
       "      <td>0.168402</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(4, 16, 8, 32, 32, 8)</td>\n",
       "      <td>[0.0, 0.1, 0.3, 0.0, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.822970</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.130311</td>\n",
       "      <td>0.541140</td>\n",
       "      <td>84</td>\n",
       "      <td>-4.835890</td>\n",
       "      <td>-4.896477</td>\n",
       "      <td>-4.848215</td>\n",
       "      <td>-5.879037</td>\n",
       "      <td>-4.828615</td>\n",
       "      <td>-5.057647</td>\n",
       "      <td>0.411376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.589028</td>\n",
       "      <td>0.565618</td>\n",
       "      <td>0.450885</td>\n",
       "      <td>0.574599</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(64, 16, 32, 64, 64, 8)</td>\n",
       "      <td>[0.3, 0.1, 0.3, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-4.869548</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.191053</td>\n",
       "      <td>0.641848</td>\n",
       "      <td>85</td>\n",
       "      <td>-4.812561</td>\n",
       "      <td>-5.219427</td>\n",
       "      <td>-4.746927</td>\n",
       "      <td>-4.914891</td>\n",
       "      <td>-5.175593</td>\n",
       "      <td>-4.973880</td>\n",
       "      <td>0.190784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>9.040403</td>\n",
       "      <td>1.685038</td>\n",
       "      <td>0.165928</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>6</td>\n",
       "      <td>(32, 8, 4, 64, 4, 32)</td>\n",
       "      <td>[0.1, 0.0, 0.1, 0.3, 0.2, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.405110</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.375657</td>\n",
       "      <td>0.379090</td>\n",
       "      <td>86</td>\n",
       "      <td>-5.346155</td>\n",
       "      <td>-5.380891</td>\n",
       "      <td>-5.276293</td>\n",
       "      <td>-5.069633</td>\n",
       "      <td>-4.890098</td>\n",
       "      <td>-5.192614</td>\n",
       "      <td>0.185831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>3.422601</td>\n",
       "      <td>0.382395</td>\n",
       "      <td>0.213016</td>\n",
       "      <td>0.061827</td>\n",
       "      <td>0.001</td>\n",
       "      <td>6</td>\n",
       "      <td>(64, 16, 4, 8, 32, 32)</td>\n",
       "      <td>[0.1, 0.3, 0.2, 0.1, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-4.548371</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.392225</td>\n",
       "      <td>0.887496</td>\n",
       "      <td>87</td>\n",
       "      <td>-4.518882</td>\n",
       "      <td>-6.087813</td>\n",
       "      <td>-5.100180</td>\n",
       "      <td>-5.217447</td>\n",
       "      <td>-4.971993</td>\n",
       "      <td>-5.179263</td>\n",
       "      <td>0.512296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>14.426819</td>\n",
       "      <td>3.061459</td>\n",
       "      <td>1.793804</td>\n",
       "      <td>3.285712</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>(16, 16, 4, 16, 16, 32)</td>\n",
       "      <td>[0.3, 0.2, 0.0, 0.3, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.201230</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.517056</td>\n",
       "      <td>0.625360</td>\n",
       "      <td>88</td>\n",
       "      <td>-5.445820</td>\n",
       "      <td>-5.597322</td>\n",
       "      <td>-5.001362</td>\n",
       "      <td>-5.494509</td>\n",
       "      <td>-5.186966</td>\n",
       "      <td>-5.345196</td>\n",
       "      <td>0.218744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6.512098</td>\n",
       "      <td>0.439773</td>\n",
       "      <td>0.160318</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(64, 64, 16, 8, 16, 4)</td>\n",
       "      <td>[0.1, 0.0, 0.1, 0.3, 0.3, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-5.241066</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.586394</td>\n",
       "      <td>0.446234</td>\n",
       "      <td>89</td>\n",
       "      <td>-5.378867</td>\n",
       "      <td>-5.076195</td>\n",
       "      <td>-4.928210</td>\n",
       "      <td>-5.642927</td>\n",
       "      <td>-5.739969</td>\n",
       "      <td>-5.353234</td>\n",
       "      <td>0.313541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>8.599574</td>\n",
       "      <td>1.625767</td>\n",
       "      <td>0.155374</td>\n",
       "      <td>0.005132</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(16, 4, 16, 64, 32, 4)</td>\n",
       "      <td>[0.1, 0.2, 0.3, 0.0, 0.0, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-6.164407</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.047069</td>\n",
       "      <td>0.298713</td>\n",
       "      <td>90</td>\n",
       "      <td>-6.379998</td>\n",
       "      <td>-4.862440</td>\n",
       "      <td>-5.837305</td>\n",
       "      <td>-6.433269</td>\n",
       "      <td>-6.167385</td>\n",
       "      <td>-5.936079</td>\n",
       "      <td>0.576233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>11.735566</td>\n",
       "      <td>3.837439</td>\n",
       "      <td>0.163388</td>\n",
       "      <td>0.019203</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 32, 4, 32, 8, 64)</td>\n",
       "      <td>[0.2, 0.2, 0.3, 0.0, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-6.989330</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.451127</td>\n",
       "      <td>0.553148</td>\n",
       "      <td>91</td>\n",
       "      <td>-7.000594</td>\n",
       "      <td>-5.779102</td>\n",
       "      <td>-6.327941</td>\n",
       "      <td>-6.332878</td>\n",
       "      <td>-5.660076</td>\n",
       "      <td>-6.220118</td>\n",
       "      <td>0.477834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>5.204273</td>\n",
       "      <td>4.720349</td>\n",
       "      <td>0.157496</td>\n",
       "      <td>0.018788</td>\n",
       "      <td>0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>(32, 64, 32, 4, 16, 16)</td>\n",
       "      <td>[0.2, 0.0, 0.0, 0.3, 0.2, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "      <td>-5.328671</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.500778</td>\n",
       "      <td>3.473391</td>\n",
       "      <td>92</td>\n",
       "      <td>-5.529484</td>\n",
       "      <td>-4.378184</td>\n",
       "      <td>-3.913424</td>\n",
       "      <td>-13.710665</td>\n",
       "      <td>-4.357785</td>\n",
       "      <td>-6.377908</td>\n",
       "      <td>3.705203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>13.113736</td>\n",
       "      <td>0.068896</td>\n",
       "      <td>0.125294</td>\n",
       "      <td>0.002814</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(16, 4, 64, 16, 16, 4)</td>\n",
       "      <td>[0.3, 0.2, 0.2, 0.3, 0.1, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-7.879407</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.706776</td>\n",
       "      <td>1.546990</td>\n",
       "      <td>93</td>\n",
       "      <td>-8.035842</td>\n",
       "      <td>-7.782463</td>\n",
       "      <td>-5.551841</td>\n",
       "      <td>-4.933928</td>\n",
       "      <td>-5.951871</td>\n",
       "      <td>-6.451189</td>\n",
       "      <td>1.236417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.281405</td>\n",
       "      <td>2.304073</td>\n",
       "      <td>0.156015</td>\n",
       "      <td>0.007284</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>(8, 64, 4, 32, 4, 16)</td>\n",
       "      <td>[0.0, 0.0, 0.3, 0.3, 0.0, 0.3]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-17.031278</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.886962</td>\n",
       "      <td>4.578487</td>\n",
       "      <td>95</td>\n",
       "      <td>-17.054244</td>\n",
       "      <td>-5.345611</td>\n",
       "      <td>-5.017079</td>\n",
       "      <td>-5.474901</td>\n",
       "      <td>-5.876398</td>\n",
       "      <td>-7.753647</td>\n",
       "      <td>4.658440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13.781819</td>\n",
       "      <td>0.409153</td>\n",
       "      <td>0.126869</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(8, 8, 32, 4, 64, 4)</td>\n",
       "      <td>[0.1, 0.0, 0.3, 0.0, 0.2, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-7.107257</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.288732</td>\n",
       "      <td>1.639294</td>\n",
       "      <td>96</td>\n",
       "      <td>-6.994251</td>\n",
       "      <td>-8.821019</td>\n",
       "      <td>-11.220460</td>\n",
       "      <td>-12.070127</td>\n",
       "      <td>-7.477776</td>\n",
       "      <td>-9.316727</td>\n",
       "      <td>2.011307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>13.640168</td>\n",
       "      <td>0.143648</td>\n",
       "      <td>0.129454</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(8, 64, 4, 16, 32, 64)</td>\n",
       "      <td>[0.1, 0.3, 0.0, 0.0, 0.0, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-11.604601</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.408201</td>\n",
       "      <td>1.736465</td>\n",
       "      <td>97</td>\n",
       "      <td>-11.870255</td>\n",
       "      <td>-8.237780</td>\n",
       "      <td>-11.194554</td>\n",
       "      <td>-6.748886</td>\n",
       "      <td>-7.933195</td>\n",
       "      <td>-9.196934</td>\n",
       "      <td>1.982275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>13.330210</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.126670</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(8, 4, 4, 8, 4, 32)</td>\n",
       "      <td>[0.3, 0.2, 0.0, 0.0, 0.1, 0.0]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-9.602331</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.505083</td>\n",
       "      <td>1.839436</td>\n",
       "      <td>98</td>\n",
       "      <td>-9.924255</td>\n",
       "      <td>-6.321433</td>\n",
       "      <td>-9.521984</td>\n",
       "      <td>-12.843763</td>\n",
       "      <td>-9.545767</td>\n",
       "      <td>-9.631440</td>\n",
       "      <td>2.067860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>13.385822</td>\n",
       "      <td>0.354089</td>\n",
       "      <td>0.126376</td>\n",
       "      <td>0.004995</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(4, 16, 64, 16, 32, 32)</td>\n",
       "      <td>[0.2, 0.1, 0.0, 0.1, 0.0, 0.1]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-10.089707</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.969133</td>\n",
       "      <td>1.492908</td>\n",
       "      <td>99</td>\n",
       "      <td>-10.085990</td>\n",
       "      <td>-11.250737</td>\n",
       "      <td>-12.850215</td>\n",
       "      <td>-11.670335</td>\n",
       "      <td>-8.318553</td>\n",
       "      <td>-10.835166</td>\n",
       "      <td>1.537871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15.869949</td>\n",
       "      <td>3.967390</td>\n",
       "      <td>0.156611</td>\n",
       "      <td>0.059104</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1</td>\n",
       "      <td>(4, 8, 64, 16, 16, 32)</td>\n",
       "      <td>[0.2, 0.2, 0.0, 0.0, 0.3, 0.2]</td>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.0001, 'mlp...</td>\n",
       "      <td>-13.486479</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.888237</td>\n",
       "      <td>2.074782</td>\n",
       "      <td>100</td>\n",
       "      <td>-13.974036</td>\n",
       "      <td>-13.038808</td>\n",
       "      <td>-10.985046</td>\n",
       "      <td>-10.274794</td>\n",
       "      <td>-15.973325</td>\n",
       "      <td>-12.849202</td>\n",
       "      <td>2.057377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "87       5.470936      0.476462         0.149602        0.009291   \n",
       "13       3.254239      0.393104         0.159371        0.001939   \n",
       "31       5.769879      0.999212         0.158720        0.003884   \n",
       "47       2.913223      0.096621         0.147412        0.003389   \n",
       "77       4.804874      0.198884         0.167738        0.013939   \n",
       "44       2.922058      0.248130         0.153787        0.005266   \n",
       "5        3.082494      0.262968         0.157247        0.006005   \n",
       "43       2.814192      0.231426         0.166566        0.012136   \n",
       "7        3.129875      0.208705         0.159137        0.005455   \n",
       "54      13.533426      1.276400         0.147617        0.003273   \n",
       "37       4.707712      0.612028         0.148188        0.007616   \n",
       "91       4.405320      0.428264         0.161241        0.016881   \n",
       "46       3.113723      0.356087         0.166270        0.005083   \n",
       "24       3.599186      0.430841         0.157698        0.001363   \n",
       "25      10.985743      1.947874         0.134508        0.001539   \n",
       "56       2.773342      0.177885         0.164475        0.016423   \n",
       "42       3.658636      0.934027         0.158290        0.007577   \n",
       "23       3.420327      0.248797         0.178662        0.024193   \n",
       "48       3.866739      1.498146         0.161018        0.009644   \n",
       "78      13.500372      0.039636         0.122368        0.003843   \n",
       "3        2.833933      0.340188         0.258638        0.155654   \n",
       "99       3.251839      0.234404         0.193962        0.015705   \n",
       "36       3.685257      0.226788         0.173976        0.005980   \n",
       "58       3.051652      0.083580         0.168676        0.017655   \n",
       "20       3.827376      0.611622         0.406129        0.419923   \n",
       "6        7.303824      1.669410         0.268030        0.212775   \n",
       "40      10.415561      1.032343         0.174150        0.016677   \n",
       "2        3.867887      0.799057         0.160561        0.005973   \n",
       "52       4.744860      0.703259         0.168538        0.004894   \n",
       "33       6.800638      1.451079         0.147935        0.004325   \n",
       "0        8.659430      1.323079         0.154089        0.005283   \n",
       "81       8.522782      1.261459         0.153680        0.004085   \n",
       "14       4.159350      0.276013         0.147613        0.003488   \n",
       "96      11.113196      1.149716         0.185216        0.017740   \n",
       "80      15.316132      0.109703         0.157230        0.002353   \n",
       "55       5.072642      1.164321         0.175479        0.027345   \n",
       "67       7.233608      1.203604         0.156587        0.003478   \n",
       "94       4.387280      1.067347         0.197560        0.019012   \n",
       "92      13.057288      2.172402         0.182621        0.017662   \n",
       "75      15.038958      0.208476         0.151496        0.009477   \n",
       "30       3.170824      0.091711         0.168823        0.009237   \n",
       "97       3.588123      0.999971         0.181228        0.020010   \n",
       "63       3.832562      0.352106         0.158609        0.014964   \n",
       "22       3.988067      0.731589         0.180840        0.021492   \n",
       "29      15.071348      1.730528         0.168402        0.006225   \n",
       "9        5.589028      0.565618         0.450885        0.574599   \n",
       "62       9.040403      1.685038         0.165928        0.005528   \n",
       "89       3.422601      0.382395         0.213016        0.061827   \n",
       "84      14.426819      3.061459         1.793804        3.285712   \n",
       "32       6.512098      0.439773         0.160318        0.006789   \n",
       "85       8.599574      1.625767         0.155374        0.005132   \n",
       "71      11.735566      3.837439         0.163388        0.019203   \n",
       "79       5.204273      4.720349         0.157496        0.018788   \n",
       "41      13.113736      0.068896         0.125294        0.002814   \n",
       "4       12.281405      2.304073         0.156015        0.007284   \n",
       "15      13.781819      0.409153         0.126869        0.004056   \n",
       "21      13.640168      0.143648         0.129454        0.006268   \n",
       "74      13.330210      0.114900         0.126670        0.002074   \n",
       "28      13.385822      0.354089         0.126376        0.004995   \n",
       "39      15.869949      3.967390         0.156611        0.059104   \n",
       "\n",
       "   param_mlp__optimizer__learning_rate param_mlp__nlayers  \\\n",
       "87                               0.001                  2   \n",
       "13                               0.005                  5   \n",
       "31                               0.001                  5   \n",
       "47                               0.005                  4   \n",
       "77                               0.001                  5   \n",
       "44                               0.005                  4   \n",
       "5                                0.005                  5   \n",
       "43                               0.005                  6   \n",
       "7                                0.005                  5   \n",
       "54                              0.0001                  4   \n",
       "37                               0.001                  4   \n",
       "91                               0.001                  3   \n",
       "46                               0.005                  6   \n",
       "24                               0.001                  4   \n",
       "25                              0.0001                  2   \n",
       "56                               0.005                  4   \n",
       "42                               0.005                  6   \n",
       "23                               0.005                  5   \n",
       "48                               0.001                  5   \n",
       "78                              0.0001                  1   \n",
       "3                                0.005                  5   \n",
       "99                               0.005                  6   \n",
       "36                               0.001                  6   \n",
       "58                               0.005                  5   \n",
       "20                               0.005                  6   \n",
       "6                               0.0001                  5   \n",
       "40                              0.0001                  6   \n",
       "2                                0.001                  5   \n",
       "52                               0.001                  5   \n",
       "33                              0.0001                  4   \n",
       "0                               0.0001                  4   \n",
       "81                              0.0001                  5   \n",
       "14                               0.001                  4   \n",
       "96                              0.0001                  4   \n",
       "80                              0.0001                  5   \n",
       "55                               0.001                  6   \n",
       "67                              0.0001                  5   \n",
       "94                               0.001                  6   \n",
       "92                              0.0001                  5   \n",
       "75                              0.0001                  4   \n",
       "30                               0.001                  5   \n",
       "97                               0.005                  5   \n",
       "63                               0.001                  4   \n",
       "22                               0.001                  6   \n",
       "29                              0.0001                  5   \n",
       "9                               0.0001                  6   \n",
       "62                              0.0001                  6   \n",
       "89                               0.001                  6   \n",
       "84                              0.0001                  4   \n",
       "32                              0.0001                  5   \n",
       "85                              0.0001                  5   \n",
       "71                              0.0001                  5   \n",
       "79                               0.001                  4   \n",
       "41                              0.0001                  1   \n",
       "4                               0.0001                  5   \n",
       "15                              0.0001                  1   \n",
       "21                              0.0001                  1   \n",
       "74                              0.0001                  1   \n",
       "28                              0.0001                  1   \n",
       "39                              0.0001                  1   \n",
       "\n",
       "          param_mlp__hiddens             param_mlp__dropouts  \\\n",
       "87   (4, 64, 32, 16, 16, 16)  [0.2, 0.0, 0.0, 0.3, 0.0, 0.0]   \n",
       "13    (4, 16, 32, 4, 64, 64)  [0.0, 0.0, 0.0, 0.0, 0.1, 0.1]   \n",
       "31      (4, 16, 4, 32, 4, 8)  [0.0, 0.0, 0.0, 0.0, 0.1, 0.1]   \n",
       "47    (8, 16, 16, 64, 8, 32)  [0.0, 0.3, 0.0, 0.3, 0.3, 0.0]   \n",
       "77      (4, 8, 32, 16, 8, 4)  [0.1, 0.0, 0.3, 0.0, 0.0, 0.2]   \n",
       "44    (4, 64, 32, 64, 32, 8)  [0.1, 0.0, 0.0, 0.3, 0.0, 0.3]   \n",
       "5     (8, 4, 16, 64, 16, 16)  [0.2, 0.0, 0.0, 0.2, 0.2, 0.0]   \n",
       "43      (32, 4, 32, 8, 4, 8)  [0.0, 0.2, 0.0, 0.0, 0.0, 0.0]   \n",
       "7      (4, 8, 64, 32, 16, 4)  [0.1, 0.2, 0.0, 0.1, 0.2, 0.1]   \n",
       "54     (4, 64, 64, 64, 8, 8)  [0.0, 0.1, 0.0, 0.0, 0.0, 0.0]   \n",
       "37     (4, 64, 32, 4, 4, 64)  [0.1, 0.1, 0.2, 0.0, 0.3, 0.0]   \n",
       "91   (16, 16, 32, 32, 4, 16)  [0.0, 0.3, 0.2, 0.3, 0.0, 0.1]   \n",
       "46     (4, 64, 16, 4, 64, 4)  [0.0, 0.1, 0.3, 0.0, 0.1, 0.2]   \n",
       "24    (64, 4, 4, 64, 32, 16)  [0.3, 0.1, 0.1, 0.1, 0.3, 0.2]   \n",
       "25    (32, 64, 8, 16, 8, 64)  [0.0, 0.2, 0.1, 0.3, 0.3, 0.1]   \n",
       "56     (16, 16, 16, 8, 8, 4)  [0.0, 0.2, 0.2, 0.3, 0.2, 0.0]   \n",
       "42    (4, 64, 4, 16, 64, 16)  [0.0, 0.0, 0.0, 0.1, 0.2, 0.0]   \n",
       "23      (4, 8, 8, 4, 32, 32)  [0.3, 0.0, 0.3, 0.0, 0.1, 0.3]   \n",
       "48   (64, 16, 32, 32, 4, 64)  [0.0, 0.3, 0.2, 0.2, 0.0, 0.0]   \n",
       "78    (64, 8, 8, 32, 64, 32)  [0.3, 0.0, 0.2, 0.0, 0.2, 0.1]   \n",
       "3    (32, 8, 32, 32, 32, 64)  [0.0, 0.3, 0.2, 0.2, 0.3, 0.1]   \n",
       "99     (8, 4, 32, 16, 8, 16)  [0.3, 0.2, 0.0, 0.3, 0.2, 0.3]   \n",
       "36   (32, 16, 32, 64, 4, 16)  [0.0, 0.2, 0.3, 0.3, 0.0, 0.1]   \n",
       "58     (4, 16, 16, 8, 8, 32)  [0.0, 0.3, 0.3, 0.1, 0.2, 0.0]   \n",
       "20        (8, 8, 8, 4, 4, 8)  [0.0, 0.0, 0.2, 0.1, 0.3, 0.0]   \n",
       "6   (64, 16, 16, 32, 32, 64)  [0.2, 0.1, 0.0, 0.3, 0.0, 0.0]   \n",
       "40    (16, 64, 4, 4, 32, 16)  [0.1, 0.2, 0.0, 0.0, 0.3, 0.1]   \n",
       "2    (16, 8, 16, 32, 32, 64)  [0.2, 0.2, 0.0, 0.1, 0.2, 0.3]   \n",
       "52     (8, 8, 16, 8, 32, 32)  [0.0, 0.2, 0.0, 0.3, 0.3, 0.0]   \n",
       "33   (16, 64, 64, 16, 4, 64)  [0.1, 0.2, 0.3, 0.0, 0.0, 0.0]   \n",
       "0     (32, 32, 32, 4, 8, 32)  [0.3, 0.2, 0.0, 0.0, 0.0, 0.1]   \n",
       "81  (16, 64, 64, 16, 16, 16)  [0.0, 0.3, 0.3, 0.0, 0.2, 0.3]   \n",
       "14       (8, 32, 4, 4, 4, 8)  [0.0, 0.0, 0.2, 0.2, 0.0, 0.1]   \n",
       "96     (64, 4, 32, 4, 16, 8)  [0.2, 0.2, 0.3, 0.0, 0.3, 0.2]   \n",
       "80    (4, 32, 16, 16, 16, 8)  [0.1, 0.0, 0.0, 0.1, 0.2, 0.0]   \n",
       "55      (4, 16, 32, 4, 4, 8)  [0.0, 0.0, 0.0, 0.0, 0.2, 0.2]   \n",
       "67     (64, 8, 4, 64, 16, 8)  [0.1, 0.0, 0.2, 0.0, 0.1, 0.0]   \n",
       "94     (4, 4, 64, 4, 64, 16)  [0.2, 0.3, 0.1, 0.1, 0.0, 0.2]   \n",
       "92    (8, 4, 32, 16, 16, 64)  [0.2, 0.0, 0.0, 0.3, 0.0, 0.1]   \n",
       "75     (4, 8, 32, 16, 8, 64)  [0.1, 0.1, 0.0, 0.1, 0.0, 0.1]   \n",
       "30   (32, 64, 16, 32, 4, 16)  [0.1, 0.2, 0.0, 0.0, 0.3, 0.1]   \n",
       "97      (8, 8, 4, 32, 4, 32)  [0.0, 0.3, 0.1, 0.0, 0.3, 0.2]   \n",
       "63     (16, 16, 4, 16, 4, 8)  [0.3, 0.1, 0.2, 0.0, 0.0, 0.0]   \n",
       "22     (64, 4, 4, 16, 32, 8)  [0.0, 0.3, 0.2, 0.2, 0.1, 0.0]   \n",
       "29     (4, 16, 8, 32, 32, 8)  [0.0, 0.1, 0.3, 0.0, 0.1, 0.2]   \n",
       "9    (64, 16, 32, 64, 64, 8)  [0.3, 0.1, 0.3, 0.0, 0.0, 0.0]   \n",
       "62     (32, 8, 4, 64, 4, 32)  [0.1, 0.0, 0.1, 0.3, 0.2, 0.3]   \n",
       "89    (64, 16, 4, 8, 32, 32)  [0.1, 0.3, 0.2, 0.1, 0.0, 0.3]   \n",
       "84   (16, 16, 4, 16, 16, 32)  [0.3, 0.2, 0.0, 0.3, 0.0, 0.0]   \n",
       "32    (64, 64, 16, 8, 16, 4)  [0.1, 0.0, 0.1, 0.3, 0.3, 0.1]   \n",
       "85    (16, 4, 16, 64, 32, 4)  [0.1, 0.2, 0.3, 0.0, 0.0, 0.0]   \n",
       "71     (8, 32, 4, 32, 8, 64)  [0.2, 0.2, 0.3, 0.0, 0.0, 0.1]   \n",
       "79   (32, 64, 32, 4, 16, 16)  [0.2, 0.0, 0.0, 0.3, 0.2, 0.1]   \n",
       "41    (16, 4, 64, 16, 16, 4)  [0.3, 0.2, 0.2, 0.3, 0.1, 0.2]   \n",
       "4      (8, 64, 4, 32, 4, 16)  [0.0, 0.0, 0.3, 0.3, 0.0, 0.3]   \n",
       "15      (8, 8, 32, 4, 64, 4)  [0.1, 0.0, 0.3, 0.0, 0.2, 0.2]   \n",
       "21    (8, 64, 4, 16, 32, 64)  [0.1, 0.3, 0.0, 0.0, 0.0, 0.2]   \n",
       "74       (8, 4, 4, 8, 4, 32)  [0.3, 0.2, 0.0, 0.0, 0.1, 0.0]   \n",
       "28   (4, 16, 64, 16, 32, 32)  [0.2, 0.1, 0.0, 0.1, 0.0, 0.1]   \n",
       "39    (4, 8, 64, 16, 16, 32)  [0.2, 0.2, 0.0, 0.0, 0.3, 0.2]   \n",
       "\n",
       "                                               params  split0_test_score  ...  \\\n",
       "87  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -2.957158  ...   \n",
       "13  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.359709  ...   \n",
       "31  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.004115  ...   \n",
       "47  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -2.979923  ...   \n",
       "77  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.120479  ...   \n",
       "44  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.184837  ...   \n",
       "5   {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.238365  ...   \n",
       "43  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.580219  ...   \n",
       "7   {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.456528  ...   \n",
       "54  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -3.357574  ...   \n",
       "37  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.077817  ...   \n",
       "91  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.516341  ...   \n",
       "46  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.416646  ...   \n",
       "24  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.027332  ...   \n",
       "25  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -3.461880  ...   \n",
       "56  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.992399  ...   \n",
       "42  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.108080  ...   \n",
       "23  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.441650  ...   \n",
       "48  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.700792  ...   \n",
       "78  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -3.714196  ...   \n",
       "3   {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -4.235472  ...   \n",
       "99  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.601571  ...   \n",
       "36  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.780503  ...   \n",
       "58  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.739882  ...   \n",
       "20  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.973052  ...   \n",
       "6   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -3.957772  ...   \n",
       "40  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -3.908104  ...   \n",
       "2   {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.676929  ...   \n",
       "52  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.318925  ...   \n",
       "33  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.649604  ...   \n",
       "0   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.097206  ...   \n",
       "81  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.487602  ...   \n",
       "14  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -3.809388  ...   \n",
       "96  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.183973  ...   \n",
       "80  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.904852  ...   \n",
       "55  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.257807  ...   \n",
       "67  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.388000  ...   \n",
       "94  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.760008  ...   \n",
       "92  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.196199  ...   \n",
       "75  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.163329  ...   \n",
       "30  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.142184  ...   \n",
       "97  {'mlp__optimizer__learning_rate': 0.005, 'mlp_...          -3.362262  ...   \n",
       "63  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.535151  ...   \n",
       "22  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.507253  ...   \n",
       "29  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.822970  ...   \n",
       "9   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -4.869548  ...   \n",
       "62  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.405110  ...   \n",
       "89  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -4.548371  ...   \n",
       "84  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.201230  ...   \n",
       "32  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -5.241066  ...   \n",
       "85  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -6.164407  ...   \n",
       "71  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -6.989330  ...   \n",
       "79  {'mlp__optimizer__learning_rate': 0.001, 'mlp_...          -5.328671  ...   \n",
       "41  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -7.879407  ...   \n",
       "4   {'mlp__optimizer__learning_rate': 0.0001, 'mlp...         -17.031278  ...   \n",
       "15  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -7.107257  ...   \n",
       "21  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...         -11.604601  ...   \n",
       "74  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...          -9.602331  ...   \n",
       "28  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...         -10.089707  ...   \n",
       "39  {'mlp__optimizer__learning_rate': 0.0001, 'mlp...         -13.486479  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "87        -3.189668        0.284323                1           -3.021284   \n",
       "13        -3.339414        0.187513                2           -3.276286   \n",
       "31        -3.388544        0.282147                5           -3.191301   \n",
       "47        -3.389105        0.262381                6           -3.050441   \n",
       "77        -3.409711        0.226976                7           -3.220394   \n",
       "44        -3.449240        0.152593                8           -3.271727   \n",
       "5         -3.456015        0.223538                9           -3.247670   \n",
       "43        -3.458093        0.514689               10           -3.624743   \n",
       "7         -3.512017        0.102197               15           -3.829248   \n",
       "54        -3.541656        0.242615               17           -3.112184   \n",
       "37        -3.544212        0.443199               19           -3.112963   \n",
       "91        -3.706200        0.290178               34           -3.411198   \n",
       "46        -3.744554        0.476248               37           -3.433923   \n",
       "24        -3.764683        0.521580               38           -3.014995   \n",
       "25        -3.784165        0.266862               40           -3.358058   \n",
       "56        -3.829739        0.283589               43           -3.822174   \n",
       "42        -3.891749        1.150862               46           -2.900171   \n",
       "23        -3.930755        0.448353               47           -4.517016   \n",
       "48        -3.952290        0.392927               48           -3.701288   \n",
       "78        -3.973507        0.277631               49           -3.674985   \n",
       "3         -4.050233        0.854436               52           -4.166159   \n",
       "99        -4.086420        0.674452               53           -3.679801   \n",
       "36        -4.128354        0.349025               55           -3.663205   \n",
       "58        -4.138396        0.496801               56           -3.758855   \n",
       "20        -4.175025        0.759491               58           -4.053489   \n",
       "6         -4.240322        0.404230               60           -3.829952   \n",
       "40        -4.241201        0.462397               61           -3.938528   \n",
       "2         -4.246026        0.367301               62           -3.886688   \n",
       "52        -4.485782        0.855480               66           -3.578431   \n",
       "33        -4.564738        0.469138               67           -4.756161   \n",
       "0         -4.576668        0.381651               68           -3.894645   \n",
       "81        -4.669700        0.443496               70           -4.535055   \n",
       "14        -4.709663        0.785014               71           -3.702351   \n",
       "96        -4.716829        0.568313               72           -4.110705   \n",
       "80        -4.730211        0.324791               73           -4.959069   \n",
       "55        -4.754752        0.567503               74           -4.401380   \n",
       "67        -4.775378        0.396797               75           -4.238092   \n",
       "94        -4.796932        1.033392               76           -4.937289   \n",
       "92        -4.881512        0.781419               78           -4.168462   \n",
       "75        -4.983803        0.980938               79           -4.700527   \n",
       "30        -4.991305        0.819548               80           -4.258037   \n",
       "97        -5.048584        1.691472               81           -3.512012   \n",
       "63        -5.058758        0.463635               82           -4.457248   \n",
       "22        -5.078855        0.782286               83           -4.556275   \n",
       "29        -5.130311        0.541140               84           -4.835890   \n",
       "9         -5.191053        0.641848               85           -4.812561   \n",
       "62        -5.375657        0.379090               86           -5.346155   \n",
       "89        -5.392225        0.887496               87           -4.518882   \n",
       "84        -5.517056        0.625360               88           -5.445820   \n",
       "32        -5.586394        0.446234               89           -5.378867   \n",
       "85        -6.047069        0.298713               90           -6.379998   \n",
       "71        -6.451127        0.553148               91           -7.000594   \n",
       "79        -6.500778        3.473391               92           -5.529484   \n",
       "41        -6.706776        1.546990               93           -8.035842   \n",
       "4         -7.886962        4.578487               95          -17.054244   \n",
       "15        -9.288732        1.639294               96           -6.994251   \n",
       "21        -9.408201        1.736465               97          -11.870255   \n",
       "74        -9.505083        1.839436               98           -9.924255   \n",
       "28       -10.969133        1.492908               99          -10.085990   \n",
       "39       -12.888237        2.074782              100          -13.974036   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "87           -3.033675           -3.146795           -3.115551   \n",
       "13           -3.058097           -3.007638           -2.995851   \n",
       "31           -3.203838           -3.386880           -3.343279   \n",
       "47           -3.067475           -3.243703           -3.062296   \n",
       "77           -3.050101           -3.269757           -3.170700   \n",
       "44           -2.954547           -3.284860           -3.516394   \n",
       "5            -3.003067           -3.510962           -3.714861   \n",
       "43           -3.823013           -3.361067           -2.679449   \n",
       "7            -3.139033           -3.222706           -3.332092   \n",
       "54           -3.550833           -3.291955           -3.909624   \n",
       "37           -3.445468           -3.464016           -3.251278   \n",
       "91           -3.340559           -3.343767           -3.677989   \n",
       "46           -3.767617           -3.445217           -3.330944   \n",
       "24           -3.359020           -3.815708           -4.145503   \n",
       "25           -3.698754           -3.577373           -3.402665   \n",
       "56           -3.603794           -3.540664           -3.151512   \n",
       "42           -3.110227           -2.890419           -3.143502   \n",
       "23           -3.795329           -3.722823           -3.454863   \n",
       "48           -3.604465           -3.363142           -3.933469   \n",
       "78           -3.643582           -3.838981           -3.634803   \n",
       "3            -4.546339           -3.158995           -3.006879   \n",
       "99           -3.348609           -4.842064           -4.919756   \n",
       "36           -3.603885           -3.809171           -4.237593   \n",
       "58           -4.097590           -4.616371           -3.692351   \n",
       "20           -3.262997           -3.994926           -5.677628   \n",
       "6            -4.127654           -3.772763           -4.157570   \n",
       "40           -4.110858           -4.649694           -4.021063   \n",
       "2            -3.296845           -4.230406           -4.545318   \n",
       "52           -4.722735           -5.098557           -3.904302   \n",
       "33           -4.141934           -4.396064           -4.651727   \n",
       "0            -4.290361           -4.646753           -4.430793   \n",
       "81           -4.475437           -4.136284           -4.725159   \n",
       "14           -5.100660           -4.410270           -4.342451   \n",
       "96           -4.895442           -4.619998           -4.177744   \n",
       "80           -4.398470           -4.014894           -4.783172   \n",
       "55           -4.095224           -4.807626           -5.376511   \n",
       "67           -4.482292           -4.405696           -4.352433   \n",
       "94           -5.290316           -4.152518           -5.304166   \n",
       "92           -5.461524           -5.127280           -4.101031   \n",
       "75           -5.791149           -4.725082           -4.299535   \n",
       "30           -4.185727           -3.815762           -5.644473   \n",
       "97           -6.887151           -4.371084           -3.867183   \n",
       "63           -4.286443           -5.660571           -4.708597   \n",
       "22           -4.399721           -4.058169           -5.960668   \n",
       "29           -4.896477           -4.848215           -5.879037   \n",
       "9            -5.219427           -4.746927           -4.914891   \n",
       "62           -5.380891           -5.276293           -5.069633   \n",
       "89           -6.087813           -5.100180           -5.217447   \n",
       "84           -5.597322           -5.001362           -5.494509   \n",
       "32           -5.076195           -4.928210           -5.642927   \n",
       "85           -4.862440           -5.837305           -6.433269   \n",
       "71           -5.779102           -6.327941           -6.332878   \n",
       "79           -4.378184           -3.913424          -13.710665   \n",
       "41           -7.782463           -5.551841           -4.933928   \n",
       "4            -5.345611           -5.017079           -5.474901   \n",
       "15           -8.821019          -11.220460          -12.070127   \n",
       "21           -8.237780          -11.194554           -6.748886   \n",
       "74           -6.321433           -9.521984          -12.843763   \n",
       "28          -11.250737          -12.850215          -11.670335   \n",
       "39          -13.038808          -10.985046          -10.274794   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "87           -3.032817         -3.070024         0.051084  \n",
       "13           -2.941716         -3.055918         0.116231  \n",
       "31           -2.948415         -3.214743         0.153461  \n",
       "47           -3.360610         -3.156905         0.124348  \n",
       "77           -3.474918         -3.237174         0.139485  \n",
       "44           -3.227274         -3.250960         0.179141  \n",
       "5            -3.077744         -3.310861         0.266957  \n",
       "43           -2.958318         -3.289318         0.420473  \n",
       "7            -3.674975         -3.439611         0.266897  \n",
       "54           -3.374830         -3.447885         0.270610  \n",
       "37           -3.177948         -3.290334         0.141314  \n",
       "91           -3.548275         -3.464358         0.130671  \n",
       "46           -4.653721         -3.726284         0.486336  \n",
       "24           -3.374597         -3.541964         0.394556  \n",
       "25           -3.438046         -3.494979         0.125558  \n",
       "56           -3.832396         -3.590108         0.248015  \n",
       "42           -6.277767         -3.664417         1.310818  \n",
       "23           -3.601107         -3.818227         0.367996  \n",
       "48           -3.762802         -3.673033         0.188371  \n",
       "78           -3.717554         -3.701981         0.074382  \n",
       "3            -4.170782         -3.809831         0.611232  \n",
       "99           -3.722860         -4.102618         0.649002  \n",
       "36           -3.905857         -3.843942         0.223739  \n",
       "58           -3.784866         -3.990007         0.342934  \n",
       "20           -3.290253         -4.055858         0.877341  \n",
       "6            -4.009025         -3.979393         0.154684  \n",
       "40           -3.861953         -4.116419         0.279250  \n",
       "2            -4.671376         -4.126127         0.495768  \n",
       "52           -4.295389         -4.319883         0.546202  \n",
       "33           -3.879096         -4.364996         0.323124  \n",
       "0            -4.756420         -4.403794         0.302060  \n",
       "81           -4.575393         -4.489466         0.194909  \n",
       "14           -4.576970         -4.426540         0.449188  \n",
       "96           -4.422670         -4.445312         0.288860  \n",
       "80           -4.988226         -4.628766         0.372029  \n",
       "55           -4.276505         -4.591449         0.457080  \n",
       "67           -5.224022         -4.540507         0.350879  \n",
       "94           -4.135320         -4.763922         0.523061  \n",
       "92           -5.682073         -4.908074         0.656015  \n",
       "75           -4.247497         -4.752758         0.555419  \n",
       "30           -5.861397         -4.753079         0.832883  \n",
       "97           -5.821289         -4.891744         1.270447  \n",
       "63           -4.695567         -4.761685         0.476177  \n",
       "22           -5.657922         -4.926551         0.744720  \n",
       "29           -4.828615         -5.057647         0.411376  \n",
       "9            -5.175593         -4.973880         0.190784  \n",
       "62           -4.890098         -5.192614         0.185831  \n",
       "89           -4.971993         -5.179263         0.512296  \n",
       "84           -5.186966         -5.345196         0.218744  \n",
       "32           -5.739969         -5.353234         0.313541  \n",
       "85           -6.167385         -5.936079         0.576233  \n",
       "71           -5.660076         -6.220118         0.477834  \n",
       "79           -4.357785         -6.377908         3.705203  \n",
       "41           -5.951871         -6.451189         1.236417  \n",
       "4            -5.876398         -7.753647         4.658440  \n",
       "15           -7.477776         -9.316727         2.011307  \n",
       "21           -7.933195         -9.196934         1.982275  \n",
       "74           -9.545767         -9.631440         2.067860  \n",
       "28           -8.318553        -10.835166         1.537871  \n",
       "39          -15.973325        -12.849202         2.057377  \n",
       "\n",
       "[60 rows x 24 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results[no_overfit].sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_results[no_overfit]\n",
    "    .sort_values(\"mean_test_score\", ascending=False)\n",
    "    .to_csv(\"best_model_2.csv\", index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>5.470936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.476462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.149602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.009291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__optimizer__learning_rate</th>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__nlayers</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__hiddens</th>\n",
       "      <td>(4, 64, 32, 16, 16, 16)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_mlp__dropouts</th>\n",
       "      <td>[0.2 0.  0.  0.3 0.  0. ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'mlp__optimizer__learning_rate': 0.001, 'mlp_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>-2.957158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>-3.613551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>-3.173981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>-3.380016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>-2.823634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>-3.189668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.284323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rank_test_score</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>-3.021284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>-3.033675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>-3.146795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>-3.115551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>-3.032817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>-3.070024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.051084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                     0\n",
       "mean_fit_time                                                                 5.470936\n",
       "std_fit_time                                                                  0.476462\n",
       "mean_score_time                                                               0.149602\n",
       "std_score_time                                                                0.009291\n",
       "param_mlp__optimizer__learning_rate                                              0.001\n",
       "param_mlp__nlayers                                                                   2\n",
       "param_mlp__hiddens                                             (4, 64, 32, 16, 16, 16)\n",
       "param_mlp__dropouts                                          [0.2 0.  0.  0.3 0.  0. ]\n",
       "params                               {'mlp__optimizer__learning_rate': 0.001, 'mlp_...\n",
       "split0_test_score                                                            -2.957158\n",
       "split1_test_score                                                            -3.613551\n",
       "split2_test_score                                                            -3.173981\n",
       "split3_test_score                                                            -3.380016\n",
       "split4_test_score                                                            -2.823634\n",
       "mean_test_score                                                              -3.189668\n",
       "std_test_score                                                                0.284323\n",
       "rank_test_score                                                                      1\n",
       "split0_train_score                                                           -3.021284\n",
       "split1_train_score                                                           -3.033675\n",
       "split2_train_score                                                           -3.146795\n",
       "split3_train_score                                                           -3.115551\n",
       "split4_train_score                                                           -3.032817\n",
       "mean_train_score                                                             -3.070024\n",
       "std_train_score                                                               0.051084"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = pd.read_csv(\"best_model_2.csv\")\n",
    "best_model.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = KerasRegressor(\n",
    "    twoLayerFeedForward, \n",
    "    epochs=200, \n",
    "    loss=\"mse\",\n",
    "    callbacks=[callback],\n",
    "    validation_split=0.2,\n",
    "    nlayers=3,\n",
    "    dropouts=[0.2, 0., 0.],\n",
    "    hiddens=(4, 4, 32),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipe = Pipeline(\n",
    "    [\n",
    "        ('scaler', PowerTransformer()),\n",
    "        (\"principal_components\", PCA(n_components=20, svd_solver = 'full')),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_513\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2468 (Dense)          (None, 4)                 84        \n",
      "                                                                 \n",
      " dropout_1147 (Dropout)      (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_2469 (Dense)          (None, 64)                320       \n",
      "                                                                 \n",
      " dense_2470 (Dense)          (None, 8)                 520       \n",
      "                                                                 \n",
      " dropout_1148 (Dropout)      (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_2471 (Dense)          (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 933 (3.64 KB)\n",
      "Trainable params: 933 (3.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp = twoLayerFeedForward(\n",
    "    hiddens=(4, 64, 8,),\n",
    "    dropouts=[0.1, 0., 0.2],\n",
    "    nlayers=3,\n",
    "    meta={\"X_shape_\":(0, 20)}\n",
    ")\n",
    "mlp.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001), loss=\"mae\")\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = preprocessing_pipe.fit_transform(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x16c1454e8b0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callback = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0,\n",
    "    patience=6,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "mlp.fit(new_df, y.values, epochs=200, validation_split=0.2, callbacks=[callback], verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16c255d31c0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_history = mlp.history.history[\"loss\"]\n",
    "loss_history_val = mlp.history.history[\"val_loss\"]\n",
    "plt.plot(loss_history, label=\"Train\")\n",
    "plt.plot(loss_history_val, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.yscale(\"log\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
